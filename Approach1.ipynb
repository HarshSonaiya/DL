{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMSD93ZvX5H++oRlt8hVyIa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshSonaiya/DL/blob/main/Approach1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install langchain elasticsearch langchain_community\n",
        "!pip install pypdf sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lL1EQ4iFq5f_",
        "outputId": "3efde4a6-825b-4707-cfbe-bb4828b19bcf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting elasticsearch\n",
            "  Downloading elasticsearch-8.15.1-py3-none-any.whl.metadata (8.7 kB)\n",
            "Collecting langchain_community\n",
            "  Downloading langchain_community-0.3.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.5)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_core-0.3.2-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.125-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting elastic-transport<9,>=8.13 (from elasticsearch)\n",
            "  Downloading elastic_transport-8.15.0-py3-none-any.whl.metadata (3.6 kB)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain_community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain_community)\n",
            "  Downloading pydantic_settings-2.5.2-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.11.1)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading marshmallow-3.22.0-py3-none-any.whl.metadata (7.2 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.2 in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.13->elasticsearch) (2.0.7)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from elastic-transport<9,>=8.13->elasticsearch) (2024.8.30)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.0->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.0->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain_community)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.0->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n",
            "Downloading langchain-0.3.0-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading elasticsearch-8.15.1-py3-none-any.whl (524 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.6/524.6 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_community-0.3.0-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m41.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading elastic_transport-8.15.0-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.4/64.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.2-py3-none-any.whl (399 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.7/399.7 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.125-py3-none-any.whl (290 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m290.2/290.2 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydantic_settings-2.5.2-py3-none-any.whl (26 kB)\n",
            "Downloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading marshmallow-3.22.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, python-dotenv, orjson, mypy-extensions, marshmallow, jsonpointer, h11, elastic-transport, typing-inspect, jsonpatch, httpcore, elasticsearch, pydantic-settings, httpx, dataclasses-json, langsmith, langchain-core, langchain-text-splitters, langchain, langchain_community\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed dataclasses-json-0.6.7 elastic-transport-8.15.0 elasticsearch-8.15.1 h11-0.14.0 httpcore-1.0.5 httpx-0.27.2 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.0 langchain-core-0.3.2 langchain-text-splitters-0.3.0 langchain_community-0.3.0 langsmith-0.1.125 marshmallow-3.22.0 mypy-extensions-1.0.0 orjson-3.10.7 pydantic-settings-2.5.2 python-dotenv-1.0.1 tenacity-8.5.0 typing-inspect-0.9.0\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-5.0.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Collecting sentence_transformers\n",
            "  Downloading sentence_transformers-3.1.1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0 in /usr/local/lib/python3.10/dist-packages (from pypdf) (4.12.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.44.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (4.66.5)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.4.1+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.3.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (0.24.7)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (10.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (3.16.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2024.6.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (2024.9.11)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.38.0->sentence_transformers) (0.19.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->sentence_transformers) (2024.8.30)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Downloading pypdf-5.0.0-py3-none-any.whl (292 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.8/292.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sentence_transformers-3.1.1-py3-none-any.whl (245 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, sentence_transformers\n",
            "Successfully installed pypdf-5.0.0 sentence_transformers-3.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "CYExB3pjq3SU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6bae8f84-794c-4e09-f92a-55a492cdf2cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "from langchain.retrievers import BM25Retriever\n",
        "from langchain.vectorstores import ElasticsearchStore\n",
        "from elasticsearch import Elasticsearch\n",
        "from typing import List\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.schema import Document\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "es_client = Elasticsearch() # Replace with your Elasticsearch connection details"
      ],
      "metadata": {
        "id": "eGuyEzl1xtuf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_name = 'temp'\n",
        "    # Define mappings for dense and sparse vectors\n",
        "mappings = {\n",
        "        \"properties\": {\n",
        "            \"content\": {\n",
        "                \"type\": \"text\",\n",
        "                \"similarity\":\"BM25\"\n",
        "            },\n",
        "            \"dense_vector\": {\n",
        "                \"type\": \"dense_vector\",\n",
        "                \"dims\": 384\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Create index with mapping\n",
        "es_client.indices.create(index=index_name, mappings = mappings)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTEnZf_OrNmA",
        "outputId": "3fb5b2bd-cdbf-4da2-93a3-6f1af80139d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'temp'})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_content_from_pdf(file: str) -> List[Document]:\n",
        "    \"\"\"\n",
        "    Extract and split content from a PDF file into chunks.\n",
        "\n",
        "    Args:\n",
        "        file (str): Path to the PDF file.\n",
        "\n",
        "    Returns:\n",
        "        List: A list of Documents containing various attributes\n",
        "        like page_content, metadata,etc. extracted from the PDF.\n",
        "    \"\"\"\n",
        "    loader = PyPDFLoader(file)\n",
        "    docs = loader.load()\n",
        "    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=400)\n",
        "    chunks = splitter.split_documents(docs)\n",
        "    return chunks\n"
      ],
      "metadata": {
        "id": "Sn7SKrNXxSTL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks = extract_content_from_pdf(\"/content/LSTM.pdf\")\n"
      ],
      "metadata": {
        "id": "ne6t8jiWwqdh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chunks"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gVBR8fpwx272",
        "outputId": "e4b93632-2754-4f57-b42c-29cfd5adb568"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Document(metadata={'source': '/content/LSTM.pdf', 'page': 0}, page_content='Communicated by Ronald Williams\\nLong Short-Term Memory\\nSepp Hochreiter\\nFakult ¨at f¨ur Informatik, Technische Universit ¨at M ¨unchen, 80290 M ¨unchen, Germany\\nJ¨urgen Schmidhuber\\nIDSIA, Corso Elvezia 36, 6900 Lugano, Switzerland\\nLearning to store information over extended time intervals by recurrent'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 0}, page_content='Long Short-Term Memory\\nSepp Hochreiter\\nFakult ¨at f¨ur Informatik, Technische Universit ¨at M ¨unchen, 80290 M ¨unchen, Germany\\nJ¨urgen Schmidhuber\\nIDSIA, Corso Elvezia 36, 6900 Lugano, Switzerland\\nLearning to store information over extended time intervals by recurrent\\nbackpropagation takes a very long time, mostly because of insufﬁcient,decaying error backﬂow. We brieﬂy review Hochreiter’s (1991) analysis ofthis problem, then address it by introducing a novel, efﬁcient, gradient-based method called long short-term memory (LSTM). Truncating thegradient where this does not do harm, LSTM can learn to bridge minimaltime lags in excess of 1000 discrete-time steps by enforcing constant errorﬂow through constant error carousels within special units. Multiplicativegate units learn to open and close access to the constant error ﬂow. LSTMis local in space and time; its computational complexity per time stepand weight is O(1). Our experiments with artiﬁcial data involve local,'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 0}, page_content='distributed, real-valued, and noisy pattern representations. In compar-isons with real-time recurrent learning, back propagation through time,recurrent cascade correlation, Elman nets, and neural sequence chunk-ing, LSTM leads to many more successful runs, and learns much faster.LSTM also solves complex, artiﬁcial long-time-lag tasks that have neverbeen solved by previous recurrent network algorithms.\\n1 Introduction\\nIn principle, recurrent networks can use their feedback connections to store'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 0}, page_content='1 Introduction\\nIn principle, recurrent networks can use their feedback connections to store\\nrepresentations of recent input events in the form of activations (short-termmemory, as opposed to long-term memory embodied by slowly changingweights). This is potentially signiﬁcant for many applications, includingspeech processing, non-Markovian control, and music composition (Mozer,1992). The most widely used algorithms for learning what to put in short-term memory, however, take too much time or do not work well at all, espe-cially when minimal time lags between inputs and corresponding teachersignals are long. Although theoretically fascinating, existing methods donot provide clear practical advantages over, say, backpropagation in feed-forward nets with limited time windows. This article reviews an analysis ofthe problem and suggests a remedy.\\nNeural Computation 9, 1735–1780 (1997) c⃝1997 Massachusetts Institute of Technology'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 1}, page_content='1736 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nThe problem. With conventional backpropagation through time (BPTT;\\nWilliams & Zipser, 1992; Werbos, 1988) or real-time recurrent learning (RTRL;Robinson & Fallside, 1987), error signals ﬂowing backward in time tend to(1) blow up or (2) vanish; the temporal evolution of the backpropagatederror exponentially depends on the size of the weights (Hochreiter, 1991).Case 1 may lead to oscillating weights; in case 2, learning to bridge longtime lags takes a prohibitive amount of time or does not work at all (seesection 3).\\nThis article presents long short-term memory (LSTM), a novel recurrent'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 1}, page_content='This article presents long short-term memory (LSTM), a novel recurrent\\nnetwork architecture in conjunction with an appropriate gradient-basedlearning algorithm. LSTM is designed to overcome these error backﬂowproblems. It can learn to bridge time intervals in excess of 1000 steps evenin case of noisy, incompressible input sequences, without loss of short-time-lag capabilities. This is achieved by an efﬁcient, gradient-based algorithm foran architecture enforcing constant (thus, neither exploding nor vanishing)error ﬂow through internal states of special units (provided the gradientcomputation is truncated at certain architecture-speciﬁc points; this doesnot affect long-term error ﬂow, though).\\nSection 2 brieﬂy reviews previous work. Section 3 begins with an outline'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 1}, page_content='Section 2 brieﬂy reviews previous work. Section 3 begins with an outline\\nof the detailed analysis of vanishing errors due to Hochreiter (1991). It thenintroduces a naive approach to constant error backpropagation for didac-tic purposes and highlights its problems concerning information storageand retrieval. These problems lead to the LSTM architecture described insection 4. Section 5 presents numerous experiments and comparisons withcompeting methods. LSTM outperforms them and also learns to solve com-plex, artiﬁcial tasks no other recurrent net algorithm has solved. Section 6discusses LSTM’s limitations and advantages. The appendix contains a de-tailed description of the algorithm (A.1) and explicit error ﬂow formulas(A.2).\\n2 Previous Work\\nThis section focuses on recurrent nets with time-varying inputs (as opposed\\nto nets with stationary inputs and ﬁxed-point-based gradient calculations;e.g., Almeida, 1987; Pineda, 1987).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 1}, page_content='2 Previous Work\\nThis section focuses on recurrent nets with time-varying inputs (as opposed\\nto nets with stationary inputs and ﬁxed-point-based gradient calculations;e.g., Almeida, 1987; Pineda, 1987).\\n2.1 Gradient-Descent Variants. The approaches of Elman (1988), Fahl-\\nman (1991), Williams (1989), Schmidhuber (1992a), Pearlmutter (1989), andmany of the related algorithms in Pearlmutter’s comprehensive overview(1995) suffer from the same problems as BPTT and RTRL (see sections 1and 3).\\n2.2 Time Delays. Other methods that seem practical for short time lags\\nonly are time-delay neural networks (Lang, Waibel, & Hinton, 1990) andPlate’s method (Plate, 1993), which updates unit activations based on a'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 2}, page_content='Long Short-Term Memory 1737\\nweighted sum of old activations (see also de Vries & Principe, 1991). Lin et\\nal. (1996) propose variants of time-delay networks called NARX networks.\\n2.3 Time Constants. To deal with long time lags, Mozer (1992) uses time\\nconstants inﬂuencing changes of unit activations (deVries and Principe’s1991 approach may in fact be viewed as a mixture of time-delay neural net-works and time constants). For long time lags, however, the time constantsneed external ﬁne tuning (Mozer, 1992). Sun, Chen, and Lee’s alternativeapproach (1993) updates the activation of a recurrent unit by adding theold activation and the (scaled) current net input. The net input, however,tends to perturb the stored information, which makes long-term storageimpractical.\\n2.4 Ring’s Approach. Ring (1993) also proposed a method for bridging'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 2}, page_content='2.4 Ring’s Approach. Ring (1993) also proposed a method for bridging\\nlong time lags. Whenever a unit in his network receives conﬂicting errorsignals, he adds a higher-order unit inﬂuencing appropriate connections.Although his approach can sometimes be extremely fast, to bridge a timelag involving 100 steps may require the addition of 100 units. Also, Ring’snet does not generalize to unseen lag durations.\\n2.5 Bengio et al.’s Approach. Bengio, Simard, and Frasconi (1994) in-\\nvestigate methods such as simulated annealing, multigrid random search,time-weighted pseudo-Newton optimization, and discrete error propaga-tion. Their “latch” and “two-sequence” problems are very similar to prob-lem 3a in this article with minimal time lag 100 (see Experiment 3). Bengioand Frasconi (1994) also propose an expectation-maximazation approachfor propagating targets. With nso-called state networks, at a given time,\\ntheir system can be in one of only ndifferent states. (See also the beginning'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 2}, page_content='their system can be in one of only ndifferent states. (See also the beginning\\nof section 5.) But to solve continuous problems such as the adding problem(section 5.4), their system would require an unacceptable number of states(i.e., state networks).\\n2.6 Kalman Filters. Puskorius and Feldkamp (1994) use Kalman ﬁlter\\ntechniques to improve recurrent net performance. Since they use “a deriva-tive discount factor imposed to decay exponentially the effects of past dy-namic derivatives,” there is no reason to believe that their Kalman ﬁlter-trained recurrent networks will be useful for very long minimal time lags.\\n2.7 Second Order Nets. We will see that LSTM uses multiplicative units'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 2}, page_content='techniques to improve recurrent net performance. Since they use “a deriva-tive discount factor imposed to decay exponentially the effects of past dy-namic derivatives,” there is no reason to believe that their Kalman ﬁlter-trained recurrent networks will be useful for very long minimal time lags.\\n2.7 Second Order Nets. We will see that LSTM uses multiplicative units\\n(MUs) to protect error ﬂow from unwanted perturbations. It is not the ﬁrstrecurrent net method using MUs, though. For instance, Watrous and Kuhn(1992) use MUs in second-order nets. There are some differences from LSTM:(1) Watrous and Kuhn’s architecture does not enforce constant error ﬂowand is not designed to solve long-time-lag problems; (2) it has fully con-nected second-order sigma-pi units, while the LSTM architecture’s MUs'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 3}, page_content='1738 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nare used only to gate access to constant error ﬂow; and (3) Watrous and\\nKuhn’s algorithm costs O(W2)operations per time step, ours only O(W),\\nwhere Wis the number of weights. See also Miller and Giles (1993) for\\nadditional work on MUs.\\n2.8 Simple Weight Guessing. To avoid long-time-lag problems of'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 3}, page_content='Kuhn’s algorithm costs O(W2)operations per time step, ours only O(W),\\nwhere Wis the number of weights. See also Miller and Giles (1993) for\\nadditional work on MUs.\\n2.8 Simple Weight Guessing. To avoid long-time-lag problems of\\ngradient-based approaches, we may simply randomly initialize all networkweights until the resulting net happens to classify all training sequencescorrectly. In fact, recently we discovered (Schmidhuber & Hochreiter, 1996;Hochreiter & Schmidhuber, 1996, 1997) that simple weight guessing solvesmany of the problems in Bengio et al. (1994), Bengio and Frasconi (1994),Miller and Giles (1993), and Lin et al. (1996) faster than the algorithms theseauthors proposed. This does not mean that weight guessing is a good algo-rithm. It just means that the problems are very simple. More realistic tasksrequire either many free parameters (e.g., input weights) or high weight pre-cision (e.g., for continuous-valued parameters), such that guessing becomescompletely infeasible.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 3}, page_content='2.9 Adaptive Sequence Chunkers. Schmidhuber’s hierarchical chun-\\nker systems (1992b, 1993) do have a capability to bridge arbitrary time lags,but only if there is local predictability across the subsequences causing thetime lags (see also Mozer, 1992). For instance, in his postdoctoral thesis,Schmidhuber (1993) uses hierarchical recurrent nets to solve rapidly cer-tain grammar learning tasks involving minimal time lags in excess of 1000steps. The performance of chunker systems, however, deteriorates as thenoise level increases and the input sequences become less compressible.LSTM does not suffer from this problem.\\n3 Constant Error Backpropagation\\n3.1 Exponentially Decaying Error3.1.1 Conventional BPTT (e.g., Williams & Zipser, 1992). Output unit k’s\\ntarget at time tis denoted by d\\nk(t). Using mean squared error, k’s error signal\\nis\\nϑk(t)=f′\\nk(netk(t))(dk(t)−yk(t)),\\nwhere\\nyi(t)=fi(neti(t))\\nis the activation of a noninput unit iwith differentiable activation function\\nfi,\\nneti(t)=∑'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 3}, page_content='3 Constant Error Backpropagation\\n3.1 Exponentially Decaying Error3.1.1 Conventional BPTT (e.g., Williams & Zipser, 1992). Output unit k’s\\ntarget at time tis denoted by d\\nk(t). Using mean squared error, k’s error signal\\nis\\nϑk(t)=f′\\nk(netk(t))(dk(t)−yk(t)),\\nwhere\\nyi(t)=fi(neti(t))\\nis the activation of a noninput unit iwith differentiable activation function\\nfi,\\nneti(t)=∑\\njwijyj(t−1)'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 4}, page_content='Long Short-Term Memory 1739\\nis unit i’s current net input, and wijis the weight on the connection from\\nunit jtoi. Some nonoutput unit j’s backpropagated error signal is\\nϑj(t)=f′\\nj(netj(t))∑\\niwijϑi(t+1).\\nThe corresponding contribution to wjl’s total weight update is αϑj(t)yl(t−1),\\nwhereαis the learning rate and lstands for an arbitrary unit connected to\\nunit j.\\n3.1.2 Outline of Hochreiter’s Analysis (1991, pp. 19–21). Suppose we have\\na fully connected net whose noninput unit indices range from 1 to n. Let\\nus focus on local error ﬂow from unit uto unit v(later we will see that the\\nanalysis immediately extends to global error ﬂow). The error occurring atan arbitrary unit uat time step tis propagated back into time for qtime\\nsteps, to an arbitrary unit v. This will scale the error by the following factor:\\n∂ϑ\\nv(t−q)\\n∂ϑu(t)={\\nf′\\nv(netv(t−1))wuv q=1\\nf′\\nv(netv(t−q))∑n\\nl=1∂ϑl(t−q+1)\\n∂ϑu(t)wlvq>1. (3.1)\\nWith lq=vand l0=u, we obtain:\\n∂ϑv(t−q)\\n∂ϑu(t)=n∑\\nl1=1...n∑\\nlq−1=1q∏\\nm=1f′'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 4}, page_content='analysis immediately extends to global error ﬂow). The error occurring atan arbitrary unit uat time step tis propagated back into time for qtime\\nsteps, to an arbitrary unit v. This will scale the error by the following factor:\\n∂ϑ\\nv(t−q)\\n∂ϑu(t)={\\nf′\\nv(netv(t−1))wuv q=1\\nf′\\nv(netv(t−q))∑n\\nl=1∂ϑl(t−q+1)\\n∂ϑu(t)wlvq>1. (3.1)\\nWith lq=vand l0=u, we obtain:\\n∂ϑv(t−q)\\n∂ϑu(t)=n∑\\nl1=1...n∑\\nlq−1=1q∏\\nm=1f′\\nlm(netlm(t−m))wlmlm−1 (3.2)\\n(proof by induction). The sum of the nq−1terms∏q\\nm=1f′\\nlm(netlm(t−m))wlmlm−1\\ndetermines the total error backﬂow (note that since the summation terms\\nmay have different signs, increasing the number of units ndoes not neces-\\nsarily increase error ﬂow).\\n3.1.3 Intuitive Explanation of Equation 3.2. If\\n|f′\\nlm(netlm(t−m))wlmlm−1|>1.0\\nfor all m(as can happen, e.g., with linear flm), then the largest product\\nincreases exponentially with q. That is, the error blows up, and conﬂicting\\nerror signals arriving at unit vcan lead to oscillating weights and unstable'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 4}, page_content='sarily increase error ﬂow).\\n3.1.3 Intuitive Explanation of Equation 3.2. If\\n|f′\\nlm(netlm(t−m))wlmlm−1|>1.0\\nfor all m(as can happen, e.g., with linear flm), then the largest product\\nincreases exponentially with q. That is, the error blows up, and conﬂicting\\nerror signals arriving at unit vcan lead to oscillating weights and unstable\\nlearning (for error blowups or bifurcations, see also Pineda, 1988; Baldi &Pineda, 1991; Doya, 1992). On the other hand, if\\n|f\\n′\\nlm(netlm(t−m))wlmlm−1|<1.0\\nfor all m, then the largest product decreases exponentially with q. That is,\\nthe error vanishes, and nothing can be learned in acceptable time.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 5}, page_content='1740 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nIfflmis the logistic sigmoid function, then the maximal value of f′\\nlmis\\n0.25. If ylm−1is constant and not equal to zero, then |f′\\nlm(netlm)wlmlm−1|takes\\non maximal values where\\nwlmlm−1=1\\nylm−1coth(1\\n2netlm)\\n,\\ngoes to zero for|wlmlm−1|→∞ , and is less than 1 .0 for|wlmlm−1|<4.0 (e.g.,\\nif the absolute maximal weight value wmaxis smaller than 4.0). Hence with\\nconventional logistic sigmoid activation functions, the error ﬂow tends tovanish as long as the weights have absolute values below 4.0, especiallyin the beginning of the training phase. In general, the use of larger initialweights will not help, though, as seen above, for |w\\nlmlm−1|→∞ the relevant'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 5}, page_content='if the absolute maximal weight value wmaxis smaller than 4.0). Hence with\\nconventional logistic sigmoid activation functions, the error ﬂow tends tovanish as long as the weights have absolute values below 4.0, especiallyin the beginning of the training phase. In general, the use of larger initialweights will not help, though, as seen above, for |w\\nlmlm−1|→∞ the relevant\\nderivative goes to zero “faster” than the absolute weight can grow (also,some weights will have to change their signs by crossing zero). Likewise,increasing the learning rate does not help either; it will not change the ratioof long-range error ﬂow and short-range error ﬂow. BPTT is too sensitiveto recent distractions. (A very similar, more recent analysis was presentedby Bengio et al., 1994.)\\n3.1.4 Global Error Flow. The local error ﬂow analysis above immediately\\nshows that global error ﬂow vanishes too. To see this, compute\\n∑\\nu:uoutput unit∂ϑv(t−q)\\n∂ϑu(t).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 5}, page_content='3.1.4 Global Error Flow. The local error ﬂow analysis above immediately\\nshows that global error ﬂow vanishes too. To see this, compute\\n∑\\nu:uoutput unit∂ϑv(t−q)\\n∂ϑu(t).\\n3.1.5 Weak Upper Bound for Scaling Factor. The following, slightly ex-\\ntended vanishing error analysis also takes n, the number of units, into ac-\\ncount. For q>1, equation 3.2 can be rewritten as\\n(WuT)TF′(t−1)q−1∏\\nm=2(WF′(t−m))Wvf′\\nv(netv(t−q)),\\nwhere the weight matrix Wis deﬁned by [ W]ij:=wij,v’s outgoing weight\\nvector Wvis deﬁned by [ Wv]i:=[W]iv=wiv,u’s incoming weight vector\\nWuTis deﬁned by [ WuT]i:=[W]ui=wui, and for m=1,..., q,F′(t−m)is\\nthe diagonal matrix of ﬁrst-order derivatives deﬁned as [ F′(t−m)]ij:=0i f\\ni̸=j, and [ F′(t−m)]ij:=f′\\ni(neti(t−m))otherwise. Here Tis the transposition\\noperator, [ A]ijis the element in the ith column and jth row of matrix A, and\\n[x]iis the ith component of vector x.\\nUsing a matrix norm ∥·∥ Acompatible with vector norm ∥·∥ x, we deﬁne\\nf′\\nmax:=max\\nm=1,...,q{∥F′(t−m)∥A}.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 5}, page_content='the diagonal matrix of ﬁrst-order derivatives deﬁned as [ F′(t−m)]ij:=0i f\\ni̸=j, and [ F′(t−m)]ij:=f′\\ni(neti(t−m))otherwise. Here Tis the transposition\\noperator, [ A]ijis the element in the ith column and jth row of matrix A, and\\n[x]iis the ith component of vector x.\\nUsing a matrix norm ∥·∥ Acompatible with vector norm ∥·∥ x, we deﬁne\\nf′\\nmax:=max\\nm=1,...,q{∥F′(t−m)∥A}.\\nFor max i=1,...,n{|xi|}≤∥ x∥xwe get|xTy|≤ n∥x∥x∥y∥x.Since\\n|f′\\nv(netv(t−q))|≤∥ F′(t−q)∥A≤f′\\nmax,'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 6}, page_content='Long Short-Term Memory 1741\\nwe obtain the following inequality:\\n⏐⏐⏐⏐∂ϑ\\nv(t−q)\\n∂ϑu(t)⏐⏐⏐⏐≤n(f\\n′\\nmax)q∥Wv∥x∥WuT∥x∥W∥q−2\\nA≤n(\\nf′\\nmax∥W∥A)q.\\nThis inequality results from\\n∥Wv∥x=∥Wev∥x≤∥W∥A∥ev∥x≤∥W∥A\\nand\\n∥WuT∥x=∥WTeu∥x≤∥W∥A∥eu∥x≤∥W∥A,\\nwhere ekis the unit vector whose components are 0 except for the kth com-\\nponent, which is 1. Note that this is a weak, extreme case upper bound; itwill be reached only if all ∥F\\n′(t−m)∥Atake on maximal values, and if the\\ncontributions of all paths across which error ﬂows back from unit uto unit\\nvhave the same sign. Large ∥W∥A, however, typically result in small values\\nof∥F′(t−m)∥A, as conﬁrmed by experiments (see, e.g., Hochreiter, 1991).\\nFor example, with norms\\n∥W∥A:=max\\nr∑\\ns|wrs|\\nand\\n∥x∥x:=max\\nr|xr|,\\nwe have f′\\nmax=0.25 for the logistic sigmoid. We observe that if\\n|wij|≤wmax<4.0\\nn∀i,j,\\nthen∥W∥A≤nwmax<4.0 will result in exponential decay. By setting\\nτ:=(nwmax\\n4.0)\\n<1.0, we obtain\\n⏐⏐⏐⏐∂ϑ\\nv(t−q)\\n∂ϑu(t)⏐⏐⏐⏐≤n(τ)\\nq.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 6}, page_content='of∥F′(t−m)∥A, as conﬁrmed by experiments (see, e.g., Hochreiter, 1991).\\nFor example, with norms\\n∥W∥A:=max\\nr∑\\ns|wrs|\\nand\\n∥x∥x:=max\\nr|xr|,\\nwe have f′\\nmax=0.25 for the logistic sigmoid. We observe that if\\n|wij|≤wmax<4.0\\nn∀i,j,\\nthen∥W∥A≤nwmax<4.0 will result in exponential decay. By setting\\nτ:=(nwmax\\n4.0)\\n<1.0, we obtain\\n⏐⏐⏐⏐∂ϑ\\nv(t−q)\\n∂ϑu(t)⏐⏐⏐⏐≤n(τ)\\nq.\\nWe refer to Hochreiter (1991) for additional results.\\n3.2 Constant Error Flow: Naive Approach.3.2.1 A Single Unit. To avoid vanishing error signals, how can we\\nachieve constant error ﬂow through a single unit jwith a single connec-\\ntion to itself? According to the rules above, at time t,j’s local error backﬂow\\nisϑ\\nj(t)=f′\\nj(netj(t))ϑj(t+1)wjj. To enforce constant error ﬂow through j,w e\\nrequire\\nf′\\nj(netj(t))wjj=1.0.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 7}, page_content='1742 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nNote the similarity to Mozer’s ﬁxed time constant system (1992)—a time\\nconstant of 1 .0 is appropriate for potentially inﬁnite time lags.1\\n3.2.2 The Constant Error Carousel. Integrating the differential equation\\nabove, we obtain\\nfj(netj(t))=netj(t)\\nwjj\\nfor arbitrary netj(t). This means fjhas to be linear, and unit j’s activation has\\nto remain constant:\\nyj(t+1)=fj(netj(t+1))=fj(wjjyj(t))=yj(t).\\nIn the experiments, this will be ensured by using the identity function fj:\\nfj(x)=x,∀x, and by setting wjj=1.0. We refer to this as the constant error\\ncarousel (CEC). CEC will be LSTM’s central feature (see section 4).\\nOf course, unit jwill not only be connected to itself but also to other\\nunits. This invokes two obvious, related problems (also inherent in all othergradient-based approaches):\\n1. Input weight conﬂict: For simplicity, let us focus on a single addi-\\ntional input weight w\\nji. Assume that the total error can be reduced by'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 7}, page_content='carousel (CEC). CEC will be LSTM’s central feature (see section 4).\\nOf course, unit jwill not only be connected to itself but also to other\\nunits. This invokes two obvious, related problems (also inherent in all othergradient-based approaches):\\n1. Input weight conﬂict: For simplicity, let us focus on a single addi-\\ntional input weight w\\nji. Assume that the total error can be reduced by\\nswitching on unit jin response to a certain input and keeping it active\\nfor a long time (until it helps to compute a desired output). Providediis nonzero, since the same incoming weight has to be used for both\\nstoring certain inputs and ignoring others, w\\njiwill often receive con-\\nﬂicting weight update signals during this time (recall that jis linear).\\nThese signals will attempt to make wjiparticipate in (1) storing the\\ninput (by switching on j) and (2) protecting the input (by preventing j'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 7}, page_content='storing certain inputs and ignoring others, w\\njiwill often receive con-\\nﬂicting weight update signals during this time (recall that jis linear).\\nThese signals will attempt to make wjiparticipate in (1) storing the\\ninput (by switching on j) and (2) protecting the input (by preventing j\\nfrom being switched off by irrelevant later inputs). This conﬂict makeslearning difﬁcult and calls for a more context-sensitive mechanism forcontrolling write operations through input weights.\\n2. Output weight conﬂict: Assume jis switched on and currently stores\\nsome previous input. For simplicity, let us focus on a single additionaloutgoing weight w\\nkj. The same wkjhas to be used for both retriev-\\ningj’s content at certain times and preventing jfrom disturbing kat\\nother times. As long as unit jis nonzero, wkjwill attract conﬂicting\\nweight update signals generated during sequence processing. Thesesignals will attempt to make w\\nkjparticipate in accessing the informa-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 7}, page_content='kj. The same wkjhas to be used for both retriev-\\ningj’s content at certain times and preventing jfrom disturbing kat\\nother times. As long as unit jis nonzero, wkjwill attract conﬂicting\\nweight update signals generated during sequence processing. Thesesignals will attempt to make w\\nkjparticipate in accessing the informa-\\ntion stored in jand—at different times—protecting unit kfrom being\\nperturbed by j. For instance, with many tasks there are certain short-\\ntime-lag errors that can be reduced in early training stages. However,\\n1We do not use the expression “time constant” in the differential sense, as Pearlmutter\\n(1995) does.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 8}, page_content='Long Short-Term Memory 1743\\nat later training stages, jmay suddenly start to cause avoidable er-\\nrors in situations that already seemed under control by attempting toparticipate in reducing more difﬁcult long-time-lag errors. Again, thisconﬂict makes learning difﬁcult and calls for a more context-sensitivemechanism for controlling read operations through output weights.\\nOf course, input and output weight conﬂicts are not speciﬁc for long time\\nlags; they occur for short time lags as well. Their effects, however, becomeparticularly pronounced in the long-time-lag case. As the time lag increases,stored information must be protected against perturbation for longer andlonger periods, and, especially in advanced stages of learning, more andmore already correct outputs also require protection against perturbation.\\nDue to the problems set out, the naive approach does not work well ex-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 8}, page_content='Due to the problems set out, the naive approach does not work well ex-\\ncept in the case of certain simple problems involving local input-output rep-resentations and nonrepeating input patterns (see Hochreiter, 1991; Silva,Amarel, Langlois, & Almeida, 1996). The next section shows how to do itright.\\n4 The Concept of Long Short-Term Memory\\n4.1 Memory Cells and Gate Units. To construct an architecture that al-\\nlows for constant error ﬂow through special, self-connected units withoutthe disadvantages of the naive approach, we extend the CEC embodiedby the self-connected, linear unit jfrom section 3.2 by introducing addi-\\ntional features. A multiplicative input gate unit is introduced to protect thememory contents stored in jfrom perturbation by irrelevant inputs, and\\na multiplicative output gate unit is introduced to protect other units fromperturbation by currently irrelevant memory contents stored in j.\\nThe resulting, more complex unit is called a memory cell (see Figure 1).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 8}, page_content='tional features. A multiplicative input gate unit is introduced to protect thememory contents stored in jfrom perturbation by irrelevant inputs, and\\na multiplicative output gate unit is introduced to protect other units fromperturbation by currently irrelevant memory contents stored in j.\\nThe resulting, more complex unit is called a memory cell (see Figure 1).\\nThe jth memory cell is denoted c\\nj. Each memory cell is built around a central\\nlinear unit with a ﬁxed self-connection (the CEC). In addition to netcj,cjgets\\ninput from a multiplicative unit outj(the output gate), and from another\\nmultiplicative unit inj(the input gate). inj’s activation at time tis denoted\\nbyyinj(t),outj’s by youtj(t). We have\\nyoutj(t)=foutj(netoutj(t));yinj(t)=finj(netinj(t));\\nwhere\\nnetoutj(t)=∑\\nuwoutjuyu(t−1),\\nand\\nnetinj(t)=∑\\nuwinjuyu(t−1).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 9}, page_content='1744 Sepp Hochreiter and J ¨ urgen Schmidhuber\\n \\ngh1.0\\nnet\\nwyinyoutnetc\\ngyin=g+ scsc yin\\nhyout\\nnetwc\\nin outwicc\\nj\\nj\\njj\\noutwj\\ninjj\\njj jjy\\njj\\njj\\ni\\ni i\\nFigure 1: Architecture of memory cell cj(the box) and its gate units inj,outj. The\\nself-recurrent connection (with weight 1.0) indicates feedback with a delay ofone time step. It builds the basis of the CEC. The gate units open and close accessto CEC. See text and appendix A.1 for details.\\nWe also have\\nnetcj(t)=∑\\nuwcjuyu(t−1).\\nThe summation indices umay stand for input units, gate units, memory\\ncells, or even conventional hidden units if there are any (see section 4.3). Allthese different types of units may convey useful information about the cur-rent state of the net. For instance, an input gate (output gate) may use inputsfrom other memory cells to decide whether to store (access) certain infor-mation in its memory cell. There even may be recurrent self-connectionslikew'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 9}, page_content='cells, or even conventional hidden units if there are any (see section 4.3). Allthese different types of units may convey useful information about the cur-rent state of the net. For instance, an input gate (output gate) may use inputsfrom other memory cells to decide whether to store (access) certain infor-mation in its memory cell. There even may be recurrent self-connectionslikew\\ncjcj. It is up to the user to deﬁne the network topology. See Figure 2 for\\nan example.\\nAt time t,cj’s output ycj(t)is computed as\\nycj(t)=youtj(t)h(scj(t)),\\nwhere the internal state scj(t)is\\nscj(0)=0,scj(t)=scj(t−1)+yinj(t)g(\\nnetcj(t))\\nfort>0.\\nThe differentiable function gsquashes netcj; the differentiable function h\\nscales memory cell outputs computed from the internal state scj.\\n4.2 Why Gate Units? To avoid input weight conﬂicts, injcontrols the\\nerror ﬂow to memory cell cj’s input connections wcji. To circumvent cj’s\\noutput weight conﬂicts, outjcontrols the error ﬂow from unit j’s output'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 10}, page_content='Long Short-Term Memory 1745\\n \\n11 2output\\nhidden\\ninputout 1\\nin 1out 2\\nin 21cell\\nblock block1cell\\nblock block\\n2cell2 cell2\\nFigure 2: Example of a net with eight input units, four output units, and two\\nmemory cell blocks of size 2. in1 marks the input gate, out1 marks the output\\ngate, and cell1/block 1 marks the ﬁrst memory cell of block 1. cell1/block 1’s archi-\\ntecture is identical to the one in Figure 1, with gate units in1 and out1 (note that'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 10}, page_content='by rotating Figure 1 by 90 degrees anticlockwise, it will match with the corre-sponding parts of Figure 2). The example assumes dense connectivity: each gateunit and each memory cell sees all non-output units. For simplicity, however,outgoing weights of only one type of unit are shown for each layer. With theefﬁcient, truncated update rule, error ﬂows only through connections to outputunit, and through ﬁxed self-connections within cell blocks (not shown here; seeFigure 1). Error ﬂow is truncated once it “wants” to leave memory cells or gateunits. Therefore, no connection shown above serves to propagate error back tothe unit from which the connection originates (except for connections to outputunits), although the connections themselves are modiﬁable. That is why the trun-cated LSTM algorithm is so efﬁcient, despite its ability to bridge very long timelags. See the text and the appendix for details. Figure 2 shows the architectureused for experiment 6a; only the bias of the noninput'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 10}, page_content='to propagate error back tothe unit from which the connection originates (except for connections to outputunits), although the connections themselves are modiﬁable. That is why the trun-cated LSTM algorithm is so efﬁcient, despite its ability to bridge very long timelags. See the text and the appendix for details. Figure 2 shows the architectureused for experiment 6a; only the bias of the noninput units is omitted.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 10}, page_content='connections. In other words, the net can use injto decide when to keep or\\noverride information in memory cell cjand outjto decide when to access\\nmemory cell cjand when to prevent other units from being perturbed by cj\\n(see Figure 1).\\nError signals trapped within a memory cell’s CEC cannot change, but\\ndifferent error signals ﬂowing into the cell (at different times) via its out-put gate may get superimposed. The output gate will have to learn which'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 11}, page_content='1746 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nerrors to trap in its CEC by appropriately scaling them. The input gate will\\nhave to learn when to release errors, again by appropriately scaling them.Essentially the multiplicative gate units open and close access to constanterror ﬂow through CEC.\\nDistributed output representations typically do require output gates.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 11}, page_content='errors to trap in its CEC by appropriately scaling them. The input gate will\\nhave to learn when to release errors, again by appropriately scaling them.Essentially the multiplicative gate units open and close access to constanterror ﬂow through CEC.\\nDistributed output representations typically do require output gates.\\nBoth gate types are not always necessary, though; one may be sufﬁcient.For instance, in experiments 2a and 2b in section 5, it will be possible touse input gates only. In fact, output gates are not required in case of localoutput encoding; preventing memory cells from perturbing already learnedoutputs can be done by simply setting the corresponding weights to zero.Even in this case, however, output gates can be beneﬁcial: they prevent thenet’s attempts at storing long-time-lag memories (which are usually hard tolearn) from perturbing activations representing easily learnable short-time-lag memories. (This will prove quite useful in experiment 1, for instance.)'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 11}, page_content='4.3 Network Topology. We use networks with one input layer, one hid-\\nden layer, and one output layer. The (fully) self-connected hidden layercontains memory cells and corresponding gate units (for convenience, werefer to both memory cells and gate units as being located in the hiddenlayer). The hidden layer may also contain conventional hidden units pro-viding inputs to gate units and memory cells. All units (except for gate units)in all layers have directed connections (serve as inputs) to all units in thelayer above (or to all higher layers; see experiments 2a and 2b).\\n4.4 Memory Cell Blocks. Smemory cells sharing the same input gate\\nand the same output gate form a structure called a memory cell block of sizeS. Memory cell blocks facilitate information storage. As with conventional'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 11}, page_content='4.4 Memory Cell Blocks. Smemory cells sharing the same input gate\\nand the same output gate form a structure called a memory cell block of sizeS. Memory cell blocks facilitate information storage. As with conventional\\nneural nets, it is not so easy to code a distributed input within a single cell.Since each memory cell block has as many gate units as a single memory cell(namely, two), the block architecture can be even slightly more efﬁcient. Amemory cell block of size 1 is just a simple memory cell. In the experimentsin section 5, we will use memory cell blocks of various sizes.\\n4.5 Learning. We use a variant of RTRL (e.g., Robinson & Fallside, 1987)\\nthat takes into account the altered, multiplicative dynamics caused by inputand output gates. To ensure nondecaying error backpropagation throughinternal states of memory cells, as with truncated BPTT (e.g., Williams &Peng, 1990), errors arriving at memory cell net inputs (for cell c\\nj, this includes'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 11}, page_content='4.5 Learning. We use a variant of RTRL (e.g., Robinson & Fallside, 1987)\\nthat takes into account the altered, multiplicative dynamics caused by inputand output gates. To ensure nondecaying error backpropagation throughinternal states of memory cells, as with truncated BPTT (e.g., Williams &Peng, 1990), errors arriving at memory cell net inputs (for cell c\\nj, this includes\\nnetcj,netinj,netoutj) do not get propagated back further in time (although they\\ndo serve to change the incoming weights). Only within memory cells, are\\nerrors propagated back through previous internal states scj.2To visualize\\n2For intracellular backpropagation in a quite different context, see also Doya and\\nYoshizawa (1989).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 12}, page_content='Long Short-Term Memory 1747\\nthis, once an error signal arrives at a memory cell output, it gets scaled by\\noutput gate activation and h′. Then it is within the memory cell’s CEC, where\\nit can ﬂow back indeﬁnitely without ever being scaled. When it leaves thememory cell through the input gate and g, it is scaled once more by input\\ngate activation and g\\n′. It then serves to change the incoming weights before\\nit is truncated (see the appendix for formulas).\\n4.6 Computational Complexity. As with Mozer’s focused recurrent back-\\npropagation algorithm (Mozer, 1989), only the derivatives ∂scj/∂wilneed to\\nbe stored and updated. Hence the LSTM algorithm is very efﬁcient, withan excellent update complexity of O(W), where Wthe number of weights\\n(see details in the appendix). Hence, LSTM and BPTT for fully recurrentnets have the same update complexity per time step (while RTRL’s is muchworse). Unlike full BPTT, however, LSTM is local in space and time:\\n3there'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 12}, page_content='be stored and updated. Hence the LSTM algorithm is very efﬁcient, withan excellent update complexity of O(W), where Wthe number of weights\\n(see details in the appendix). Hence, LSTM and BPTT for fully recurrentnets have the same update complexity per time step (while RTRL’s is muchworse). Unlike full BPTT, however, LSTM is local in space and time:\\n3there\\nis no need to store activation values observed during sequence processingin a stack with potentially unlimited size.\\n4.7 Abuse Problem and Solutions. In the beginning of the learning'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 12}, page_content='phase, error reduction may be possible without storing information overtime. The network will thus tend to abuse memory cells, for example, asbias cells (it might make their activations constant and use the outgoingconnections as adaptive thresholds for other units). The potential difﬁcultyis that it may take a long time to release abused memory cells and makethem available for further learning. A similar “abuse problem” appears iftwo memory cells store the same (redundant) information. There are at leasttwo solutions to the abuse problem: (1) sequential network construction(e.g., Fahlman, 1991): a memory cell and the corresponding gate units areadded to the network whenever the error stops decreasing (see experiment 2in section 5), and (2) output gate bias: each output gate gets a negative initialbias, to push initial memory cell activations toward zero. Memory cells withmore negative bias automatically get “allocated” later (see experiments 1,3, 4, 5, and 6 in section 5).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 12}, page_content='4.8 Internal State Drift and Remedies. If memory cell c\\nj’s inputs are\\nmostly positive or mostly negative, then its internal state sjwill tend to drift\\naway over time. This is potentially dangerous, for the h′(sj)will then adopt\\nvery small values, and the gradient will vanish. One way to circumvent thisproblem is to choose an appropriate function h. But h(x)=x, for instance,\\nhas the disadvantage of unrestricted memory cell output range. Our simple\\n3Following Schmidhuber (1989), we say that a recurrent net algorithm is local in space if\\nthe update complexity per time step and weight does not depend on network size. We saythat a method is local in time if its storage requirements do not depend on input sequence\\nlength. For instance, RTRL is local in time but not in space. BPTT is local in space but notin time.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 13}, page_content='1748 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nbut effective way of solving drift problems at the beginning of learning is\\ninitially to bias the input gate injtoward zero. Although there is a trade-off\\nbetween the magnitudes of h′(sj)on the one hand and of yinjand f′\\ninjon the\\nother, the potential negative effect of input gate bias is negligible compared\\nto the one of the drifting effect. With logistic sigmoid activation functions,there appears to be no need for ﬁne-tuning the initial bias, as conﬁrmed byexperiments 4 and 5 in section 5.4.\\n5 Experiments\\nWhich tasks are appropriate to demonstrate the quality of a novel long-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 13}, page_content='5 Experiments\\nWhich tasks are appropriate to demonstrate the quality of a novel long-\\ntime-lag algorithm? First, minimal time lags between relevant input signalsand corresponding teacher signals must be long for all training sequences.In fact, many previous recurrent net algorithms sometimes manage to gen-eralize from very short training sequences to very long test sequences (see,e.g., Pollack, 1991). But a real long-time-lag problem does not have anyshort-time-lag exemplars in the training set. For instance, Elman’s trainingprocedure, BPTT, ofﬂine RTRL, online RTRL, and others fail miserably onreal long-time-lag problems. (See, e.g., Hochreiter, 1991; Mozer, 1992.) Asecond important requirement is that the tasks should be complex enoughsuch that they cannot be solved quickly by simple-minded strategies suchas random weight guessing.\\nRecently we discovered (Schmidhuber & Hochreiter, 1996; Hochreiter &'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 13}, page_content='Recently we discovered (Schmidhuber & Hochreiter, 1996; Hochreiter &\\nSchmidhuber, 1996, 1997) that many long-time-lag tasks used in previouswork can be solved more quickly by simple random weight guessing than bythe proposed algorithms. For instance, guessing solved a variant of Bengioand Frasconi’s parity problem (1994) much faster\\n4than the seven methods\\ntested by Bengio et al. (1994) and Bengio and Frasconi (1994). The same istrue for some of Miller and Giles’s problems (1993). Of course, this does notmean that guessing is a good algorithm. It just means that some previouslyused problems are not extremely appropriate to demonstrate the quality ofpreviously proposed algorithms.\\nAll our experiments (except experiment 1) involve long minimal time'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 13}, page_content='tested by Bengio et al. (1994) and Bengio and Frasconi (1994). The same istrue for some of Miller and Giles’s problems (1993). Of course, this does notmean that guessing is a good algorithm. It just means that some previouslyused problems are not extremely appropriate to demonstrate the quality ofpreviously proposed algorithms.\\nAll our experiments (except experiment 1) involve long minimal time\\nlags; there are no short-time-lag training exemplars facilitating learning.Solutions to most of our tasks are sparse in weight space. They require eithermany parameters and inputs or high weight precision, such that randomweight guessing becomes infeasible.\\nWe always use online learning (as opposed to batch learning) and logistic\\nsigmoids as activation functions. For experiments 1 and 2, initial weightsare chosen in the range [ −0.2,0.2], for the other experiments in [ −0.1,0.1].\\nTraining sequences are generated randomly according to the various task'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 13}, page_content='We always use online learning (as opposed to batch learning) and logistic\\nsigmoids as activation functions. For experiments 1 and 2, initial weightsare chosen in the range [ −0.2,0.2], for the other experiments in [ −0.1,0.1].\\nTraining sequences are generated randomly according to the various task\\n4Different input representations and different types of noise may lead to worse guess-\\ning performance (Yoshua Bengio, personal communication, 1996).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 14}, page_content='Long Short-Term Memory 1749\\ndescriptions. In slight deviation from the notation in appendix A.1, each\\ndiscrete time step of each input sequence involves three processing steps:(1) use current input to set the input units, (2) compute activations of hiddenunits (including input gates, output gates, memory cells), and (3) computeoutput unit activations. Except for experiments 1, 2a, and 2b, sequence ele-ments are randomly generated online, and error signals are generated onlyat sequence ends. Net activations are reset after each processed input se-quence.\\nFor comparisons with recurrent nets taught by gradient descent, we give'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 14}, page_content='For comparisons with recurrent nets taught by gradient descent, we give\\nresults only for RTRL, except for comparison 2a, which also includes BPTT.Note, however, that untruncated BPTT (see, e.g., Williams & Peng, 1990)computes exactly the same gradient as ofﬂine RTRL. With long-time-lagproblems, ofﬂine RTRL (or BPTT) and the online version of RTRL (no ac-tivation resets, online weight changes) lead to almost identical, negativeresults (as conﬁrmed by additional simulations in Hochreiter, 1991; see alsoMozer, 1992). This is because ofﬂine RTRL, online RTRL, and full BPTT allsuffer badly from exponential error decay.\\nOur LSTM architectures are selected quite arbitrarily. If nothing is known\\nabout the complexity of a given problem, a more systematic approach wouldbe to: start with a very small net consisting of one memory cell. If this doesnot work, try two cells, and so on. Alternatively, use sequential networkconstruction (e.g., Fahlman, 1991).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 14}, page_content='Our LSTM architectures are selected quite arbitrarily. If nothing is known\\nabout the complexity of a given problem, a more systematic approach wouldbe to: start with a very small net consisting of one memory cell. If this doesnot work, try two cells, and so on. Alternatively, use sequential networkconstruction (e.g., Fahlman, 1991).\\nFollowing is an outline of the experiments:\\n•Experiment 1 focuses on a standard benchmark test for recurrent nets:'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 14}, page_content='Following is an outline of the experiments:\\n•Experiment 1 focuses on a standard benchmark test for recurrent nets:\\nthe embedded Reber grammar. Since it allows for training sequenceswith short time lags, it is not a long-time-lag problem. We include itbecause it provides a nice example where LSTM’s output gates are trulybeneﬁcial, and it is a popular benchmark for recurrent nets that hasbeen used by many authors. We want to include at least one experimentwhere conventional BPTT and RTRL do not fail completely (LSTM,however, clearly outperforms them). The embedded Reber grammar’sminimal time lags represent a border case in the sense that it is stillpossible to learn to bridge them with conventional algorithms. Onlyslightly longer minimal time lags would make this almost impossible.The more interesting tasks in our article, however, are those that RTRL,BPTT, and others cannot solve at all.\\n•Experiment 2 focuses on noise-free and noisy sequences involving nu-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 14}, page_content='•Experiment 2 focuses on noise-free and noisy sequences involving nu-\\nmerous input symbols distracting from the few important ones. Themost difﬁcult task (task 2c) involves hundreds of distractor symbols atrandom positions and minimal time lags of 1000 steps. LSTM solves it;BPTT and RTRL already fail in case of 10-step minimal time lags (seealso Hochreiter, 1991; Mozer, 1992). For this reason RTRL and BPTTare omitted in the remaining, more complex experiments, all of whichinvolve much longer time lags.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 15}, page_content='1750 Sepp Hochreiter and J ¨ urgen Schmidhuber\\n•Experiment 3 addresses long-time-lag problems with noise and sig-\\nnal on the same input line. Experiments 3a and 3b focus on Bengio etal.’s 1994 two-sequence problem. Because this problem can be solvedquickly by random weight guessing, we also include a far more difﬁ-cult two-sequence problem (experiment 3c), which requires learningreal-valued, conditional expectations of noisy targets, given the inputs.\\n•Experiments 4 and 5 involve distributed, continuous-valued input rep-\\nresentations and require learning to store precise, real values for verylong time periods. Relevant input signals can occur at quite differentpositions in input sequences. Again minimal time lags involve hun-dreds of steps. Similar tasks never have been solved by other recurrentnet algorithms.\\n•Experiment 6 involves tasks of a different complex type that also has'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 15}, page_content='resentations and require learning to store precise, real values for verylong time periods. Relevant input signals can occur at quite differentpositions in input sequences. Again minimal time lags involve hun-dreds of steps. Similar tasks never have been solved by other recurrentnet algorithms.\\n•Experiment 6 involves tasks of a different complex type that also has\\nnot been solved by other recurrent net algorithms. Again, relevantinput signals can occur at quite different positions in input sequences.The experiment shows that LSTM can extract information conveyedby the temporal order of widely separated inputs.\\nSection 5.7 provides a detailed summary of experimental conditions in\\ntwo tables for reference.\\n5.1 Experiment 1: Embedded Reber Grammar.5.1.1 Task. Our ﬁrst task is to learn the embedded Reber grammar'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 15}, page_content='Section 5.7 provides a detailed summary of experimental conditions in\\ntwo tables for reference.\\n5.1 Experiment 1: Embedded Reber Grammar.5.1.1 Task. Our ﬁrst task is to learn the embedded Reber grammar\\n(Smith & Zipser, 1989; Cleeremans, Servan-Schreiber, & McClelland, 1989;Fahlman,1991). Since it allows for training sequences with short time lags(of as few as nine steps), it is not a long-time-lag problem. We include itfor two reasons: (1) it is a popular recurrent net benchmark used by manyauthors, and we wanted to have at least one experiment where RTRL andBPTT do not fail completely, and (2) it shows nicely how output gates canbe beneﬁcial.\\nStarting at the left-most node of the directed graph in Figure 3, sym-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 15}, page_content='Starting at the left-most node of the directed graph in Figure 3, sym-\\nbol strings are generated sequentially (beginning with the empty string)by following edges—and appending the associated symbols to the currentstring—until the right-most node is reached (the Reber grammar substringsare analogously generated from Figure 4). Edges are chosen randomly ifthere is a choice (probability: 0.5). The net’s task is to read strings, one sym-bol at a time, and to predict the next symbol (error signals occur at everytime step). To predict the symbol before last, the net has to remember thesecond symbol.\\n5.1.2 Comparison. We compare LSTM to Elman nets trained by Elman’s\\ntraining procedure (ELM) (results taken from Cleeremans et al., 1989), Fahl-man’s recurrent cascade-correlation (RCC) (results taken from Fahlman,'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 16}, page_content='Long Short-Term Memory 1751\\n \\nBTS\\nX\\nXP\\nV\\nTPVS\\nE\\nFigure 3: Transition diagram for the Reber grammar.\\n \\nBT\\nPET\\nPGRAMMAR\\nGRAMMARREBER\\nREBER\\nFigure 4: Transition diagram for the embedded Reber grammar. Each box rep-\\nresents a copy of the Reber grammar (see Figure 3).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 17}, page_content='1752 Sepp Hochreiter and J ¨ urgen Schmidhuber\\n1991), and RTRL (results taken from Smith & Zipser, 1989), where only the\\nfew successful trials are listed). Smith and Zipser actually make the taskeasier by increasing the probability of short-time-lag exemplars. We did notdo this for LSTM.\\n5.1.3 Training/Testing. We use a local input-output representation (seven'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 17}, page_content='1991), and RTRL (results taken from Smith & Zipser, 1989), where only the\\nfew successful trials are listed). Smith and Zipser actually make the taskeasier by increasing the probability of short-time-lag exemplars. We did notdo this for LSTM.\\n5.1.3 Training/Testing. We use a local input-output representation (seven\\ninput units, seven output units). Following Fahlman, we use 256 trainingstrings and 256 separate test strings. The training set is generated ran-domly; training exemplars are picked randomly from the training set. Testsequences are generated randomly, too, but sequences already used in thetraining set are not used for testing. After string presentation, all activationsare reinitialized with zeros. A trial is considered successful if all string sym-bols of all sequences in both test set and training set are predicted correctly—that is, if the output unit(s) corresponding to the possible next symbol(s)is(are) always the most active ones.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 17}, page_content='5.1.4 Architectures. Architectures for RTRL, ELM, and RCC are reported\\nin the references listed above. For LSTM, we use three (four) memory cellblocks. Each block has two (one) memory cells. The output layer’s onlyincoming connections originate at memory cells. Each memory cell andeach gate unit receives incoming connections from all memory cells andgate units (the hidden layer is fully connected; less connectivity may workas well). The input layer has forward connections to all units in the hiddenlayer. The gate units are biased. These architecture parameters make it easyto store at least three input signals (architectures 3-2 and 4-1 are employedto obtain comparable numbers of weights for both architectures: 264 for 4-1and 276 for 3-2). Other parameters may be appropriate as well, however. Allsigmoid functions are logistic with output range [0 ,1], except for h, whose\\nrange is [−1,1], and g, whose range is [−2,2]. All weights are initialized in'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 17}, page_content='range is [−1,1], and g, whose range is [−2,2]. All weights are initialized in\\n[−0.2,0.2], except for the output gate biases, which are initialized to −1,−2,\\nand−3, respectively (see abuse problem, solution 2 of section 4). We tried\\nlearning rates of 0.1, 0.2, and 0.5.\\n5.1.5 Results. We use three different, randomly generated pairs of train-\\ning and test sets. With each such pair we run 10 trials with different initialweights. See Table 1 for results (mean of 30 trials). Unlike the other methods,LSTM always learns to solve the task. Even when we ignore the unsuccessfultrials of the other approaches, LSTM learns much faster.\\n5.1.6 Importance of Output Gates. The experiment provides a nice exam-\\nple where the output gate is truly beneﬁcial. Learning to store the ﬁrst Tor P should not perturb activations representing the more easily learnabletransitions of the original Reber grammar. This is the job of the output gates.Without output gates, we did not achieve fast learning.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 18}, page_content='Long Short-Term Memory 1753\\nTable 1: Experiment 1: Embedded Reber Grammar.\\nNumber of Learning\\nMethod Hidden Units Weights Rate % of Success After\\nRTRL 3 ≈170 0.05 Some fraction 173,000\\nRTRL 12 ≈494 0.1 Some fraction 25,000\\nELM 15 ≈435 0 >200,000\\nRCC 7–9 ≈119–198 50 182,000\\nLSTM 4 blocks, size 1 264 0.1 100 39,740LSTM 3 blocks, size 2 276 0.1 100 21,730LSTM 3 blocks, size 2 276 0.2 97 14,060LSTM 4 blocks, size 1 264 0.5 97 9500LSTM 3 blocks, size 2 276 0.5 100 8440'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 18}, page_content='RTRL 12 ≈494 0.1 Some fraction 25,000\\nELM 15 ≈435 0 >200,000\\nRCC 7–9 ≈119–198 50 182,000\\nLSTM 4 blocks, size 1 264 0.1 100 39,740LSTM 3 blocks, size 2 276 0.1 100 21,730LSTM 3 blocks, size 2 276 0.2 97 14,060LSTM 4 blocks, size 1 264 0.5 97 9500LSTM 3 blocks, size 2 276 0.5 100 8440\\nNotes: Percentage of successful trials and number of sequence presentations until successfor RTRL (results taken from Smith & Zipser, 1989), Elman net trained by Elman’s pro-cedure (results taken from Cleeremans et al., 1989), recurrent cascade-correlation (resultstaken from Fahlman, 1991), and our new approach (LSTM). Weight numbers in the ﬁrstfour rows are estimates, the corresponding papers do not provide all the technical details.Only LSTM almost always learns to solve the task (only 2 failures out of 150 trials). Evenwhen we ignore the unsuccessful trials of the other approaches, LSTM learns much faster(the number of required training examples in the bottom row varies between 3800 and24,100).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 18}, page_content='5.2 Experiment 2: Noise-Free and Noisy Sequences.\\n5.2.1 Task 2a: Noise-Free Sequences with Long Time Lags. There are p+1\\npossible input symbols denoted a1,..., ap−1,ap=x,ap+1=y.aiis locally\\nrepresented by the p+1-dimensional vector whose ith component is 1 (all\\nother components are 0). A net with p+1 input units and p+1 output\\nunits sequentially observes input symbol sequences, one at a time, per-manently trying to predict the next symbol; error signals occur at everytime step. To emphasize the long-time-lag problem, we use a training setconsisting of only two very similar sequences: (y,a\\n1,a2,..., ap−1,y)and\\n(x,a1,a2,..., ap−1,x). Each is selected with probability 0.5. To predict the ﬁ-\\nnal element, the net has to learn to store a representation of the ﬁrst elementforptime steps.\\nWe compare real-time recurrent learning for fully recurrent nets (RTRL),'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 18}, page_content='1,a2,..., ap−1,y)and\\n(x,a1,a2,..., ap−1,x). Each is selected with probability 0.5. To predict the ﬁ-\\nnal element, the net has to learn to store a representation of the ﬁrst elementforptime steps.\\nWe compare real-time recurrent learning for fully recurrent nets (RTRL),\\nback-propagation through time (BPTT), the sometimes very successfultwo-net neural sequence chunker (CH; Schmidhuber, 1992b), and our newmethod (LSTM). In all cases, weights are initialized in [ −0.2,0.2]. Due to\\nlimited computation time, training is stopped after 5 million sequence pre-sentations. A successful run is one that fulﬁlls the following criterion: aftertraining, during 10,000 successive, randomly chosen input sequences, themaximal absolute error of all output units is always below 0 .25.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 19}, page_content='1754 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nTable 2: Task 2a: Percentage of Successful Trials and Number of Training Se-\\nquences until Success.\\nLearning Number of % Successful Success\\nMethod Delay p Rate Weights Trials After\\nRTRL 4 1.0 36 78 1,043,000\\nRTRL 4 4.0 36 56 892,000RTRL 4 10.0 36 22 254,000RTRL 10 1.0–10.0 144 0 >5,000,000\\nRTRL 100 1.0–10.0 10404 0 >5,000,000\\nBPTT 100 1.0–10.0 10404 0 >5,000,000\\nCH 100 1.0 10506 33 32,400\\nLSTM 100 1.0 10504 100 5,040\\nNotes: Table entries refer to means of 18 trials. With 100 time-step delays, only CH andLSTM achieve successful trials. Even when we ignore the unsuccessful trials of the otherapproaches, LSTM learns much faster.\\nArchitectures.\\nRTRL: One self-recurrent hidden unit, p+1 nonrecurrent output units.\\nEach layer has connections from all layers below. All units usethe logistic activation function sigmoid in [0 ,1].\\nBPTT: Same architecture as the one trained by RTRL.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 19}, page_content='Architectures.\\nRTRL: One self-recurrent hidden unit, p+1 nonrecurrent output units.\\nEach layer has connections from all layers below. All units usethe logistic activation function sigmoid in [0 ,1].\\nBPTT: Same architecture as the one trained by RTRL.\\nCH: Both net architectures like RTRL’s, but one has an additional out-\\nput for predicting the hidden unit of the other one (see Schmid-huber, 1992b, for details).\\nLSTM: As with RTRL, but the hidden unit is replaced by a memory cell\\nand an input gate (no output gate required). gis the logistic sig-\\nmoid, and his the identity function h:h(x)=x,∀x. Memory cell\\nand input gate are added once the error has stopped decreasing(see abuse problem: solution 1 in section 4).\\nResults. Using RTRL and a short four-time-step delay ( p=4), 7/9o fa l l\\ntrials were successful. No trial was successful with p=10. With long time'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 19}, page_content='and an input gate (no output gate required). gis the logistic sig-\\nmoid, and his the identity function h:h(x)=x,∀x. Memory cell\\nand input gate are added once the error has stopped decreasing(see abuse problem: solution 1 in section 4).\\nResults. Using RTRL and a short four-time-step delay ( p=4), 7/9o fa l l\\ntrials were successful. No trial was successful with p=10. With long time\\nlags, only the neural sequence chunker and LSTM achieved successful trials;BPTT and RTRL failed. With p=100, the two-net sequence chunker solved\\nthe task in only one-third of all trials. LSTM, however, always learned tosolve the task. Comparing successful trials only, LSTM learned much faster.See Table 2 for details. It should be mentioned, however, that a hierarchicalchunker can also always quickly solve this task (Schmidhuber, 1992c, 1993).\\n5.2.2 Task 2b: No Local Regularities. With task 2a, the chunker some-\\ntimes learns to predict the ﬁnal element correctly, but only because of pre-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 20}, page_content='Long Short-Term Memory 1755\\ndictable local regularities in the input stream that allow for compressing\\nthe sequence. In a more difﬁcult task, involving many more different pos-sible sequences, we remove compressibility by replacing the determin-istic subsequence (a\\n1,a2,..., ap−1)by a random subsequence (of length\\np−1) over the alphabet a1,a2,..., ap−1. We obtain two classes (two sets\\nof sequences){(y,ai1,ai2,..., aip−1,y)|1≤i1,i2,..., ip−1≤p−1}and\\n{(x,ai1,ai2,..., aip−1,x)|1≤i1,i2,..., ip−1≤p−1}. Again, every next se-\\nquence element has to be predicted. The only totally predictable targets,however, are xand y, which occur at sequence ends. Training exemplars\\nare chosen randomly from the two classes. Architectures and parametersare the same as in experiment 2a. A successful run is one that fulﬁlls thefollowing criterion: after training, during 10,000 successive, randomly cho-sen input sequences, the maximal absolute error of all output units is below0.25 at sequence end.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 20}, page_content='are chosen randomly from the two classes. Architectures and parametersare the same as in experiment 2a. A successful run is one that fulﬁlls thefollowing criterion: after training, during 10,000 successive, randomly cho-sen input sequences, the maximal absolute error of all output units is below0.25 at sequence end.\\nResults. As expected, the chunker failed to solve this task (so did BPTT\\nand RTRL, of course). LSTM, however, was always successful. On average(mean of 18 trials), success for p=100 was achieved after 5680 sequence\\npresentations. This demonstrates that LSTM does not require sequence reg-ularities to work well.\\n5.2.3 Task 2c: Very Long Time Lags—No Local Regularities. This is the\\nmost difﬁcult task in this subsection. To our knowledge, no other recur-rent net algorithm can solve it. Now there are p+4 possible input symbols\\ndenoted a\\n1,..., ap−1,ap,ap+1=e,ap+2=b,ap+3=x,ap+4=y.a1,..., ap\\nare also called distractor symbols. Again, aiis locally represented by the'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 20}, page_content='5.2.3 Task 2c: Very Long Time Lags—No Local Regularities. This is the\\nmost difﬁcult task in this subsection. To our knowledge, no other recur-rent net algorithm can solve it. Now there are p+4 possible input symbols\\ndenoted a\\n1,..., ap−1,ap,ap+1=e,ap+2=b,ap+3=x,ap+4=y.a1,..., ap\\nare also called distractor symbols. Again, aiis locally represented by the\\np+4-dimensional vector whose ith component is 1 (all other components\\nare 0). A net with p+4 input units and 2 output units sequentially ob-\\nserves input symbol sequences, one at a time. Training sequences are ran-domly chosen from the union of two very similar subsets of sequences:{(b,y,a\\ni1,ai2,..., aiq+k,e,y)|1≤i1,i2,..., iq+k≤q}and{(b,x,ai1,ai2,...,\\naiq+k,e,x)|1≤i1,i2,..., iq+k≤q}. To produce a training sequence, we\\nrandomly generate a sequence preﬁx of length q+2, randomly generate\\na sequence sufﬁx of additional elements ( ̸=b,e,x,y) with probability 9 /10'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 20}, page_content='i1,ai2,..., aiq+k,e,y)|1≤i1,i2,..., iq+k≤q}and{(b,x,ai1,ai2,...,\\naiq+k,e,x)|1≤i1,i2,..., iq+k≤q}. To produce a training sequence, we\\nrandomly generate a sequence preﬁx of length q+2, randomly generate\\na sequence sufﬁx of additional elements ( ̸=b,e,x,y) with probability 9 /10\\nor, alternatively, an ewith probability 1 /10. In the latter case, we conclude\\nthe sequence with xory, depending on the second element. For a given k,\\nthis leads to a uniform distribution on the possible sequences with lengthq+k+4. The minimal sequence length is q+4; the expected length is\\n4+∞∑\\nk=01\\n10(9\\n10)k\\n(q+k)=q+14.\\nThe expected number of occurrences of element ai,1≤i≤p, in a sequence\\nis(q+10)/p≈q\\np. The goal is to predict the last symbol, which always occurs\\nafter the “trigger symbol” e. Error signals are generated only at sequence'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 21}, page_content='1756 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nTable 3: Task 2c: LSTM with Very Long Minimal Time Lags q+1 and a Lot of\\nNoise.\\np(Number of Number of\\nq(Time Lag−1) Random Inputs)q\\npWeights Success After\\n50 50 1 364 30,000\\n100 100 1 664 31,000200 200 1 1264 33,000500 500 1 3064 38,000\\n1000 1,000 1 6064 49,0001000 500 2 3064 49,0001000 200 5 1264 75,0001000 100 10 664 135,0001000 50 20 364 203,000\\nNotes: pis the number of available distractor symbols ( p+4 is the number of input units).\\nq/pis the expected number of occurrences of a given distractor symbol in a sequence.\\nThe right-most column lists the number of training sequences required by LSTM (BPTT,RTRL, and the other competitors have no chance of solving this task). If we let the numberof distractor symbols (and weights) increase in proportion to the time lag, learning timeincreases very slowly. The lower block illustrates the expected slowdown due to increasedfrequency of distractor symbols.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 21}, page_content='The right-most column lists the number of training sequences required by LSTM (BPTT,RTRL, and the other competitors have no chance of solving this task). If we let the numberof distractor symbols (and weights) increase in proportion to the time lag, learning timeincreases very slowly. The lower block illustrates the expected slowdown due to increasedfrequency of distractor symbols.\\nends. To predict the ﬁnal element, the net has to learn to store a represen-\\ntation of the second element for at least q+1 time steps (until it sees the\\ntrigger symbol e). Success is deﬁned as prediction error (for ﬁnal sequence\\nelement) of both output units always below 0 .2, for 10,000 successive, ran-\\ndomly chosen input sequences.\\nArchitecture/Learning. The net has p+4 input units and 2 output units.\\nWeights are initialized in [ −0.2,0.2]. To avoid too much learning time vari-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 21}, page_content='trigger symbol e). Success is deﬁned as prediction error (for ﬁnal sequence\\nelement) of both output units always below 0 .2, for 10,000 successive, ran-\\ndomly chosen input sequences.\\nArchitecture/Learning. The net has p+4 input units and 2 output units.\\nWeights are initialized in [ −0.2,0.2]. To avoid too much learning time vari-\\nance due to different weight initializations, the hidden layer gets two mem-ory cells (two cell blocks of size 1, although one would be sufﬁcient). Thereare no other hidden units. The output layer receives connections only frommemory cells. Memory cells and gate units receive connections from inputunits, memory cells, and gate units (the hidden layer is fully connected).No bias weights are used. hand gare logistic sigmoids with output ranges\\n[−1,1] and [−2,2], respectively. The learning rate is 0.01. Note that the min-\\nimal time lag is q+1; the net never sees short training sequences facilitating\\nthe classiﬁcation of long test sequences.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 21}, page_content='[−1,1] and [−2,2], respectively. The learning rate is 0.01. Note that the min-\\nimal time lag is q+1; the net never sees short training sequences facilitating\\nthe classiﬁcation of long test sequences.\\nResults. Twenty trials were made for all tested pairs (p,q). Table 3 lists\\nthe mean of the number of training sequences required by LSTM to achievesuccess (BPTT and RTRL have no chance of solving nontrivial tasks withminimal time lags of 1000 steps).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 22}, page_content='Long Short-Term Memory 1757\\nScaling. Table 3 shows that if we let the number of input symbols (and\\nweights) increase in proportion to the time lag, learning time increases veryslowly. This is another remarkable property of LSTM not shared by anyother method we are aware of. Indeed, RTRL and BPTT are far from scalingreasonably; instead, they appear to scale exponentially and appear quiteuseless when the time lags exceed as few as 10 steps.\\nDistractor Inﬂuence. In Table 3, the column headed by q/pgives the ex-\\npected frequency of distractor symbols. Increasing this frequency decreaseslearning speed, an effect due to weight oscillations caused by frequentlyobserved input symbols.\\n5.3 Experiment 3: Noise and Signal on Same Channel. This experiment'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 22}, page_content='Distractor Inﬂuence. In Table 3, the column headed by q/pgives the ex-\\npected frequency of distractor symbols. Increasing this frequency decreaseslearning speed, an effect due to weight oscillations caused by frequentlyobserved input symbols.\\n5.3 Experiment 3: Noise and Signal on Same Channel. This experiment\\nserves to illustrate that LSTM does not encounter fundamental problems ifnoise and signal are mixed on the same input line. We initially focus onBengio et al.’s simple 1994 two-sequence problem. In experiment 3c wepose a more challenging two-sequence problem.\\n5.3.1 Task 3a (Two-Sequence Problem). The task is to observe and then\\nclassify input sequences. There are two classes, each occurring with proba-bility 0.5. There is only one input line. Only the ﬁrst Nreal-valued sequence\\nelements convey relevant information about the class. Sequence elementsat positions t>Nare generated by a gaussian with mean zero and variance'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 22}, page_content='5.3.1 Task 3a (Two-Sequence Problem). The task is to observe and then\\nclassify input sequences. There are two classes, each occurring with proba-bility 0.5. There is only one input line. Only the ﬁrst Nreal-valued sequence\\nelements convey relevant information about the class. Sequence elementsat positions t>Nare generated by a gaussian with mean zero and variance\\n0.2. Case N=1: the ﬁrst sequence element is 1.0 for class 1, and −1.0 for\\nclass 2. Case N=3: the ﬁrst three elements are 1.0 for class 1 and −1.0 for\\nclass 2. The target at the sequence end is 1.0 for class 1 and 0.0 for class 2.Correct classiﬁcation is deﬁned as absolute output error at sequence endbelow 0.2. Given a constant T, the sequence length is randomly selected\\nbetween Tand T+T/10 (a difference to Bengio et al.’s problem is that they\\nalso permit shorter sequences of length T/2).\\nGuessing. Bengio et al. (1994) and Bengio and Frasconi (1994) tested'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 22}, page_content='between Tand T+T/10 (a difference to Bengio et al.’s problem is that they\\nalso permit shorter sequences of length T/2).\\nGuessing. Bengio et al. (1994) and Bengio and Frasconi (1994) tested\\nseven different methods on the two-sequence problem. We discovered, how-ever, that random weight guessing easily outperforms them all because theproblem is so simple.\\n5See Schmidhuber and Hochreiter (1996) and Hochre-\\niter and Schmidhuber (1996, 1997) for additional results in this vein.\\nLSTM Architecture. We use a three-layer net with one input unit, one\\noutput unit, and three cell blocks of size 1. The output layer receives connec-tions only from memory cells. Memory cells and gate units receive inputsfrom input units, memory cells, and gate units and have bias weights. Gate\\n5However, different input representations and different types of noise may lead to\\nworse guessing performance (Yoshua Bengio, personal communication, 1996).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 23}, page_content='1758 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nTable 4: Task 3a: Bengio et al.’s Two-Sequence Problem.\\nNumber ST2: Fraction\\nT N Stop: ST1 Stop: ST2 of Weights Misclassiﬁed\\n100 3 27,380 39,850 102 0.000195\\n100 1 58,370 64,330 102 0.000117\\n1000 3 446,850 452,460 102 0.000078\\nNotes: Tis minimal sequence length. Nis the number of information-conveying\\nelements at sequence begin. The column headed by ST1 (ST2) gives the numberof sequence presentations required to achieve stopping criterion ST1 (ST2). Theright-most column lists the fraction of misclassiﬁed posttraining sequences (withabsolute error >0.2) from a test set consisting of 2560 sequences (tested after ST2\\nwas achieved). All values are means of 10 trials. We discovered, however, thatthis problem is so simple that random weight guessing solves it faster than LSTMand any other method for which there are published results.\\nunits and output unit are logistic sigmoid in [0 ,1],hin [−1,1], and gin\\n[−2,2].'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 23}, page_content='was achieved). All values are means of 10 trials. We discovered, however, thatthis problem is so simple that random weight guessing solves it faster than LSTMand any other method for which there are published results.\\nunits and output unit are logistic sigmoid in [0 ,1],hin [−1,1], and gin\\n[−2,2].\\nTraining/Testing. All weights (except the bias weights to gate units) are\\nrandomly initialized in the range [ −0.1,0.1]. The ﬁrst input gate bias is\\ninitialized with−1.0, the second with −3.0, and the third with −5.0. The\\nﬁrst output gate bias is initialized with −2.0, the second with−4.0, and the\\nthird with−6.0. The precise initialization values hardly matter though, as\\nconﬁrmed by additional experiments. The learning rate is 1.0. All activationsare reset to zero at the beginning of a new sequence.\\nWe stop training (and judge the task as being solved) according to the'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 23}, page_content='ﬁrst output gate bias is initialized with −2.0, the second with−4.0, and the\\nthird with−6.0. The precise initialization values hardly matter though, as\\nconﬁrmed by additional experiments. The learning rate is 1.0. All activationsare reset to zero at the beginning of a new sequence.\\nWe stop training (and judge the task as being solved) according to the\\nfollowing criteria: ST1: none of 256 sequences from a randomly chosen testset is misclassiﬁed; ST2: ST1 is satisﬁed, and mean absolute test set erroris below 0.01. In case of ST2, an additional test set consisting of 2560 ran-domly chosen sequences is used to determine the fraction of misclassiﬁedsequences.\\nResults. See Table 4. The results are means of 10 trials with different\\nweight initializations in the range [ −0.1,0.1]. LSTM is able to solve this prob-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 23}, page_content='Results. See Table 4. The results are means of 10 trials with different\\nweight initializations in the range [ −0.1,0.1]. LSTM is able to solve this prob-\\nlem, though by far not as fast as random weight guessing (see “Guessing”above). Clearly this trivial problem does not provide a very good testbedto compare performance of various nontrivial algorithms. Still, it demon-strates that LSTM does not encounter fundamental problems when facedwith signal and noise on the same channel.\\n5.3.2 Task 3b. The architecture, parameters, and other elements are as in\\ntask 3a, but now with gaussian noise (mean 0 and variance 0.2) added to the'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 24}, page_content='Long Short-Term Memory 1759\\nTable 5: Task 3b: Modiﬁed Two-Sequence Problem.\\nNumber ST2: Fraction\\nT N Stop: ST1 Stop: ST2 of Weights Misclassiﬁed\\n100 3 41,740 43,250 102 0.00828\\n100 1 74,950 78,430 102 0.01500\\n1000 1 481,060 485,080 102 0.01207\\nNote: Same as in Table 4, but now the information-conveying elements are alsoperturbed by noise.\\ninformation-conveying elements ( t<=N). We stop training (and judge the\\ntask as being solved) according to the following, slightly redeﬁned criteria:ST1: fewer than 6 out of 256 sequences from a randomly chosen test set aremisclassiﬁed; ST2: ST1 is satisﬁed, and mean absolute test set error is below0.04. In case of ST2, an additional test set consisting of 2560 randomly chosensequences is used to determine the fraction of misclassiﬁed sequences.\\nResults. See Table 5. The results represent means of 10 trials with differ-\\nent weight initializations. LSTM easily solves the problem.\\n5.3.3 Task 3c. The architecture, parameters, and other elements are as in'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 24}, page_content='Results. See Table 5. The results represent means of 10 trials with differ-\\nent weight initializations. LSTM easily solves the problem.\\n5.3.3 Task 3c. The architecture, parameters, and other elements are as in\\ntask 3a, but with a few essential changes that make the task nontrivial: thetargets are 0.2 and 0.8 for class 1 and class 2, respectively, and there is gaus-sian noise on the targets (mean 0 and variance 0.1; S.D. 0.32). To minimizemean squared error, the system has to learn the conditional expectations ofthe targets given the inputs. Misclassiﬁcation is deﬁned as absolute differ-ence between output and noise-free target (0.2 for class 1 and 0.8 for class2)>0.1. The network output is considered acceptable if the mean absolute\\ndifference between noise-free target and output is below 0.015. Since thisrequires high weight precision, task 3c (unlike tasks 3a and 3b) cannot besolved quickly by random guessing.\\nTraining/Testing. The learning rate is 0 .1. We stop training according to'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 24}, page_content='difference between noise-free target and output is below 0.015. Since thisrequires high weight precision, task 3c (unlike tasks 3a and 3b) cannot besolved quickly by random guessing.\\nTraining/Testing. The learning rate is 0 .1. We stop training according to\\nthe following criterion: none of 256 sequences from a randomly chosen testset is misclassiﬁed, and mean absolute difference between the noise-freetarget and output is below 0.015. An additional test set consisting of 2560randomly chosen sequences is used to determine the fraction of misclassi-ﬁed sequences.\\nResults. See Table 6. The results represent means of 10 trials with dif-\\nferent weight initializations. Despite the noisy targets, LSTM still can solvethe problem by learning the expected target values.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 25}, page_content='1760 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nTable 6: Task 3c: Modiﬁed, More Challenging Two-Sequence Problem.\\nNumber Fraction Average Difference\\nT N Stop of Weights Misclassiﬁed to Mean\\n100 3 269,650 102 0.00558 0.014\\n100 1 565,640 102 0.00441 0.012\\nNotes: Same as in Table 4, but with noisy real-valued targets. The system has to learn theconditional expectations of the targets given the inputs. The right-most column providesthe average difference between network output and expected target. Unlike tasks 3a and3b, this one cannot be solved quickly by random weight guessing.\\n5.4 Experiment 4: Adding Problem. The difﬁcult task in this section is\\nof a type that has never been solved by other recurrent net algorithms. Itshows that LSTM can solve long-time-lag problems involving distributed,continuous-valued representations.\\n5.4.1 Task. Each element of each input sequence is a pair of compo-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 25}, page_content='5.4 Experiment 4: Adding Problem. The difﬁcult task in this section is\\nof a type that has never been solved by other recurrent net algorithms. Itshows that LSTM can solve long-time-lag problems involving distributed,continuous-valued representations.\\n5.4.1 Task. Each element of each input sequence is a pair of compo-\\nnents. The ﬁrst component is a real value randomly chosen from the interval[−1,1]; the second is 1.0, 0.0, or −1.0 and is used as a marker. At the end of\\neach sequence, the task is to output the sum of the ﬁrst components of thosepairs that are marked by second components equal to 1.0. Sequences haverandom lengths between the minimal sequence length Tand T+T/10. In a\\ngiven sequence, exactly two pairs are marked, as follows: we ﬁrst randomlyselect and mark one of the ﬁrst 10 pairs (whose ﬁrst component we call X\\n1).\\nThen we randomly select and mark one of the ﬁrst T/2−1 still unmarked\\npairs (whose ﬁrst component we call X2). The second components of all'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 25}, page_content='given sequence, exactly two pairs are marked, as follows: we ﬁrst randomlyselect and mark one of the ﬁrst 10 pairs (whose ﬁrst component we call X\\n1).\\nThen we randomly select and mark one of the ﬁrst T/2−1 still unmarked\\npairs (whose ﬁrst component we call X2). The second components of all\\nremaining pairs are zero except for the ﬁrst and ﬁnal pair, whose secondcomponents are−1. (In the rare case where the ﬁrst pair of the sequence\\ngets marked, we set X\\n1to zero.) An error signal is generated only at the\\nsequence end: the target is 0 .5+(X1+X2)/4.0 (the sum X1+X2scaled to\\nthe interval [0 ,1]). A sequence is processed correctly if the absolute error at\\nthe sequence end is below 0.04.\\n5.4.2 Architecture. We use a three-layer net with two input units, one'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 25}, page_content='gets marked, we set X\\n1to zero.) An error signal is generated only at the\\nsequence end: the target is 0 .5+(X1+X2)/4.0 (the sum X1+X2scaled to\\nthe interval [0 ,1]). A sequence is processed correctly if the absolute error at\\nthe sequence end is below 0.04.\\n5.4.2 Architecture. We use a three-layer net with two input units, one\\noutput unit, and two cell blocks of size 2. The output layer receives connec-tions only from memory cells. Memory cells and gate units receive inputsfrom memory cells and gate units (the hidden layer is fully connected; lessconnectivity may work as well). The input layer has forward connectionsto all units in the hidden layer. All noninput units have bias weights. Thesearchitecture parameters make it easy to store at least two input signals (acell block size of 1 works well, too). All activation functions are logistic withoutput range [0 ,1], except for h, whose range is [−1,1], and g, whose range\\nis [−2,2].'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 26}, page_content='Long Short-Term Memory 1761\\nTable 7: Experiment 4: Results for the Adding Problem.\\nNumber of Number of\\nT Minimal Lag Weights Wrong Predictions Success After\\n100 50 93 1 out of 2560 74,000\\n500 250 93 0 out of 2560 209,000\\n1000 500 93 1 out of 2560 853,000\\nNotes: Tis the minimal sequence length, T/2 the minimal time lag. “Number of Wrong\\nPredictions” is the number of incorrectly processed sequences (error >0.04) from a test set\\ncontaining 2560 sequences. The right-most column gives the number of training sequencesrequired to achieve the stopping criterion. All values are means of 10 trials. For T=1000\\nthe number of required training examples varies between 370,000 and 2,020,000, exceeding700,000 in only three cases.\\n5.4.3 State Drift Versus Initial Bias. Note that the task requires storing'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 26}, page_content='containing 2560 sequences. The right-most column gives the number of training sequencesrequired to achieve the stopping criterion. All values are means of 10 trials. For T=1000\\nthe number of required training examples varies between 370,000 and 2,020,000, exceeding700,000 in only three cases.\\n5.4.3 State Drift Versus Initial Bias. Note that the task requires storing\\nthe precise values of real numbers for long durations; the system must learnto protect memory cell contents against even minor internal state drift (seesection 4). To study the signiﬁcance of the drift problem, we make the taskeven more difﬁcult by biasing all noninput units, thus artiﬁcially inducinginternal state drift. All weights (including the bias weights) are randomlyinitialized in the range [ −0.1,0.1]. Following section 4’s remedy for state\\ndrifts, the ﬁrst input gate bias is initialized with −3.0 and the second with\\n−6.0 (though the precise values hardly matter, as conﬁrmed by additional\\nexperiments).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 26}, page_content='drifts, the ﬁrst input gate bias is initialized with −3.0 and the second with\\n−6.0 (though the precise values hardly matter, as conﬁrmed by additional\\nexperiments).\\n5.4.4 Training/Testing. The learning rate is 0.5. Training is stopped once\\nthe average training error is below 0.01, and the 2000 most recent sequenceswere processed correctly.\\n5.4.5 Results. With a test set consisting of 2560 randomly chosen se-\\nquences, the average test set error was always below 0.01, and there werenever more than three incorrectly processed sequences. Table 7 shows de-tails.\\nThe experiment demonstrates that LSTM is able to work well with dis-\\ntributed representations, LSTM is able to learn to perform calculations in-volving continuous values, and since the system manages to store continu-ous values without deterioration for minimal delays of T/2 time steps, there\\nis no signiﬁcant, harmful internal state drift.\\n5.5 Experiment 5: Multiplication Problem. One may argue that LSTM'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 26}, page_content='tributed representations, LSTM is able to learn to perform calculations in-volving continuous values, and since the system manages to store continu-ous values without deterioration for minimal delays of T/2 time steps, there\\nis no signiﬁcant, harmful internal state drift.\\n5.5 Experiment 5: Multiplication Problem. One may argue that LSTM\\nis a bit biased toward tasks such as the adding problem from the previoussubsection. Solutions to the adding problem may exploit the CEC’s built-inintegration capabilities. Although this CEC property may be viewed as a'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 27}, page_content='1762 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nTable 8: Experiment 5: Results for the Multiplication Problem.\\nMinimal Number of Number of Success\\nT Lag Weights nseq Wrong Predictions MSE After\\n100 50 93 140 139 out of 2560 0.0223 482,000\\n100 50 93 13 14 out of 2560 0.0139 1,273,000\\nNotes: Tis the minimal sequence length and T/2 the minimal time lag. We test on a test\\nset containing 2560 sequences as soon as less than nseqof the 2000 most recent training\\nsequences lead to error >0.04. “Number of Wrong Predictions” is the number of test\\nsequences with error >0.04. MSE is the mean squared error on the test set. The right-most\\ncolumn lists numbers of training sequences required to achieve the stopping criterion. Allvalues are means of 10 trials.\\nfeature rather than a disadvantage (integration seems to be a natural subtask'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 27}, page_content='sequences lead to error >0.04. “Number of Wrong Predictions” is the number of test\\nsequences with error >0.04. MSE is the mean squared error on the test set. The right-most\\ncolumn lists numbers of training sequences required to achieve the stopping criterion. Allvalues are means of 10 trials.\\nfeature rather than a disadvantage (integration seems to be a natural subtask\\nof many tasks occurring in the real world), the question arises whether LSTMcan also solve tasks with inherently nonintegrative solutions. To test this,we change the problem by requiring the ﬁnal target to equal the product(instead of the sum) of earlier marked inputs.\\n5.5.1 Task. This is like the task in section 5.4, except that the ﬁrst com-\\nponent of each pair is a real value randomly chosen from the interval [0 ,1].\\nIn the rare case where the ﬁrst pair of the input sequence gets marked, wesetX\\n1to 1.0. The target at sequence end is the product X1×X2.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 27}, page_content='5.5.1 Task. This is like the task in section 5.4, except that the ﬁrst com-\\nponent of each pair is a real value randomly chosen from the interval [0 ,1].\\nIn the rare case where the ﬁrst pair of the input sequence gets marked, wesetX\\n1to 1.0. The target at sequence end is the product X1×X2.\\n5.5.2 Architecture. This is as in section 5.4. All weights (including the\\nbias weights) are randomly initialized in the range [ −0.1,0.1].\\n5.5.3 Training/Testing. The learning rate is 0.1. We test performance twice:\\nas soon as less than nseqof the 2000 most recent training sequences lead to\\nabsolute errors exceeding 0.04, where nseq=140 and nseq=13. Why these\\nvalues? nseq=140 is sufﬁcient to learn storage of the relevant inputs. It is not\\nenough though to ﬁne-tune the precise ﬁnal outputs. nseq=13, however,\\nleads to quite satisfactory results.\\n5.5.4 Results. Fornseq=140 ( nseq=13) with a test set consisting of 2560'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 27}, page_content='absolute errors exceeding 0.04, where nseq=140 and nseq=13. Why these\\nvalues? nseq=140 is sufﬁcient to learn storage of the relevant inputs. It is not\\nenough though to ﬁne-tune the precise ﬁnal outputs. nseq=13, however,\\nleads to quite satisfactory results.\\n5.5.4 Results. Fornseq=140 ( nseq=13) with a test set consisting of 2560\\nrandomly chosen sequences, the average test set error was always below0.026 (0.013), and there were never more than 170 (15) incorrectly processedsequences. Table 8 shows details. (A net with additional standard hiddenunits or with a hidden layer above the memory cells may learn the ﬁne-tuning part more quickly.)\\nThe experiment demonstrates that LSTM can solve tasks involving both\\ncontinuous-valued representations and nonintegrative information process-ing.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 28}, page_content='Long Short-Term Memory 1763\\n5.6 Experiment 6: Temporal Order. In this subsection, LSTM solves\\nother difﬁcult (but artiﬁcial) tasks that have never been solved by previ-ous recurrent net algorithms. The experiment shows that LSTM is able toextract information conveyed by the temporal order of widely separatedinputs.\\n5.6.1 Task 6a: Two Relevant, Widely Separated Symbols. The goal is to clas-\\nsify sequences. Elements and targets are represented locally (input vec-tors with only one nonzero bit). The sequence starts with an E, ends with\\naB(the “trigger symbol”), and otherwise consists of randomly chosen\\nsymbols from the set {a,b,c,d}except for two elements at positions t\\n1\\nand t2that are either XorY. The sequence length is randomly chosen\\nbetween 100 and 110, t1is randomly chosen between 10 and 20, and t2\\nis randomly chosen between 50 and 60. There are four sequence classes\\nQ,R,S,U, which depend on the temporal order of Xand Y. The rules are:\\nX,X→Q;X,Y→R;Y,X→S;Y,Y→U.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 28}, page_content='symbols from the set {a,b,c,d}except for two elements at positions t\\n1\\nand t2that are either XorY. The sequence length is randomly chosen\\nbetween 100 and 110, t1is randomly chosen between 10 and 20, and t2\\nis randomly chosen between 50 and 60. There are four sequence classes\\nQ,R,S,U, which depend on the temporal order of Xand Y. The rules are:\\nX,X→Q;X,Y→R;Y,X→S;Y,Y→U.\\n5.6.2 Task 6b: Three Relevant, Widely Separated Symbols. Again, the goal\\nis to classify sequences. Elements and targets are represented locally. Thesequence starts with an E, ends with a B(the trigger symbol), and otherwise\\nconsists of randomly chosen symbols from the set {a,b,c,d}except for three\\nelements at positions t\\n1,t2, and t3that are either XorY. The sequence\\nlength is randomly chosen between 100 and 110, t1is randomly chosen\\nbetween 10 and 20, t2is randomly chosen between 33 and 43, and t3is'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 28}, page_content='consists of randomly chosen symbols from the set {a,b,c,d}except for three\\nelements at positions t\\n1,t2, and t3that are either XorY. The sequence\\nlength is randomly chosen between 100 and 110, t1is randomly chosen\\nbetween 10 and 20, t2is randomly chosen between 33 and 43, and t3is\\nrandomly chosen between 66 and 76. There are eight sequence classes—Q,R,S,U,V,A,B,C—which depend on the temporal order of the Xs and\\nYs. The rules are: X,X,X→Q;X,X,Y→R;X,Y,X→S;X,Y,Y→\\nU;Y,X,X→V;Y,X,Y→A;Y,Y,X→B;Y,Y,Y→C.\\nThere are as many output units as there are classes. Each class is locally\\nrepresented by a binary target vector with one nonzero component. Withboth tasks, error signals occur only at the end of a sequence. The sequenceis classiﬁed correctly if the ﬁnal absolute error of all output units is below0.3.\\nArchitecture. We use a three-layer net with eight input units, two (three)'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 28}, page_content='There are as many output units as there are classes. Each class is locally\\nrepresented by a binary target vector with one nonzero component. Withboth tasks, error signals occur only at the end of a sequence. The sequenceis classiﬁed correctly if the ﬁnal absolute error of all output units is below0.3.\\nArchitecture. We use a three-layer net with eight input units, two (three)\\ncell blocks of size 2, and four (eight) output units for task 6a (6b). Again allnoninput units have bias weights, and the output layer receives connectionsfrom memory cells, only. Memory cells and gate units receive inputs frominput units, memory cells, and gate units (the hidden layer is fully con-nected; less connectivity may work as well). The architecture parametersfor task 6a (6b) make it easy to store at least two (three) input signals. Allactivation functions are logistic with output range [0 ,1], except for h, whose\\nrange is [−1,1], and g, whose range is [−2,2].'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 29}, page_content='1764 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nTable 9: Experiment 6: Results for the Temporal Order Problem.\\nNumber of Number of\\nTask Weights Wrong Predictions Success After\\nTask 6a 156 1 out of 2560 31,390\\nTask 6b 308 2 out of 2560 571,100\\nNotes: “Number of Wrong Predictions” is the number of incorrectlyclassiﬁed sequences (error >0.3 for at least one output unit) from a test\\nset containing 2560 sequences. The right-most column gives the numberof training sequences required to achieve the stopping criterion. Theresults for task 6a are means of 20 trials; those for task 6b of 10 trials.\\nTraining/Testing. The learning rate is 0.5 (0.1) for experiment 6a (6b).\\nTraining is stopped once the average training error falls below 0.1 and the2000 most recent sequences were classiﬁed correctly. All weights are initial-ized in the range [−0.1,0.1]. The ﬁrst input gate bias is initialized with −2.0,\\nthe second with−4.0, and (for experiment 6b) the third with −6.0 (again, we'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 29}, page_content='Training/Testing. The learning rate is 0.5 (0.1) for experiment 6a (6b).\\nTraining is stopped once the average training error falls below 0.1 and the2000 most recent sequences were classiﬁed correctly. All weights are initial-ized in the range [−0.1,0.1]. The ﬁrst input gate bias is initialized with −2.0,\\nthe second with−4.0, and (for experiment 6b) the third with −6.0 (again, we\\nconﬁrmed by additional experiments that the precise values hardly matter).\\nResults. With a test set consisting of 2560 randomly chosen sequences,\\nthe average test set error was always below 0.1, and there were never morethan three incorrectly classiﬁed sequences. Table 9 shows details.\\nThe experiment shows that LSTM is able to extract information conveyed\\nby the temporal order of widely separated inputs. In task 6a, for instance,the delays between the ﬁrst and second relevant input and between thesecond relevant input and sequence end are at least 30 time steps.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 29}, page_content='The experiment shows that LSTM is able to extract information conveyed\\nby the temporal order of widely separated inputs. In task 6a, for instance,the delays between the ﬁrst and second relevant input and between thesecond relevant input and sequence end are at least 30 time steps.\\nTypical Solutions. In experiment 6a, how does LSTM distinguish be-\\ntween temporal orders (X,Y)and(Y,X)? One of many possible solutions\\nis to store the ﬁrst XorYin cell block 1 and the second X/Yin cell block 2.\\nBefore the ﬁrst X/Yoccurs, block 1 can see that it is still empty by means\\nof its recurrent connections. After the ﬁrst X/Y, block 1 can close its input\\ngate. Once block 1 is ﬁlled and closed, this fact will become visible to block2 (recall that all gate units and all memory cells receive connections from allnonoutput units).\\nTypical solutions, however, require only one memory cell block. The\\nblock stores the ﬁrst XorY; once the second X/Yoccurs, it changes its'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 29}, page_content='of its recurrent connections. After the ﬁrst X/Y, block 1 can close its input\\ngate. Once block 1 is ﬁlled and closed, this fact will become visible to block2 (recall that all gate units and all memory cells receive connections from allnonoutput units).\\nTypical solutions, however, require only one memory cell block. The\\nblock stores the ﬁrst XorY; once the second X/Yoccurs, it changes its\\nstate depending on the ﬁrst stored symbol. Solution type 1 exploits theconnection between memory cell output and input gate unit. The followingevents cause different input gate activations: Xoccurs in conjunction with\\na ﬁlled block; Xoccurs in conjunction with an empty block. Solution type 2\\nis based on a strong, positive connection between memory cell output andmemory cell input. The previous occurrence of X(Y) is represented by a'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 30}, page_content='Long Short-Term Memory 1765\\nTable 10: Summary of Experimental Conditions for LSTM, Part I.\\n(1) (2) (3) (4) (5) (6) (7) (8) (9) (10) (11) (12) (13) (14) (15)\\nTask p lag b s in out w c ogb igb bias h g α\\n1-1 9 9 4 1 7 7 264 F −1,−2,−3,−4 r ga h1 g2 0.1\\n1-2 9 9 3 2 7 7 276 F −1,−2,−3 r ga h1 g2 0.1\\n1-3 9 9 3 2 7 7 276 F −1,−2,−3 r ga h1 g2 0.2\\n1-4 9 9 4 1 7 7 264 F −1,−2,−3,−4 r ga h1 g2 0.5\\n1-5 9 9 3 2 7 7 276 F −1,−2,−3 r ga h1 g2 0.5\\n2a 100 100 1 1 101 101 10,504 B No og None None id g1 1.0\\n2b 100 100 1 1 101 101 10,504 B No og None None id g1 1.0'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 30}, page_content='Task p lag b s in out w c ogb igb bias h g α\\n1-1 9 9 4 1 7 7 264 F −1,−2,−3,−4 r ga h1 g2 0.1\\n1-2 9 9 3 2 7 7 276 F −1,−2,−3 r ga h1 g2 0.1\\n1-3 9 9 3 2 7 7 276 F −1,−2,−3 r ga h1 g2 0.2\\n1-4 9 9 4 1 7 7 264 F −1,−2,−3,−4 r ga h1 g2 0.5\\n1-5 9 9 3 2 7 7 276 F −1,−2,−3 r ga h1 g2 0.5\\n2a 100 100 1 1 101 101 10,504 B No og None None id g1 1.0\\n2b 100 100 1 1 101 101 10,504 B No og None None id g1 1.0\\n2c-1 50 50 2 1 54 2 364 F None None None h1 g2 0.012c-2 100 100 2 1 104 2 664 F None None None h1 g2 0.012c-3 200 200 2 1 204 2 1264 F None None None h1 g2 0.012c-4 500 500 2 1 504 2 3064 F None None None h1 g2 0.012c-5 1000 1000 2 1 1004 2 6064 F None None None h1 g2 0.012c-6 1000 1000 2 1 504 2 3064 F None None None h1 g2 0.012c-7 1000 1000 2 1 204 2 1264 F None None None h1 g2 0.012c-8 1000 1000 2 1 104 2 664 F None None None h1 g2 0.012c-9 1000 1000 2 1 54 2 364 F None None None h1 g2 0.01\\n3a 100 100 3 1 1 1 102 F −2,−4,−6−1,−3,−5 b1 h1 g2 1.0'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 30}, page_content='3a 100 100 3 1 1 1 102 F −2,−4,−6−1,−3,−5 b1 h1 g2 1.0\\n3b 100 100 3 1 1 1 102 F −2,−4,−6−1,−3,−5 b1 h1 g2 1.0\\n3c 100 100 3 1 1 1 102 F −2,−4,−6−1,−3,−5 b1 h1 g2 0.1\\n4-1 100 50 2 2 2 1 93 F r −3,−6 All h1 g2 0.5\\n4-2 500 250 2 2 2 1 93 F r −3,−6 All h1 g2 0.5\\n4-3 1000 500 2 2 2 1 93 F r −3,−6 All h1 g2 0.5\\n5 100 50 2 2 2 1 93 F r r All h1 g2 0.1\\n6a 100 40 2 2 8 4 156 F r −2,−4 All h1 g2 0.5\\n6b 100 24 3 2 8 8 308 F r −2,−4,−6 All h1 g2 0.1\\nNotes: Col. 1: task number. Col. 2: minimal sequence length p. Col. 3: minimal number of\\nsteps between most recent relevant input information and teacher signal. Col. 4: numberof cell blocks b. Col. 5: block size s. Col. 6: Number of input units in. Col. 7: Number of\\noutput units out. Col. 8: number of weights w. Col. 9: cdescribes connectivity: Fmeans\\n“output layer receives connections from memory cells; memory cells and gate units receiveconnections from input units, memory cells and gate units”; Bmeans “each layer receives'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 30}, page_content='output units out. Col. 8: number of weights w. Col. 9: cdescribes connectivity: Fmeans\\n“output layer receives connections from memory cells; memory cells and gate units receiveconnections from input units, memory cells and gate units”; Bmeans “each layer receives\\nconnections from all layers below.” Col. 10: Initial output gate bias ogb, where rstands for\\n“randomly chosen from the interval [ −0.1,0.1]” and no og means “no output gate used.”\\nCol. 11: initial input gate bias igb(see Col. 10). Col. 12: which units have bias weights?\\nb1stands for “all hidden units”, gafor “only gate units,” and allfor “all noninput units.”\\nCol. 13: the function h, where idis identity function, h1is logistic sigmoid in [ −2,2]. Col. 14:\\nthe logistic function g, where g1 is sigmoid in [0 ,1],g2i n[−1,1]. Col. 15: learning rate α.\\npositive (negative) internal state. Once the input gate opens for the second'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 30}, page_content='b1stands for “all hidden units”, gafor “only gate units,” and allfor “all noninput units.”\\nCol. 13: the function h, where idis identity function, h1is logistic sigmoid in [ −2,2]. Col. 14:\\nthe logistic function g, where g1 is sigmoid in [0 ,1],g2i n[−1,1]. Col. 15: learning rate α.\\npositive (negative) internal state. Once the input gate opens for the second\\ntime, so does the output gate, and the memory cell output is fed back toits own input. This causes (X,Y)to be represented by a positive internal\\nstate, because Xcontributes to the new internal state twice (via current\\ninternal state and cell output feedback). Similarly, (Y,X)gets represented\\nby a negative internal state.\\n5.7 Summary of Experimental Conditions. Tables 10 and 11 provide an\\noverview of the most important LSTM parameters and architectural detailsfor experiments 1 through 6. The conditions of the simple experiments 2a'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 31}, page_content='1766 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nTable 11: Summary of Experimental Conditions for LSTM, Part II.\\n(1) (2) (3) (4) (5) (6)\\nTask Select Interval Test Set Size Stopping Criterion Success\\n1t 1 [−0.2,0.2] 256 Training and test correctly pred. See text\\n2a t1 [−0.2,0.2] no test set After 5 million exemplars ABS(0.25)\\n2b t2 [−0.2,0.2] 10,000 After 5 million exemplars ABS(0.25)\\n2c t2 [−0.2,0.2] 10,000 After 5 million exemplars ABS(0.2)\\n3a t3 [−0.1,0.1] 2560 ST1 and ST2 (see text) ABS(0.2)\\n3b t3 [−0.1,0.1] 2560 ST1 and ST2 (see text) ABS(0.2)\\n3c t3 [−0.1,0.1] 2560 ST1 and ST2 (see text) See text\\n4t 3 [−0.1,0.1] 2560 ST3(0.01) ABS(0.04)\\n5t 3 [−0.1,0.1] 2560 see text ABS(0.04)\\n6a t3 [−0.1,0.1] 2560 ST3(0.1) ABS(0.3)\\n6b t3 [−0.1,0.1] 2560 ST3(0.1) ABS(0.3)\\nNotes: Col. 1: task number. Col. 2: training exemplar selelction, where t1 stands for “ran-\\ndomly chosen form training set,” t2 for “randomly chosen from two classes,” and t3 for'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 31}, page_content='3c t3 [−0.1,0.1] 2560 ST1 and ST2 (see text) See text\\n4t 3 [−0.1,0.1] 2560 ST3(0.01) ABS(0.04)\\n5t 3 [−0.1,0.1] 2560 see text ABS(0.04)\\n6a t3 [−0.1,0.1] 2560 ST3(0.1) ABS(0.3)\\n6b t3 [−0.1,0.1] 2560 ST3(0.1) ABS(0.3)\\nNotes: Col. 1: task number. Col. 2: training exemplar selelction, where t1 stands for “ran-\\ndomly chosen form training set,” t2 for “randomly chosen from two classes,” and t3 for\\n“randomly generated on line.” Col. 3: weight initialization interval. Col. 4: test set size.Col. 5: Stopping criterion for training, where ST3(β)stands for “average training error\\nbelowβand the 2000 most recent sequences were processed correctly.” Col. 6: success\\n(correct classiﬁcation) criterion, where ABS(β)stands for “absolute error of all output\\nunits at sequence end is below β.”\\nand 2b differ slightly from those of the other, more systematic experiments,\\ndue to historical reasons.\\n6 Discussion\\n6.1 Limitations of LSTM.\\n•The particularly efﬁcient truncated backpropagation version of the'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 31}, page_content='(correct classiﬁcation) criterion, where ABS(β)stands for “absolute error of all output\\nunits at sequence end is below β.”\\nand 2b differ slightly from those of the other, more systematic experiments,\\ndue to historical reasons.\\n6 Discussion\\n6.1 Limitations of LSTM.\\n•The particularly efﬁcient truncated backpropagation version of the\\nLSTM algorithm will not easily solve problems similar to stronglydelayed XOR problems, where the goal is to compute the XOR oftwo widely separated inputs that previously occurred somewhere ina noisy sequence. The reason is that storing only one of the inputs willnot help to reduce the expected error; the task is nondecomposable inthe sense that it is impossible to reduce the error incrementally by ﬁrstsolving an easier subgoal.\\nIn theory, this limitation can be circumvented by using the full gra-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 31}, page_content='In theory, this limitation can be circumvented by using the full gra-\\ndient (perhaps with additional conventional hidden units receivinginput from the memory cells). But we do not recommend computingthe full gradient for the following reasons: (1) It increases computa-tional complexity, (2) constant error ﬂow through CECs can be shownonly for truncated LSTM, and (3) we actually did conduct a few exper-iments with nontruncated LSTM. There was no signiﬁcant differenceto truncated LSTM, exactly because outside the CECs, error ﬂow tends'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 32}, page_content='Long Short-Term Memory 1767\\nto vanish quickly. For the same reason, full BPTT does not outperform\\ntruncated BPTT.\\n•Each memory cell block needs two additional units (input and output\\ngate). In comparison to standard recurrent nets, however, this doesnot increase the number of weights by more than a factor of 9: eachconventional hidden unit is replaced by at most three units in theLSTM architecture, increasing the number of weights by a factor of 3\\n2\\nin the fully connected case. Note, however, that our experiments usequite comparable weight numbers for the architectures of LSTM andcompeting approaches.\\n•Due to its constant error ﬂow through CECs within memory cells,'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 32}, page_content='2\\nin the fully connected case. Note, however, that our experiments usequite comparable weight numbers for the architectures of LSTM andcompeting approaches.\\n•Due to its constant error ﬂow through CECs within memory cells,\\nLSTM generally runs into problems similar to those of feedforwardnets’ seeing the entire input string at once. For instance, there are tasksthat can be quickly solved by random weight guessing but not by thetruncated LSTM algorithm with small weight initializations, such asthe 500-step parity problem (see the introduction to section 5). Here,LSTM’s problems are similar to the ones of a feedforward net with 500inputs, trying to solve 500-bit parity. Indeed LSTM typically behavesmuch like a feedforward net trained by backpropagation that sees theentire input. But that is also precisely why it so clearly outperformsprevious approaches on many nontrivial tasks with signiﬁcant searchspaces.\\n•LSTM does not have any problems with the notion of recency that'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 32}, page_content='•LSTM does not have any problems with the notion of recency that\\ngo beyond those of other approaches. All gradient-based approaches,however, suffer from a practical inability to count discrete time stepsprecisely. If it makes a difference whether a certain signal occurred99 or 100 steps ago, then an additional counting mechanism seemsnecessary. Easier tasks, however, such as one that requires making adifference only between, say, 3 and 11 steps, do not pose any problemsto LSTM. For instance, by generating an appropriate negative con-nection between memory cell output and input, LSTM can give moreweight to recent inputs and learn decays where necessary.\\n6.2 Advantages of LSTM.\\n•The constant error backpropagation within memory cells results in\\nLSTM’s ability to bridge very long time lags in case of problems similarto those discussed above.\\n•For long-time-lag problems such as those discussed in this article,'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 32}, page_content='6.2 Advantages of LSTM.\\n•The constant error backpropagation within memory cells results in\\nLSTM’s ability to bridge very long time lags in case of problems similarto those discussed above.\\n•For long-time-lag problems such as those discussed in this article,\\nLSTM can handle noise, distributed representations, and continuousvalues. In contrast to ﬁnite state automata or hidden Markov models,LSTM does not require an a priori choice of a ﬁnite number of states.In principle, it can deal with unlimited state numbers.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 33}, page_content='1768 Sepp Hochreiter and J ¨ urgen Schmidhuber\\n•For problems discussed in this article, LSTM generalizes well, even\\nif the positions of widely separated, relevant inputs in the input se-quence do not matter. Unlike previous approaches, ours quickly learnsto distinguish between two or more widely separated occurrences ofa particular element in an input sequence, without depending on ap-propriate short-time-lag training exemplars.\\n•There appears to be no need for parameter ﬁne tuning. LSTM works\\nwell over a broad range of parameters such as learning rate, input gatebias, and output gate bias. For instance, to some readers the learn-ing rates used in our experiments may seem large. However, a largelearning rate pushes the output gates toward zero, thus automaticallycountermanding its own negative effects.\\n•The LSTM algorithm’s update complexity per weight and time step is\\nessentially that of BPTT, namely, O(1). This is excellent in comparison'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 33}, page_content='•The LSTM algorithm’s update complexity per weight and time step is\\nessentially that of BPTT, namely, O(1). This is excellent in comparison\\nto other approaches such as RTRL. Unlike full BPTT, however, LSTMis local in both space and time.\\n7 Conclusion\\nEach memory cell’s internal architecture guarantees constant error ﬂow\\nwithin its CEC, provided that truncated backpropagation cuts off error ﬂowtrying to leak out of memory cells. This represents the basis for bridgingvery long time lags. Two gate units learn to open and close access to errorﬂow within each memory cell’s CEC. The multiplicative input gate affordsprotection of the CEC from perturbation by irrelevant inputs. Similarly,the multiplicative output gate protects other units from perturbation bycurrently irrelevant memory contents.\\nTo ﬁnd out about LSTM’s practical limitations we intend to apply it to'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 33}, page_content='To ﬁnd out about LSTM’s practical limitations we intend to apply it to\\nreal-world data. Application areas will include time-series prediction, musiccomposition, and speech processing. It will also be interesting to augmentsequence chunkers (Schmidhuber, 1992b, 1993) by LSTM to combine theadvantages of both.\\nAppendix\\nA.1 Algorithm Details. In what follows, the index kranges over output\\nunits, iranges over hidden units, cjstands for the jth memory cell block, cv\\nj\\ndenotes the vth unit of memory cell block cj,u,l,mstand for arbitrary units,\\nand tranges over all time steps of a given input sequence.\\nThe gate unit logistic sigmoid (with range [0 ,1]) used in the experiments\\nis\\nf(x)=1\\n1+exp(−x). (A.1)'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 34}, page_content='Long Short-Term Memory 1769\\nThe function h(with range [−1,1]) used in the experiments is\\nh(x)=2\\n1+exp(−x)−1 . (A.2)\\nThe function g(with range [−2,2]) used in the experiments is\\ng(x)=4\\n1+exp(−x)−2 . (A.3)\\nA.1.1 Forward Pass. The net input and the activation of hidden unit i\\nare\\nneti(t)=∑\\nuwiuyu(t−1) (A.4)\\nyi(t)=fi(neti(t)).\\nThe net input and the activation of injare\\nnetinj(t)=∑\\nuwinjuyu(t−1) (A.5)\\nyinj(t)=finj(netinj(t)).\\nThe net input and the activation of outjare\\nnetoutj(t)=∑\\nuwoutjuyu(t−1) (A.6)\\nyoutj(t)=foutj(netoutj(t)).\\nThe net input netcv\\nj, the internal state scv\\nj, and the output activation ycv\\njof\\nthevth memory cell of memory cell block cjare:\\nnetcv\\nj(t)=∑\\nuwcv\\njuyu(t−1) (A.7)\\nscv\\nj(t)=scv\\nj(t−1)+yinj(t)g(\\nnetcv\\nj(t))\\nycv\\nj(t)=youtj(t)h(scv\\nj(t)).\\nThe net input and the activation of output unit kare\\nnetk(t)=∑\\nu:unot a gatewkuyu(t−1)\\nyk(t)=fk(netk(t)).\\nThe backward pass to be described later is based on the following trun-\\ncated backpropagation formulas.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 35}, page_content='1770 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nA.1.2 Approximate Derivatives for Truncated Backpropagation. The trun-\\ncated version (see section 4) only approximates the partial derivatives,which is reﬂected by the ≈\\ntrsigns in the notation below. It truncates er-\\nror ﬂow once it leaves memory cells or gate units. Truncation ensures thatthere are no loops across which an error that left some memory cell throughits input or input gate can reenter the cell through its output or output gate.This in turn ensures constant error ﬂow through the memory cell’s CEC.\\nIn the truncated backpropagation version, the following derivatives are\\nreplaced by zero:\\n∂net\\ninj(t)\\n∂yu(t−1)≈tr0∀u,\\n∂netoutj(t)\\n∂yu(t−1)≈tr0∀u,\\nand\\n∂netcj(t)\\n∂yu(t−1)≈tr0∀u.\\nTherefore we get\\n∂yinj(t)\\n∂yu(t−1)=f′\\ninj(netinj(t))∂netinj(t)\\n∂yu(t−1)≈tr0∀u,\\n∂youtj(t)\\n∂yu(t−1)=f′\\noutj(netoutj(t))∂netoutj(t)\\n∂yu(t−1)≈tr0∀u,\\nand\\n∂ycj(t)\\n∂yu(t−1)=∂ycj(t)\\n∂netoutj(t)∂netoutj(t)\\n∂yu(t−1)+∂ycj(t)\\n∂netinj(t)∂netinj(t)\\n∂yu(t−1)\\n+∂ycj(t)'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 35}, page_content='replaced by zero:\\n∂net\\ninj(t)\\n∂yu(t−1)≈tr0∀u,\\n∂netoutj(t)\\n∂yu(t−1)≈tr0∀u,\\nand\\n∂netcj(t)\\n∂yu(t−1)≈tr0∀u.\\nTherefore we get\\n∂yinj(t)\\n∂yu(t−1)=f′\\ninj(netinj(t))∂netinj(t)\\n∂yu(t−1)≈tr0∀u,\\n∂youtj(t)\\n∂yu(t−1)=f′\\noutj(netoutj(t))∂netoutj(t)\\n∂yu(t−1)≈tr0∀u,\\nand\\n∂ycj(t)\\n∂yu(t−1)=∂ycj(t)\\n∂netoutj(t)∂netoutj(t)\\n∂yu(t−1)+∂ycj(t)\\n∂netinj(t)∂netinj(t)\\n∂yu(t−1)\\n+∂ycj(t)\\n∂netcj(t)∂netcj(t)\\n∂yu(t−1)≈tr0∀u.\\nThis implies for all wlmnot on connections to cv\\nj,inj,outj(that is, l̸∈{cv\\nj,inj,outj}):\\n∂ycv\\nj(t)\\n∂wlm=∑\\nu∂ycv\\nj(t)\\n∂yu(t−1)∂yu(t−1)\\n∂wlm≈tr0.\\nThe truncated derivatives of output unit kare:\\n∂yk(t)\\n∂wlm=f′\\nk(netk(t))(∑\\nu:unot a gatewku∂yu(t−1)\\n∂wlm+δklym(t−1))'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 36}, page_content='Long Short-Term Memory 1771\\n≈trf′\\nk(netk(t))\\uf8eb\\n\\uf8ed∑\\njSj∑\\nv=1δcv\\njlwkcv\\nj∂ycv\\nj(t−1)\\n∂wlm\\n+∑\\nj(\\nδinjl+δoutjl)Sj∑\\nv=1wkcv\\nj∂ycv\\nj(t−1)\\n∂wlm\\n+∑\\ni:ihidden unitwki∂yi(t−1)\\n∂wlm+δklym(t−1))\\n=f′\\nk(netk(t))\\uf8f1\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f4\\uf8f3y\\nm(t−1) l=k\\nwkcv\\nj∂ycv\\nj(t−1)\\n∂wlml=cv\\nj\\n∑Sj\\nv=1wkcv\\nj∂ycv\\nj(t−1)\\n∂wlml=injORl=outj∑\\ni:ihidden unit wki∂yi(t−1)\\n∂wlmlotherwise\\n(A.8)\\nwhereδis the Kronecker delta ( δab=1i fa=band 0 otherwise), and Sjis\\nthe size of memory cell block cj. The truncated derivatives of a hidden unit\\nithat is not part of a memory cell are:\\n∂yi(t)\\n∂wlm=f′\\ni(neti(t))∂neti(t)\\n∂wlm≈trδlif′\\ni(neti(t))ym(t−1). (A.9)\\n(Here it would be possible to use the full gradient without affecting constant\\nerror ﬂow through internal states of memory cells.)\\nCell block cj’s truncated derivatives are:\\n∂yinj(t)\\n∂wlm=f′\\ninj(netinj(t))∂netinj(t)\\n∂wlm\\n≈trδinjlf′\\ninj(netinj(t))ym(t−1). (A.10)\\n∂youtj(t)\\n∂wlm=f′\\noutj(netoutj(t))∂netoutj(t)\\n∂wlm\\n≈trδoutjlf′\\noutj(netoutj(t))ym(t−1). (A.11)\\n∂scv\\nj(t)\\n∂wlm=∂scv\\nj(t−1)\\n∂wlm\\n+∂yinj(t)'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 36}, page_content='(Here it would be possible to use the full gradient without affecting constant\\nerror ﬂow through internal states of memory cells.)\\nCell block cj’s truncated derivatives are:\\n∂yinj(t)\\n∂wlm=f′\\ninj(netinj(t))∂netinj(t)\\n∂wlm\\n≈trδinjlf′\\ninj(netinj(t))ym(t−1). (A.10)\\n∂youtj(t)\\n∂wlm=f′\\noutj(netoutj(t))∂netoutj(t)\\n∂wlm\\n≈trδoutjlf′\\noutj(netoutj(t))ym(t−1). (A.11)\\n∂scv\\nj(t)\\n∂wlm=∂scv\\nj(t−1)\\n∂wlm\\n+∂yinj(t)\\n∂wlmg(\\nnetcv\\nj(t))\\n+yinj(t)g′(\\nnetcv\\nj(t))∂netcv\\nj(t)\\n∂wlm\\n≈tr(\\nδinjl+δcv\\njl)∂scv\\nj(t−1)\\n∂wlm+δinjl∂yinj(t)\\n∂wlmg(\\nnetcv\\nj(t))\\n+δcv\\njlyinj(t)g′(\\nnetcv\\nj(t))∂netcv\\nj(t)\\n∂wlm'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 37}, page_content='1772 Sepp Hochreiter and J ¨ urgen Schmidhuber\\n=(\\nδinjl+δcv\\njl)∂scv\\nj(t−1)\\n∂wlm\\n+δinjlf′\\ninj(netinj(t))g(\\nnetcv\\nj(t))\\nym(t−1)\\n+δcv\\njlyinj(t)g′(\\nnetcv\\nj(t))\\nym(t−1). (A.12)\\n∂ycv\\nj(t)\\n∂wlm=∂youtj(t)\\n∂wlmh(scv\\nj(t))+h′(scv\\nj(t))∂scv\\nj(t)\\n∂wlmyoutj(t)\\n≈trδoutjl∂youtj(t)\\n∂wlmh(scv\\nj(t))\\n+(\\nδinjl+δcv\\njl)\\nh′(scv\\nj(t))∂scv\\nj(t)\\n∂wlmyoutj(t). (A.13)\\nTo update the system efﬁciently at time t, the only (truncated) derivatives\\nthat need to be stored at time t−1a r e\\n∂scv\\nj(t−1)\\n∂wlm,\\nwhere l=cv\\njorl=inj.\\nA.1.3 Backward Pass. We will describe the backward pass only for the\\nparticularly efﬁcient truncated gradient version of the LSTM algorithm. Forsimplicity we will use equal signs even where approximations are madeaccording to the truncated backpropagation equations above.\\nThe squared error at time tis given by\\nE(t)=∑\\nk:koutput unit(\\ntk(t)−yk(t))2\\n, (A.14)\\nwhere tk(t)is output unit k’s target at time t.\\nTime t’s contribution to wlm’s gradient-based update with learning rate\\nαis\\n1wlm(t)=−α∂E(t)'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 37}, page_content='The squared error at time tis given by\\nE(t)=∑\\nk:koutput unit(\\ntk(t)−yk(t))2\\n, (A.14)\\nwhere tk(t)is output unit k’s target at time t.\\nTime t’s contribution to wlm’s gradient-based update with learning rate\\nαis\\n1wlm(t)=−α∂E(t)\\n∂wlm. (A.15)\\nWe deﬁne some unit l’s error at time step tby\\nel(t):=−∂E(t)\\n∂netl(t). (A.16)\\nUsing (almost) standard backpropagation, we ﬁrst compute updates for\\nweights to output units ( l=k), weights to hidden units ( l=i) and weights'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 38}, page_content='Long Short-Term Memory 1773\\nto output gates ( l=outj). We obtain (compare formulas A.8, A.9, and A.11):\\nl=k(output) : ek(t)=f′\\nk(netk(t))(\\ntk(t)−yk(t))\\n, (A.17)\\nl=i(hidden) : ei(t)=f′\\ni(neti(t))∑\\nk:koutput unitwkiek(t), (A.18)\\nl=outj(output gates) :\\neoutj(t)=f′\\noutj(netoutj(t))\\uf8eb\\n\\uf8edSj∑\\nv=1h(scv\\nj(t))∑\\nk:koutput unitwkcv\\njek(t)\\uf8f6\\n\\uf8f8. (A.19)\\nFor all possible ltime t’s contribution to wlm’s update is\\n1wlm(t)=αel(t)ym(t−1). (A.20)\\nThe remaining updates for weights to input gates ( l=inj) and to cell\\nunits ( l=cv\\nj) are less conventional. We deﬁne some internal state scv\\nj’s error:\\nescv\\nj:=−∂E(t)\\n∂scv\\nj(t)\\n=foutj(netoutj(t))h′(scv\\nj(t))∑\\nk:koutput unitwkcv\\njek(t). (A.21)\\nWe obtain for l=injorl=cv\\nj,v=1,..., Sj\\n−∂E(t)\\n∂wlm=Sj∑\\nv=1escv\\nj(t)∂scv\\nj(t)\\n∂wlm. (A.22)\\nThe derivatives of the internal states with respect to weights and the\\ncorresponding weight updates are as follows (compare expression A.12):\\nl=inj(input gates) :\\n∂scv\\nj(t)\\n∂winjm=∂scv\\nj(t−1)\\n∂winjm+g(netcv\\nj(t))f′'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 38}, page_content='escv\\nj:=−∂E(t)\\n∂scv\\nj(t)\\n=foutj(netoutj(t))h′(scv\\nj(t))∑\\nk:koutput unitwkcv\\njek(t). (A.21)\\nWe obtain for l=injorl=cv\\nj,v=1,..., Sj\\n−∂E(t)\\n∂wlm=Sj∑\\nv=1escv\\nj(t)∂scv\\nj(t)\\n∂wlm. (A.22)\\nThe derivatives of the internal states with respect to weights and the\\ncorresponding weight updates are as follows (compare expression A.12):\\nl=inj(input gates) :\\n∂scv\\nj(t)\\n∂winjm=∂scv\\nj(t−1)\\n∂winjm+g(netcv\\nj(t))f′\\ninj(netinj(t))ym(t−1); (A.23)\\ntherefore, time t’s contribution to winjm’s update is (compare expression A.8):\\n1winjm(t)=αSj∑\\nv=1escv\\nj(t)∂scv\\nj(t)\\n∂winjm. (A.24)'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 39}, page_content='1774 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nSimilarly we get (compare expression A.12):\\nl=cv\\nj(memory cells) :\\n∂scv\\nj(t)\\n∂wcv\\njm=∂scv\\nj(t−1)\\n∂wcv\\njm+g′(netcv\\nj(t))finj(netinj(t))ym(t−1); (A.25)\\ntherefore time t’s contribution to wcv\\njm’s update is (compare expression A.8):\\n1wcv\\njm(t)=αescv\\nj(t)∂scv\\nj(t)\\n∂wcv\\njm. (A.26)\\nAll we need to implement for the backward pass are equations A.17 through\\nA.21 and A.23 through A.26. Each weight’s total update is the sum of thecontributions of all time steps.\\nA.1.4 Computational Complexity. LSTM’s update complexity per time\\nstep is\\nO(KH+KCS+HI+CSI)=O(W), (A.27)\\nwhere Kis the number of output units, Cis the number of memory cell\\nblocks, S>0 is the size of the memory cell blocks, His the number of hidden\\nunits, Iis the (maximal) number of units forward connected to memory cells,\\ngate units and hidden units, and\\nW=KH+KCS+CSI+2CI+HI=O(KH+KCS+CSI+HI)\\nis the number of weights. Expression A.27 is obtained by considering all'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 39}, page_content='O(KH+KCS+HI+CSI)=O(W), (A.27)\\nwhere Kis the number of output units, Cis the number of memory cell\\nblocks, S>0 is the size of the memory cell blocks, His the number of hidden\\nunits, Iis the (maximal) number of units forward connected to memory cells,\\ngate units and hidden units, and\\nW=KH+KCS+CSI+2CI+HI=O(KH+KCS+CSI+HI)\\nis the number of weights. Expression A.27 is obtained by considering all\\ncomputations of the backward pass: equation A.17 needs Ksteps; A.18 needs\\nKHsteps; A.19 needs KSC steps; A.20 needs K(H+C)steps for output units,\\nHIsteps for hidden units, CIsteps for output gates; A.21 needs KCS steps;\\nA.23 needs CSIsteps; A.24 needs CSIsteps; A.25 needs CSIsteps; A.26 needs\\nCSI steps. The total is K+2KH+KC+2KSC+HI+CI+4CSI steps, or\\nO(KH+KSC+HI+CSI)steps. We conclude that LSTM algorithm’s update\\ncomplexity per time step is just like BPTT’s for a fully recurrent net.\\nAt a given time step, only the 2 CSI most recent ∂s\\ncv\\nj/∂wlmvalues from'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 39}, page_content='A.23 needs CSIsteps; A.24 needs CSIsteps; A.25 needs CSIsteps; A.26 needs\\nCSI steps. The total is K+2KH+KC+2KSC+HI+CI+4CSI steps, or\\nO(KH+KSC+HI+CSI)steps. We conclude that LSTM algorithm’s update\\ncomplexity per time step is just like BPTT’s for a fully recurrent net.\\nAt a given time step, only the 2 CSI most recent ∂s\\ncv\\nj/∂wlmvalues from\\nequations A.23 and A.25 need to be stored. Hence LSTM’s storage complex-\\nity also is O(W); it does not depend on the input sequence length.\\nA.2 Error Flow. We compute how much an error signal is scaled while\\nﬂowing back through a memory cell for qtime steps. As a by-product, this\\nanalysis reconﬁrms that the error ﬂow within a memory cell’s CEC is indeedconstant, provided that truncated backpropagation cuts off error ﬂow tryingto leave memory cells (see also section 3.2). The analysis also highlights a'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 40}, page_content='Long Short-Term Memory 1775\\npotential for undesirable long-term drifts of scj, as well as the beneﬁcial,\\ncountermanding inﬂuence of negatively biased input gates.\\nUsing the truncated backpropagation learning rule, we obtain\\n∂scj(t−k)\\n∂scj(t−k−1)=1+∂yinj(t−k)\\n∂scj(t−k−1)g(\\nnetcj(t−k))\\n+yinj(t−k)g′(\\nnetcj(t−k))∂netcj(t−k)\\n∂scj(t−k−1)\\n=1+∑\\nu[\\n∂yinj(t−k)\\n∂yu(t−k−1)∂yu(t−k−1)\\n∂scj(t−k−1)]\\n×g(\\nnetcj(t−k))\\n+yinj(t−k)g′(\\nnetcj(t−k))\\n×∑\\nu[\\n∂netcj(t−k)\\n∂yu(t−k−1)∂yu(t−k−1)\\n∂scj(t−k−1)]\\n≈tr1. (A.28)\\nThe≈trsign indicates equality due to the fact that truncated backpropaga-\\ntion replaces by zero the following derivatives:\\n∂yinj(t−k)\\n∂yu(t−k−1)∀uand∂netcj(t−k)\\n∂yu(t−k−1)∀u.\\nIn what follows, an error ϑj(t)starts ﬂowing back at cj’s output. We re-\\ndeﬁne\\nϑj(t):=∑\\niwicjϑi(t+1). (A.29)\\nFollowing the deﬁnitions and conventions of section 3.1, we compute\\nerror ﬂow for the truncated backpropagation learning rule. The error occur-ring at the output gate is\\nϑ\\noutj(t)≈tr∂youtj(t)\\n∂netoutj(t)∂ycj(t)'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 40}, page_content='∂yinj(t−k)\\n∂yu(t−k−1)∀uand∂netcj(t−k)\\n∂yu(t−k−1)∀u.\\nIn what follows, an error ϑj(t)starts ﬂowing back at cj’s output. We re-\\ndeﬁne\\nϑj(t):=∑\\niwicjϑi(t+1). (A.29)\\nFollowing the deﬁnitions and conventions of section 3.1, we compute\\nerror ﬂow for the truncated backpropagation learning rule. The error occur-ring at the output gate is\\nϑ\\noutj(t)≈tr∂youtj(t)\\n∂netoutj(t)∂ycj(t)\\n∂youtj(t)ϑj(t). (A.30)\\nThe error occurring at the internal state is\\nϑscj(t)=∂scj(t+1)\\n∂scj(t)ϑscj(t+1)+∂ycj(t)\\n∂scj(t)ϑj(t). (A.31)\\nSince we use truncated backpropagation we have\\nϑj(t)=∑\\ni:ino gate and no memory cellwicjϑi(t+1);'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 41}, page_content='1776 Sepp Hochreiter and J ¨ urgen Schmidhuber\\ntherefore we get\\n∂ϑj(t)\\n∂ϑscj(t+1)=∑\\niwicj∂ϑi(t+1)\\n∂ϑscj(t+1)≈tr0 . (A.32)\\nEquations A.31 and A.32 imply constant error ﬂow through internal\\nstates of memory cells:\\n∂ϑscj(t)\\n∂ϑscj(t+1)=∂scj(t+1)\\n∂scj(t)≈tr1 . (A.33)\\nThe error occurring at the memory cell input is\\nϑcj(t)=∂g(netcj(t))\\n∂netcj(t)∂scj(t)\\n∂g(netcj(t))ϑscj(t). (A.34)\\nThe error occurring at the input gate is\\nϑinj(t)≈tr∂yinj(t)\\n∂netinj(t)∂scj(t)\\n∂yinj(t))ϑscj(t). (A.35)\\nA.2.1 No External Error Flow. Errors are propagated back from units lto\\nunit valong outgoing connections with weights wlv. This “external error”\\n(note that for conventional units there is nothing but external error) at timetis\\nϑ\\ne\\nv(t)=∂yv(t)\\n∂netv(t)∑\\nl∂netl(t+1)\\n∂yv(t)ϑl(t+1). (A.36)\\nWe obtain\\n∂ϑe\\nv(t−1)\\n∂ϑj(t)=∂yv(t−1)\\n∂netv(t−1)(∂ϑoutj(t)\\n∂ϑj(t)∂netoutj(t)\\n∂yv(t−1)\\n+∂ϑinj(t)\\n∂ϑj(t)∂netinj(t)\\n∂yv(t−1)+∂ϑcj(t)\\n∂ϑj(t)∂netcj(t)\\n∂yv(t−1))\\n≈tr0 . (A.37)'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 41}, page_content='unit valong outgoing connections with weights wlv. This “external error”\\n(note that for conventional units there is nothing but external error) at timetis\\nϑ\\ne\\nv(t)=∂yv(t)\\n∂netv(t)∑\\nl∂netl(t+1)\\n∂yv(t)ϑl(t+1). (A.36)\\nWe obtain\\n∂ϑe\\nv(t−1)\\n∂ϑj(t)=∂yv(t−1)\\n∂netv(t−1)(∂ϑoutj(t)\\n∂ϑj(t)∂netoutj(t)\\n∂yv(t−1)\\n+∂ϑinj(t)\\n∂ϑj(t)∂netinj(t)\\n∂yv(t−1)+∂ϑcj(t)\\n∂ϑj(t)∂netcj(t)\\n∂yv(t−1))\\n≈tr0 . (A.37)\\nWe observe that the error ϑjarriving at the memory cell output is not back-\\npropagated to units vby external connections to inj,outj,cj.\\nA.2.2 Error Flow Within Memory Cells. We now focus on the error back-\\nﬂow within a memory cell’s CEC. This is actually the only type of error ﬂowthat can bridge several time steps. Suppose error ϑ\\nj(t)arrives at cj’s output'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 42}, page_content='Long Short-Term Memory 1777\\nat time tand is propagated back for qsteps until it reaches injor the memory\\ncell input g(netcj). It is scaled by a factor of\\n∂ϑv(t−q)\\n∂ϑj(t),\\nwhere v=inj,cj. We ﬁrst compute\\n∂ϑscj(t−q)\\n∂ϑj(t)≈tr\\uf8f1\\n\\uf8f4\\uf8f2\\n\\uf8f4\\uf8f3∂ycj(t)\\n∂scj(t)q=0\\n∂scj(t−q+1)\\n∂scj(t−q)∂ϑscj(t−q+1)\\n∂ϑj(t)q>0. (A.38)\\nExpanding equation A.38, we obtain\\n∂ϑv(t−q)\\n∂ϑj(t)≈tr∂ϑv(t−q)\\n∂ϑscj(t−q)∂ϑscj(t−q)\\n∂ϑj(t)\\n≈tr∂ϑv(t−q)\\n∂ϑscj(t−q)(1∏\\nm=q∂scj(t−m+1)\\n∂scj(t−m))\\n∂ycj(t)\\n∂scj(t)\\n≈tryoutj(t)h′(scj(t)){\\ng′(netcj(t−q)yinj(t−q) v=cj\\ng(netcj(t−q)f′\\ninj(netinj(t−q))v=inj.(A.39)\\nConsider the factors in the previous equation’s last expression. Obvi-\\nously, error ﬂow is scaled only at times t(when it enters the cell) and t−q\\n(when it leaves the cell), but not in between (constant error ﬂow throughthe CEC). We observe:\\n1. The output gate’s effect is y\\noutj(t)scales down those errors that can be'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 42}, page_content='g′(netcj(t−q)yinj(t−q) v=cj\\ng(netcj(t−q)f′\\ninj(netinj(t−q))v=inj.(A.39)\\nConsider the factors in the previous equation’s last expression. Obvi-\\nously, error ﬂow is scaled only at times t(when it enters the cell) and t−q\\n(when it leaves the cell), but not in between (constant error ﬂow throughthe CEC). We observe:\\n1. The output gate’s effect is y\\noutj(t)scales down those errors that can be\\nreduced early during training without using the memory cell. It alsoscales down those errors resulting from using (activating/deactivating)the memory cell at later training stages. Without the output gate, thememory cell might, for instance, suddenly start causing avoidable er-rors in situations that already seemed under control (because it waseasy to reduce the corresponding errors without memory cells). See“Output Weight Conﬂict” in section 3 and “Abuse Problem and Solu-tion” (section 4.7).\\n2. If there are large positive or negative s\\ncj(t)values (because scjhas'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 42}, page_content='2. If there are large positive or negative s\\ncj(t)values (because scjhas\\ndrifted since time step t−q), then h′(scj(t))may be small (assuming\\nthat his a logistic sigmoid). See section 4. Drifts of the memory cell’s\\ninternal state scjcan be countermanded by negatively biasing the input\\ngate inj(see section 4 and the next point). Recall from section 4 that\\nthe precise bias value does not matter much.\\n3.yinj(t−q)and f′\\ninj(netinj(t−q))are small if the input gate is negatively\\nbiased (assume finjis a logistic sigmoid). However, the potential sig-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 43}, page_content='1778 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nniﬁcance of this is negligible compared to the potential signiﬁcance of\\ndrifts of the internal state scj.\\nSome of the factors above may scale down LSTM’s overall error ﬂow,\\nbut not in a manner that depends on the length of the time lag. The ﬂowwill still be much more effective than an exponentially (of order q) decaying\\nﬂow without memory cells.\\nAcknowledgments\\nThanks to Mike Mozer, Wilfried Brauer, Nic Schraudolph, and several anony-\\nmous referees for valuable comments and suggestions that helped to im-prove a previous version of this article (Hochreiter and Schmidhuber, 1995).This work was supported by DFG grant SCHM 942/3-1 from DeutscheForschungsgemeinschaft.\\nReferences\\nAlmeida, L. B. (1987). A learning rule for asynchronous perceptrons with feed-\\nback in a combinatorial environment. In IEEE 1st International Conference on\\nNeural Networks, San Diego (Vol. 2, pp. 609–618).'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 43}, page_content='References\\nAlmeida, L. B. (1987). A learning rule for asynchronous perceptrons with feed-\\nback in a combinatorial environment. In IEEE 1st International Conference on\\nNeural Networks, San Diego (Vol. 2, pp. 609–618).\\nBaldi, P ., & Pineda, F. (1991). Contrastive learning and neural oscillator. Neural\\nComputation, 3 , 526–545.\\nBengio, Y., & Frasconi, P . (1994). Credit assignment through time: Alternatives to\\nbackpropagation. In J. D. Cowan, G. Tesauro, & J. Alspector (Eds.), Advances\\nin neural information processing systems 6 (pp. 75–82). San Mateo, CA: Morgan\\nKaufmann.\\nBengio, Y., Simard, P ., & Frasconi, P . (1994). Learning long-term dependencies\\nwith gradient descent is difﬁcult. IEEE Transactions on Neural Networks ,5(2),\\n157–166.\\nCleeremans, A., Servan-Schreiber, D., & McClelland, J. L. (1989). Finite-state\\nautomata and simple recurrent networks. Neural Computation ,1, 372–381.\\nde Vries, B., & Principe, J. C. (1991). A theory for neural networks with time'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 43}, page_content='Bengio, Y., Simard, P ., & Frasconi, P . (1994). Learning long-term dependencies\\nwith gradient descent is difﬁcult. IEEE Transactions on Neural Networks ,5(2),\\n157–166.\\nCleeremans, A., Servan-Schreiber, D., & McClelland, J. L. (1989). Finite-state\\nautomata and simple recurrent networks. Neural Computation ,1, 372–381.\\nde Vries, B., & Principe, J. C. (1991). A theory for neural networks with time\\ndelays. In R. P . Lippmann, J. E. Moody, & D. S. Touretzky (Eds.), Advances in\\nneural information processing systems 3 , (pp. 162–168). San Mateo, CA: Morgan\\nKaufmann.\\nDoya, K. (1992). Bifurcations in the learning of recurrent neural networks.\\nInProceedings of 1992 IEEE International Symposium on Circuits and Systems\\n(pp. 2777–2780).\\nDoya, K., & Yoshizawa, S. (1989). Adaptive neural oscillator using continuous-\\ntime backpropagation learning. Neural Networks, 2 , 375–385.\\nElman, J. L. (1988). Finding structure in time (Tech. Rep. No. CRL 8801). San Diego:'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 43}, page_content='Doya, K. (1992). Bifurcations in the learning of recurrent neural networks.\\nInProceedings of 1992 IEEE International Symposium on Circuits and Systems\\n(pp. 2777–2780).\\nDoya, K., & Yoshizawa, S. (1989). Adaptive neural oscillator using continuous-\\ntime backpropagation learning. Neural Networks, 2 , 375–385.\\nElman, J. L. (1988). Finding structure in time (Tech. Rep. No. CRL 8801). San Diego:\\nCenter for Research in Language, University of California, San Diego.\\nFahlman, S. E. (1991). The recurrent cascade-correlation learning algorithm. In\\nR. P . Lippmann, J. E. Moody, & D. S. Touretzky (Eds.), Advances in neural infor-\\nmation processing systems 3 (pp. 190–196). San Mateo, CA: Morgan Kaufmann.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 44}, page_content='Long Short-Term Memory 1779\\nHochreiter, J. (1991). Untersuchungen zu dynamischen neuronalen Netzen . Diploma\\nthesis, Institut f ¨ ur Informatik, Lehrstuhl Prof. Brauer, Technische Universit¨ at\\nM¨unchen. See http://www7.informatik.tu-muenchen.de/˜hochreit.\\nHochreiter, S., & Schmidhuber, J. (1995). Long short-term memory (Tech. Rep.\\nNo. FKI-207-95). Fakult¨ at f ¨ur Informatik, Technische Universit¨ at M ¨ unchen.\\nHochreiter, S., & Schmidhuber, J. (1996). Bridging long time lags by weight\\nguessing and “long short-term memory.” In F. L. Silva, J. C. Principe, &L. B. Almeida (Eds.), Spatiotemporal models in biological and artiﬁcial systems\\n(pp. 65–72). Amsterdam: IOS Press.\\nHochreiter, S., & Schmidhuber, J. (1997). LSTM can solve hard long time lag\\nproblems. In Advances in neural information processing systems 9 . Cambridge,\\nMA: MIT Press.\\nLang, K., Waibel, A., & Hinton, G. E. (1990). A time-delay neural network archi-\\ntecture for isolated word recognition. Neural Networks, 3 , 23–43.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 44}, page_content='(pp. 65–72). Amsterdam: IOS Press.\\nHochreiter, S., & Schmidhuber, J. (1997). LSTM can solve hard long time lag\\nproblems. In Advances in neural information processing systems 9 . Cambridge,\\nMA: MIT Press.\\nLang, K., Waibel, A., & Hinton, G. E. (1990). A time-delay neural network archi-\\ntecture for isolated word recognition. Neural Networks, 3 , 23–43.\\nLin, T., Horne, B. G., Tino, P ., & Giles, C. L. (1996). Learning long-term de-\\npendencies in NARX recurrent neural networks. IEEE Transactions on Neural\\nNetworks ,7, 1329–1338.\\nMiller, C. B., & Giles, C. L. (1993). Experimental comparison of the effect of order\\nin recurrent neural networks. International Journal of Pattern Recognition and\\nArtiﬁcial Intelligence, 7 (4), 849–872.\\nMozer, M. C. (1989). A focused back-propagation algorithm for temporal se-\\nquence recognition. Complex Systems, 3 , 349–381.\\nMozer, M. C. (1992). Induction of multiscale temporal structure. In J. E. Moody,'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 44}, page_content='in recurrent neural networks. International Journal of Pattern Recognition and\\nArtiﬁcial Intelligence, 7 (4), 849–872.\\nMozer, M. C. (1989). A focused back-propagation algorithm for temporal se-\\nquence recognition. Complex Systems, 3 , 349–381.\\nMozer, M. C. (1992). Induction of multiscale temporal structure. In J. E. Moody,\\nS. J. Hanson, & R. P . Lippman (Eds.), Advances in neural information processing\\nsystems 4 (pp. 275–282). San Mateo, CA: Morgan Kaufmann.\\nPearlmutter, B. A. (1989). Learning state space trajectories in recurrent neural\\nnetworks. Neural Computation, 1 (2), 263–269.\\nPearlmutter, B. A. (1995). Gradient calculations for dynamic recurrent neural\\nnetworks: A survey. IEEE Transactions on Neural Networks, 6 (5), 1212–1228.\\nPineda, F. J. (1987). Generalization of back-propagation to recurrent neural net-\\nworks. Physical Review Letters, 19 (59), 2229–2232.\\nPineda, F. J. (1988). Dynamics and architecture for neural computation. Journal\\nof Complexity, 4 , 216–245.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 44}, page_content='Pearlmutter, B. A. (1995). Gradient calculations for dynamic recurrent neural\\nnetworks: A survey. IEEE Transactions on Neural Networks, 6 (5), 1212–1228.\\nPineda, F. J. (1987). Generalization of back-propagation to recurrent neural net-\\nworks. Physical Review Letters, 19 (59), 2229–2232.\\nPineda, F. J. (1988). Dynamics and architecture for neural computation. Journal\\nof Complexity, 4 , 216–245.\\nPlate, T. A. (1993). Holographic recurrent networks. In S. J. Hanson, J. D. Cowan,\\n& C. L. Giles ( Eds.), Advances in neural information processing systems 5 (pp. 34–\\n41). San Mateo, CA: Morgan Kaufmann.\\nPollack, J. B. (1991). Language induction by phase transition in dynamical rec-\\nognizers. In R. P . Lippmann, J. E. Moody, & D. S. Touretzky (Eds.), Advances in\\nneural information processing systems 3 (pp. 619–626). San Mateo, CA: Morgan\\nKaufmann.\\nPuskorius, G. V ., and Feldkamp, L. A. (1994). Neurocontrol of nonlinear dynam-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 44}, page_content='41). San Mateo, CA: Morgan Kaufmann.\\nPollack, J. B. (1991). Language induction by phase transition in dynamical rec-\\nognizers. In R. P . Lippmann, J. E. Moody, & D. S. Touretzky (Eds.), Advances in\\nneural information processing systems 3 (pp. 619–626). San Mateo, CA: Morgan\\nKaufmann.\\nPuskorius, G. V ., and Feldkamp, L. A. (1994). Neurocontrol of nonlinear dynam-\\nical systems with Kalman ﬁlter trained recurrent networks. IEEE Transactions\\non Neural Networks, 5 (2), 279–297.\\nRing, M. B. (1993). Learning sequential tasks by incrementally adding higher\\norders. In S. J. Hanson, J. D. Cowan, & C. L. Giles (Eds.), Advances in neu-\\nral information processing systems 5 (pp. 115–122). San Mateo, CA: Morgan\\nKaufmann.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 45}, page_content='1780 Sepp Hochreiter and J ¨ urgen Schmidhuber\\nRobinson, A. J., & Fallside, F. (1987). The utility driven dynamic error propagation\\nnetwork (Tech. Rep. No. CUED/F-INFENG/TR.1). Cambridge: Cambridge\\nUniversity Engineering Department.\\nSchmidhuber, J. (1989). A local learning algorithm for dynamic feedforward and\\nrecurrent networks. Connection Science, 1 (4), 403–412.\\nSchmidhuber, J. (1992a). A ﬁxed size storage O(n3)time complexity learning\\nalgorithm for fully recurrent continually running networks. Neural Compu-\\ntation, 4 (2), 243–248.\\nSchmidhuber, J. (1992b). Learning complex, extended sequences using the prin-\\nciple of history compression. Neural Computation, 4 (2), 234–242.\\nSchmidhuber, J. (1992c). Learning unambiguous reduced sequence descriptions.\\nIn J. E. Moody, S. J. Hanson, & R. P . Lippman (Eds.), Advances in neural infor-\\nmation processing systems 4 (pp. 291–298). San Mateo, CA: Morgan Kaufmann.\\nSchmidhuber, J. (1993). Netzwerkarchitekturen, Zielfunktionen und Kettenregel. Ha-'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 45}, page_content='ciple of history compression. Neural Computation, 4 (2), 234–242.\\nSchmidhuber, J. (1992c). Learning unambiguous reduced sequence descriptions.\\nIn J. E. Moody, S. J. Hanson, & R. P . Lippman (Eds.), Advances in neural infor-\\nmation processing systems 4 (pp. 291–298). San Mateo, CA: Morgan Kaufmann.\\nSchmidhuber, J. (1993). Netzwerkarchitekturen, Zielfunktionen und Kettenregel. Ha-\\nbilitationsschrift, Institut f ¨ ur Informatik, Technische Universit¨ at M ¨ unchen.\\nSchmidhuber, J., & Hochreiter, S. (1996). Guessing can outperform many long time\\nlag algorithms (Tech. Rep. No. IDSIA-19-96). Lugano, Switzerland: Instituto\\nDalle Molle di Studi sull’Intelligenza Artiﬁciale.\\nSilva, G. X., Amaral, J. D., Langlois, T., & Almeida, L. B. (1996). Faster training of\\nrecurrent networks. In F. L. Silva, J. C. Principe, & L. B. Almeida (Eds.), Spa-\\ntiotemporal models in biological and artiﬁcial systems (pp. 168–175). Amsterdam:\\nIOS Press.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 45}, page_content='lag algorithms (Tech. Rep. No. IDSIA-19-96). Lugano, Switzerland: Instituto\\nDalle Molle di Studi sull’Intelligenza Artiﬁciale.\\nSilva, G. X., Amaral, J. D., Langlois, T., & Almeida, L. B. (1996). Faster training of\\nrecurrent networks. In F. L. Silva, J. C. Principe, & L. B. Almeida (Eds.), Spa-\\ntiotemporal models in biological and artiﬁcial systems (pp. 168–175). Amsterdam:\\nIOS Press.\\nSmith, A. W., & Zipser, D. (1989). Learning sequential structures with the real-\\ntime recurrent learning algorithm. International Journal of Neural Systems, 1 (2),\\n125–131.\\nSun, G., Chen, H., & Lee, Y. (1993). Time warping invariant neural networks. In\\nS. J. Hanson, J. D. Cowan, & C. L. Giles (Eds.), Advances in neural information\\nprocessing systems 5 (pp. 180–187). San Mateo, CA: Morgan Kaufmann.\\nWatrous, R. L., & Kuhn, G. M. (1992). Induction of ﬁnite-state languages using\\nsecond-order recurrent networks. Neural Computation, 4 , 406–414.'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 45}, page_content='125–131.\\nSun, G., Chen, H., & Lee, Y. (1993). Time warping invariant neural networks. In\\nS. J. Hanson, J. D. Cowan, & C. L. Giles (Eds.), Advances in neural information\\nprocessing systems 5 (pp. 180–187). San Mateo, CA: Morgan Kaufmann.\\nWatrous, R. L., & Kuhn, G. M. (1992). Induction of ﬁnite-state languages using\\nsecond-order recurrent networks. Neural Computation, 4 , 406–414.\\nWerbos, P . J. (1988). Generalization of backpropagation with application to a\\nrecurrent gas market model. Neural Networks, 1 , 339–356.\\nWilliams, R. J. (1989). Complexity of exact gradient computation algorithms for re-\\ncurrent neural networks (Tech. Rep. No. NU-CCS-89-27). Boston: Northeastern\\nUniversity, College of Computer Science.\\nWilliams, R. J. & Peng, J. (1990). An efﬁcient gradient-based algorithm for on-line\\ntraining of recurrent network trajectories. Neural Computation, 4 , 491–501.\\nWilliams, R. J., & Zipser, D. (1992). Gradient-based learning algorithms for'),\n",
              " Document(metadata={'source': '/content/LSTM.pdf', 'page': 45}, page_content='current neural networks (Tech. Rep. No. NU-CCS-89-27). Boston: Northeastern\\nUniversity, College of Computer Science.\\nWilliams, R. J. & Peng, J. (1990). An efﬁcient gradient-based algorithm for on-line\\ntraining of recurrent network trajectories. Neural Computation, 4 , 491–501.\\nWilliams, R. J., & Zipser, D. (1992). Gradient-based learning algorithms for\\nrecurrent networks and their computational complexity. In Y. Chauvin, &D. E. Rumelhart (Eds.), Back-propagation: Theory, architectures and applications .\\nHillsdale, NJ: Erlbaum.\\nReceived August 28, 1995; accepted February 24, 1997.')]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DENSE_MODEL = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "dense_embedding_model = SentenceTransformer(DENSE_MODEL)"
      ],
      "metadata": {
        "id": "Ipr2xpE5yBj2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dense_vector(docs: Document, model: SentenceTransformer) :\n",
        "    \"\"\"\n",
        "    Encode a list of Document objects using a HuggingFace model.\n",
        "\n",
        "    Args:\n",
        "        docs (Document): A Document object with 'page_content'.\n",
        "        model (SentenceTransformer): An instance of SentenceTransformer.\n",
        "\n",
        "    Returns:\n",
        "        List[float]: A list of embeddings, one for each document.\n",
        "    \"\"\"\n",
        "    # Extract page content from documents\n",
        "    embeddings = [model.encode(docs.page_content)]\n",
        "\n",
        "    return embeddings[0].tolist()\n"
      ],
      "metadata": {
        "id": "7A8YW6Z04S-e"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(chunks):\n",
        "\n",
        "        dense_embedding = create_dense_vector(doc, dense_embedding_model)\n",
        "\n",
        "        document = {\n",
        "            \"content\": doc.page_content,\n",
        "            \"dense_vector\": dense_embedding,\n",
        "        }\n",
        "\n",
        "        es_client.index(index=\"temp\", id=str(i), body=document)"
      ],
      "metadata": {
        "id": "RwlMYkk0ypRH"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"What is the use of Gated Cell Units in LSTMs ?\""
      ],
      "metadata": {
        "id": "zF3lzFfc-P2w"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_query = {\n",
        "    \"query\": {\n",
        "        \"match\": {\n",
        "            \"content\": {\n",
        "                \"query\": user_query\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"size\": 10\n",
        "}\n",
        "\n",
        "bm25_results = es_client.search(index=index_name, body=bm25_query)\n"
      ],
      "metadata": {
        "id": "eTZwa-o099KK"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bm25_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oTKKK9nl98_e",
        "outputId": "175f0832-1b65-41a3-bac2-53af5086ab51"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectApiResponse({'took': 617, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 160, 'relation': 'eq'}, 'max_score': 8.395834, 'hits': [{'_index': 'temp', '_id': '127', '_score': 8.395834, '_source': {'content': 'To ﬁnd out about LSTM’s practical limitations we intend to apply it to\\nreal-world data. Application areas will include time-series prediction, musiccomposition, and speech processing. It will also be interesting to augmentsequence chunkers (Schmidhuber, 1992b, 1993) by LSTM to combine theadvantages of both.\\nAppendix\\nA.1 Algorithm Details. In what follows, the index kranges over output\\nunits, iranges over hidden units, cjstands for the jth memory cell block, cv\\nj\\ndenotes the vth unit of memory cell block cj,u,l,mstand for arbitrary units,\\nand tranges over all time steps of a given input sequence.\\nThe gate unit logistic sigmoid (with range [0 ,1]) used in the experiments\\nis\\nf(x)=1\\n1+exp(−x). (A.1)', 'dense_vector': [-0.04604389891028404, -0.0751531645655632, 0.0009022757294587791, -0.03636178746819496, 0.031175322830677032, 0.09012547135353088, -0.03815111145377159, -0.009829736314713955, -0.02542305365204811, -0.07234065234661102, 3.2243283385469113e-06, -0.00909651443362236, 0.012789322063326836, -0.06569314748048782, -0.04890067130327225, -0.028000032529234886, -0.025114785879850388, 0.07368838042020798, -0.008465485647320747, -0.07226812839508057, 0.07027315348386765, 0.0340452715754509, -0.0219801627099514, 0.011837481521070004, 0.02536717988550663, -0.023424241691827774, -0.031092794612050056, 0.02616969309747219, 0.07720243185758591, -0.014303852804005146, 0.053434669971466064, 0.043102651834487915, 0.07725522667169571, 0.03197205439209938, -0.12772074341773987, -0.015666404739022255, -0.09195958077907562, -0.07288950681686401, 0.0034932687412947416, -5.8516121498541906e-05, 0.02687794342637062, 0.043864767998456955, 0.022713568061590195, -0.03678939491510391, 0.018379410728812218, 0.026621105149388313, -0.014791102148592472, -0.03162059560418129, -0.1238722950220108, 0.07429763674736023, -0.060517244040966034, 0.11912595480680466, -0.0014821812510490417, 0.1388794183731079, -0.11577996611595154, -0.030913151800632477, 0.0010131598683074117, 0.08431356400251389, -0.00013871915871277452, 0.004997854586690664, -0.04998726770281792, -0.03802517056465149, -0.014470531605184078, -0.07106614857912064, 0.009361224249005318, -0.09215381741523743, 0.052019018679857254, 0.01697136089205742, 0.041950639337301254, 0.06449021399021149, 0.004800825379788876, 0.016892466694116592, -0.03503463789820671, 0.07244788855314255, -0.018879558891057968, 0.04973572492599487, 0.08381657302379608, -0.0011034529889002442, 0.018842199817299843, -0.01007172279059887, 0.002748709637671709, -0.004473679233342409, 0.04071839898824692, -0.10213109850883484, -0.0004819985188078135, -0.047888629138469696, -0.028706833720207214, 0.09712330251932144, 0.005810289178043604, 0.013160460628569126, -0.0016097916522994637, -0.08957801759243011, -0.03370995447039604, -0.03482590243220329, 0.08401418477296829, 0.01811332255601883, 0.015380503609776497, 0.03364808112382889, 0.029619142413139343, -0.01904284581542015, 0.008021916262805462, 0.08777323365211487, 0.0070177530869841576, 0.026729192584753036, 0.04745647683739662, -0.04477386921644211, 0.05350160226225853, 0.0056901415809988976, 0.013511186465620995, -0.08248786628246307, 0.06613655388355255, 0.02663842774927616, 0.03989775851368904, -0.012157764285802841, 0.132695272564888, -0.08884989470243454, 0.038368966430425644, -0.08798393607139587, 0.029817719012498856, 0.07990870624780655, -0.026878755539655685, -0.028630109503865242, -0.03754078596830368, 0.07199417799711227, -0.035677965730428696, -0.025541456416249275, -0.022334659472107887, 5.633838060276041e-33, -0.0036530743818730116, 0.014883887022733688, -0.014105036854743958, -0.07552825659513474, 0.02753148041665554, -0.018889784812927246, -0.0495416559278965, 0.0326865017414093, -0.006592400372028351, 0.05309944227337837, -0.08154737949371338, 0.012059304863214493, 0.02604186348617077, 0.06745154410600662, -0.007016101386398077, -0.044349342584609985, -0.005874415393918753, 0.009542464278638363, 0.005851767957210541, -0.03324607387185097, -0.01708095893263817, -0.05389995127916336, 0.05435950309038162, -0.03561733663082123, 0.010151948779821396, 0.005571546033024788, 0.022196142002940178, -0.028750935569405556, -0.024011768400669098, 0.009736144915223122, -0.072147898375988, 0.04158865287899971, -0.06667657196521759, -0.04622203856706619, 0.11093669384717941, 0.029337769374251366, -0.037312787026166916, -0.05552374944090843, 0.05957212671637535, -0.12415606528520584, -0.07109905034303665, 0.01525688823312521, 0.08359065651893616, -0.05456395447254181, -0.13925054669380188, -0.06942599266767502, -0.018066124990582466, 0.0015978423180058599, 0.047131214290857315, -0.0352356918156147, 0.0315367691218853, -0.005983181297779083, -0.10072818398475647, -0.07063428312540054, 0.06598017364740372, 0.03065449744462967, 0.07596267014741898, 0.0695548728108406, 0.019858429208397865, 0.14172354340553284, -0.018868418410420418, 0.06746567785739899, 0.06430242210626602, 0.08780154585838318, 0.018799249082803726, 0.05519790202379227, -0.07278671115636826, 0.006936386227607727, 0.02903163991868496, -0.047585099935531616, 0.010236398316919804, -0.08612112700939178, 0.06980094313621521, -0.00019899135804735124, 0.030706239864230156, -0.002940927166491747, 0.11992685496807098, -0.0948321744799614, -0.08341790735721588, 0.0034540602937340736, -0.01677824929356575, 0.023771893233060837, -0.02608269825577736, -0.07996439188718796, -0.016504254192113876, -0.054716020822525024, 0.09680619090795517, -0.06494123488664627, -0.05680150166153908, -0.05182921886444092, -0.06945376843214035, 0.001654843334108591, 0.0643121674656868, 0.03775282949209213, -0.11076592653989792, -7.262348097438068e-33, -0.050206899642944336, 0.04033457115292549, 0.01558875385671854, -0.004415918607264757, -0.01126317959278822, -0.031568627804517746, -0.036823779344558716, 0.029593899846076965, -0.017528817057609558, 0.05816638842225075, 0.007557498291134834, 0.02496788464486599, 0.03642145171761513, -0.009565661661326885, -0.020156502723693848, -0.027186689898371696, 0.06647364795207977, -0.029763922095298767, -0.0036390970926731825, -0.027364613488316536, 0.05111991986632347, 0.023341268301010132, -0.03524381294846535, 0.022940609604120255, -0.11329632252454758, -0.0010695314267650247, -0.04282551258802414, 0.06503386795520782, -0.037608593702316284, -0.07268994301557541, -0.05524427816271782, -0.06452139467000961, -0.0566922128200531, -0.05669955164194107, -0.0016810415545478463, 0.014991885051131248, 0.008489510975778103, -0.0018754838965833187, 0.02931971848011017, 6.756108632544056e-05, 0.06912534683942795, 0.06790981441736221, 0.03265107423067093, -0.02237408421933651, 0.028922419995069504, -0.022925380617380142, -0.08228202909231186, 0.057274363934993744, -0.0028096160385757685, -0.01393471471965313, 0.012711352668702602, 0.05782323703169823, 0.010388900525867939, 0.010049708187580109, -0.008840561844408512, 0.023069312795996666, -0.07378922402858734, 0.00906812772154808, 0.000622795254457742, -0.03135313466191292, -0.02909708395600319, -0.011738507077097893, -0.019641615450382233, -0.08916256576776505, 0.033225782215595245, 0.011364865116775036, 0.030902916565537453, -0.0018968547228723764, -0.007038897834718227, 0.007462737150490284, 0.10321737825870514, 0.04432147741317749, 0.08002791553735733, 0.030115077272057533, -0.11048457026481628, -0.06426748633384705, -0.0773855522274971, -0.11902082711458206, -0.06902280449867249, -0.04637030512094498, 0.0173967108130455, 0.023707056418061256, 0.01954795978963375, 0.03660575672984123, 0.037114374339580536, 0.060002222657203674, 0.012127546593546867, 0.0533837266266346, 0.03839249536395073, -0.07436477392911911, -0.011586946435272694, 0.03188396617770195, -0.025469614192843437, -0.030412347987294197, 0.004674522206187248, -5.996594865109728e-08, -0.004298028070479631, -0.02033805660903454, -0.015253483317792416, 0.009547533467411995, 0.05962206423282623, -0.10979434847831726, -0.01509543601423502, -0.005675760563462973, 0.06336317211389542, -0.04406984895467758, 0.14760372042655945, -0.015395654365420341, -0.11574480682611465, 0.008834580890834332, 0.02867339737713337, -0.00315800285898149, -0.013647186569869518, 0.03330158442258835, 6.172766734380275e-05, 0.018349289894104004, 0.00271604978479445, 0.06681706011295319, 0.014880722388625145, 0.0240681990981102, 0.04292601719498634, -0.01311170682311058, 0.004727643448859453, 0.0420481376349926, 0.017716247588396072, 0.04011280834674835, 0.03371196985244751, 0.06354951113462448, -0.0015092859975993633, 0.030450094491243362, -0.010777190327644348, 0.009557053446769714, 0.009854814037680626, 0.05425242707133293, -0.05501282215118408, 0.005227051209658384, 0.011923989281058311, 0.014293709769845009, -0.02900533936917782, 0.05878652259707451, -0.07267390191555023, -0.015066780149936676, -0.04057685285806656, -0.09472806751728058, 0.005380674265325069, -0.03649260476231575, 0.009323595091700554, 0.060191333293914795, 0.01602490246295929, 0.039012931287288666, 0.11613185703754425, -0.028917301446199417, -0.06959456205368042, -0.021802052855491638, 0.04169691354036331, 0.009230528958141804, -0.029084790498018265, 0.0270271934568882, -0.024522125720977783, 0.01042398065328598]}}, {'_index': 'temp', '_id': '41', '_score': 6.765766, '_source': {'content': '4.4 Memory Cell Blocks. Smemory cells sharing the same input gate\\nand the same output gate form a structure called a memory cell block of sizeS. Memory cell blocks facilitate information storage. As with conventional\\nneural nets, it is not so easy to code a distributed input within a single cell.Since each memory cell block has as many gate units as a single memory cell(namely, two), the block architecture can be even slightly more efﬁcient. Amemory cell block of size 1 is just a simple memory cell. In the experimentsin section 5, we will use memory cell blocks of various sizes.\\n4.5 Learning. We use a variant of RTRL (e.g., Robinson & Fallside, 1987)\\nthat takes into account the altered, multiplicative dynamics caused by inputand output gates. To ensure nondecaying error backpropagation throughinternal states of memory cells, as with truncated BPTT (e.g., Williams &Peng, 1990), errors arriving at memory cell net inputs (for cell c\\nj, this includes', 'dense_vector': [-0.03481133654713631, -0.021827740594744682, -0.02635514736175537, 0.0650075152516365, -0.04990686848759651, -0.008070969954133034, -0.01970481127500534, 0.03459857031702995, 0.048487916588783264, 0.00845949724316597, -0.014924810267984867, 0.012530935928225517, 0.0077863335609436035, -0.06236855313181877, -0.10288012772798538, 0.03652770444750786, -0.03836763650178909, 0.1206677109003067, -0.10728255659341812, -0.06940999627113342, -0.025089804083108902, -0.04969783499836922, -0.01225475873798132, -0.003919620998203754, -0.024904202669858932, 0.024021577090024948, -0.11281975358724594, -0.016050562262535095, 0.057197052985429764, -0.06457016617059708, -0.026045018807053566, -0.027986126020550728, -0.03726627677679062, 0.06790625303983688, -0.05391629785299301, 0.04832214489579201, -0.06237110123038292, 0.0037515435833483934, 0.008079557679593563, -0.05899808183312416, 0.007735990919172764, 0.03333333507180214, 0.01651749573647976, 0.04007488861680031, 0.03929940611124039, -0.021575992926955223, 0.02608274482190609, -0.038101524114608765, -0.052402887493371964, -0.030451061204075813, -0.06060536205768585, 0.08474842458963394, -0.01933554746210575, 0.10624851286411285, -0.00888296402990818, -0.006179730873554945, -0.02638721652328968, 0.05416884273290634, -0.006484277546405792, 0.0509776771068573, -0.07105134427547455, -0.0666363313794136, -0.006788254715502262, -0.04712684452533722, -0.014947202987968922, 0.005709915421903133, 0.0007882767240516841, 0.04865705221891403, 0.03443455696105957, -0.0085446173325181, 0.06288313120603561, 0.027222955599427223, -0.055696625262498856, 0.043147556483745575, 0.07675805687904358, 0.004347146023064852, 0.025070862844586372, 0.05286021903157234, 0.02435913495719433, -0.00045384562690742314, -0.01744610071182251, 0.013106090947985649, 0.012902637012302876, -0.05082642287015915, 0.07819775491952896, -0.03209434822201729, -0.03914109244942665, 0.052241165190935135, 0.05413622036576271, 0.015018309466540813, 0.07731428742408752, -0.0032446226105093956, -0.010525616817176342, -0.0056558107025921345, 0.05317981541156769, -0.07921188324689865, 0.012752664275467396, -0.04302051663398743, -0.041629206389188766, 0.020167920738458633, 0.06098317727446556, 0.1103326827287674, 0.015594317577779293, 0.011547147296369076, 0.04325287416577339, -0.04032740741968155, 0.1063438206911087, 0.005293552298098803, 0.030566932633519173, -0.1279916912317276, -0.02246721088886261, 0.010158966295421124, 0.005756249185651541, 0.08096886426210403, 0.02956327423453331, -0.09060310572385788, -0.026885082945227623, -0.051332563161849976, 0.07597795873880386, 0.043173130601644516, -0.08158330619335175, -0.016574464738368988, -0.08037585765123367, 0.006338864099234343, -0.0018404844449833035, -0.07177645713090897, -0.08044790476560593, 4.690986898450946e-33, -0.04381237551569939, 0.026928769424557686, -0.021208811551332474, 0.014918106608092785, 0.03791327029466629, 0.033780671656131744, -0.001879392541013658, -0.006541413255035877, -0.023574665188789368, 0.02094392478466034, -0.04120408743619919, -0.10598994791507721, -0.00278398091904819, 0.06802836805582047, 0.07288559526205063, -0.06688297539949417, -0.07843054831027985, 0.017661970108747482, 0.020078957080841064, -0.05617960914969444, 0.008351664990186691, 0.0420316606760025, -0.0023571213241666555, -0.047924719750881195, -0.021038148552179337, -0.01136239618062973, 0.04207911714911461, 0.023593295365571976, 0.0007437670137733221, 0.0693424642086029, -0.04783787205815315, 0.01619333028793335, -0.0471656359732151, -0.03633489832282066, 0.05216486379504204, 0.006377591751515865, -0.010862423107028008, -0.02972031757235527, 0.04575192928314209, -0.11436233669519424, -0.042405903339385986, 0.03218291699886322, -0.004722003825008869, -0.02767137624323368, -0.06860442459583282, -0.12235423922538757, 0.0345940962433815, 0.027174798771739006, -0.036974385380744934, -0.06950042396783829, -0.009939716197550297, 0.022549469023942947, 0.005218206439167261, -0.07053590565919876, 0.04437018930912018, 0.030729195103049278, 0.015831973403692245, 0.0940663143992424, 0.07722096145153046, 0.16768041253089905, -0.03547879680991173, 0.07091361284255981, -0.05303412303328514, 0.10842593014240265, -0.0032947626896202564, 0.10828426480293274, -0.09887593239545822, 0.007532348856329918, 0.0234944187104702, -0.05572310462594032, 0.07288601994514465, -0.030032580718398094, -0.002329504117369652, 0.0031354536768049, 0.034092217683792114, -0.020577900111675262, 0.07126884162425995, -0.053540151566267014, -0.09413217008113861, -0.014730265364050865, 0.022449282929301262, -0.026271870359778404, -0.05352672189474106, 0.04211743175983429, -0.07334551215171814, -0.067174531519413, 0.08241436630487442, -0.06590259075164795, -0.043506644666194916, 0.02512785792350769, -0.006379950325936079, -0.08307353407144547, 0.08501850068569183, 0.004782882519066334, -0.0845547467470169, -5.294646961294362e-33, -0.03801289200782776, 0.01911928318440914, -0.034623198211193085, 0.03224636986851692, -0.06817170977592468, -0.014067884534597397, 0.016226982697844505, -0.017370428889989853, -0.026992330327630043, -0.03982618451118469, 0.022450705990195274, 0.0663779228925705, -0.01615343615412712, 0.024260321632027626, 0.005107024684548378, 0.004901014268398285, 0.019080786034464836, -0.0080918800085783, 0.08423906564712524, -0.027839399874210358, -0.027938589453697205, 0.027988344430923462, 0.007080621086061001, 0.13242344558238983, -0.06534043699502945, 0.06520062685012817, -0.07723928242921829, 0.1091088354587555, 0.008133836090564728, -0.0009383235592395067, -0.03289005905389786, 0.001627773861400783, -0.031431894749403, 0.0959550216794014, 0.002910314127802849, -0.03797268867492676, 0.05646584555506706, -0.02983335219323635, 0.00569247966632247, -0.017174817621707916, 0.12129335105419159, 0.030393993481993675, -0.008828873746097088, -0.019797543063759804, 0.042180899530649185, 0.03541986271739006, -0.059726420789957047, 0.07499350607395172, -0.03368513658642769, -0.012380200438201427, -0.03687532991170883, 0.04852985218167305, -0.07204300165176392, -0.04206269234418869, -0.01743396557867527, 0.05061683431267738, 0.014840099029242992, 0.06587999314069748, 0.05367639288306236, 0.01028671208769083, -0.10636194795370102, -0.11941859126091003, 0.04217765852808952, -0.05927403271198273, 0.01617111824452877, -0.012691277079284191, 0.03791378065943718, 0.01821206510066986, 0.0414046049118042, 0.01762315072119236, 0.03817162662744522, 0.10478045791387558, -0.02871502749621868, -0.03566299378871918, -0.012708401307463646, 0.04258138686418533, -0.007978360168635845, -0.08612433820962906, -0.01816578023135662, 0.017975695431232452, -0.03463888168334961, 0.017427220940589905, 0.025333277881145477, 0.00679022865369916, 0.07004109770059586, 0.041338931769132614, 0.038654595613479614, 0.030397914350032806, -0.003438033629208803, -0.043193235993385315, 0.024793613702058792, 0.03975723311305046, -0.0034177873749285936, 0.005183025263249874, -0.006239526905119419, -5.440688255475834e-08, 0.05452699959278107, -0.01567162200808525, 0.0978928655385971, 0.038354188203811646, 0.053977128118276596, -0.14482584595680237, 0.02717350795865059, 0.014758776873350143, 0.011463873088359833, 0.005510538816452026, 0.05095107480883598, 0.002525676041841507, -0.06368312984704971, -0.07893888652324677, 0.023482074961066246, 0.08894377946853638, -0.07617492973804474, -0.09145758301019669, 0.020595476031303406, -0.03704910725355148, 0.055243782699108124, -0.03596554324030876, 0.037078626453876495, 0.05653388798236847, 0.04790361225605011, -0.0836801677942276, -0.050899047404527664, 0.006634686142206192, 0.019945112988352776, -0.06111929938197136, 0.09323989599943161, 0.04482787102460861, 0.07238885015249252, 0.024451257660984993, 0.013370110653340816, 0.034651320427656174, 0.06232066452503204, -0.003739723237231374, -0.020137296989560127, -0.0020744591020047665, 0.010080021806061268, -0.01107751950621605, -0.0178085807710886, -0.003388661891222, -0.04352976009249687, 0.03919822722673416, -0.026754094287753105, -0.05500713735818863, -0.06311370432376862, -0.017937758937478065, 0.011295781470835209, 0.09398320317268372, 0.007446255534887314, 0.04122931510210037, 0.08167275786399841, -0.03760943189263344, -0.04529909789562225, -0.06886309385299683, 0.022658025845885277, 0.05685363709926605, 0.040914032608270645, 0.07202459871768951, -0.08683516830205917, -0.01642146334052086]}}, {'_index': 'temp', '_id': '141', '_score': 6.5975647, '_source': {'content': '∂yinj(t−k)\\n∂yu(t−k−1)∀uand∂netcj(t−k)\\n∂yu(t−k−1)∀u.\\nIn what follows, an error ϑj(t)starts ﬂowing back at cj’s output. We re-\\ndeﬁne\\nϑj(t):=∑\\niwicjϑi(t+1). (A.29)\\nFollowing the deﬁnitions and conventions of section 3.1, we compute\\nerror ﬂow for the truncated backpropagation learning rule. The error occur-ring at the output gate is\\nϑ\\noutj(t)≈tr∂youtj(t)\\n∂netoutj(t)∂ycj(t)\\n∂youtj(t)ϑj(t). (A.30)\\nThe error occurring at the internal state is\\nϑscj(t)=∂scj(t+1)\\n∂scj(t)ϑscj(t+1)+∂ycj(t)\\n∂scj(t)ϑj(t). (A.31)\\nSince we use truncated backpropagation we have\\nϑj(t)=∑\\ni:ino gate and no memory cellwicjϑi(t+1);', 'dense_vector': [-0.10033991932868958, -0.007119225338101387, 0.07028555870056152, 0.03161114826798439, 0.03413110971450806, 0.0256208423525095, 0.006864513736218214, 0.0349518321454525, -0.033738356083631516, -0.002176736481487751, 0.04873434454202652, 0.03632562234997749, 0.08198003470897675, -0.14115124940872192, -0.07060295343399048, 0.03720125928521156, -0.0841202363371849, 0.05203477665781975, -0.05114239454269409, -0.10289100557565689, -0.046795785427093506, 0.019750911742448807, -0.04652189463376999, 0.03366220369935036, 0.02080460824072361, 0.06638217717409134, -0.012690271250903606, 0.004272415302693844, 0.0005043576238676906, -0.002746909623965621, -0.029954714700579643, -0.019327253103256226, -0.040922604501247406, -0.057601045817136765, -0.03210081532597542, 0.07881215214729309, -0.05733714997768402, -0.044015273451805115, 0.011788352392613888, -0.017768915742635727, 0.0243956558406353, 0.008569170720875263, 0.021503331139683723, 0.03524958714842796, 0.07217127084732056, -0.03787819668650627, 0.02312014065682888, -0.06953173130750656, -0.04826781153678894, 0.0021666737738996744, 0.07252462953329086, 0.09926879405975342, 0.014535442925989628, 0.07352421432733536, 0.0175457950681448, 0.010626818984746933, -0.03330710157752037, 0.04150671884417534, -0.053149838000535965, 0.0142643628641963, 0.005250413902103901, -0.06631460785865784, -0.07193152606487274, -0.06262503564357758, 0.03295740485191345, -0.05944693461060524, 0.03432141989469528, -0.03806650638580322, 0.02609935589134693, 0.034522660076618195, -0.0542883463203907, 0.013580591417849064, -0.03164486959576607, 0.05248499661684036, 0.010077332146465778, -0.00075726886279881, 0.09438470751047134, 0.04190821573138237, -0.005640722345560789, -0.10506376624107361, 0.05995332822203636, 0.00159241643268615, 0.03252842277288437, -0.09765758365392685, 0.0066161309368908405, 0.03911008685827255, -0.0895739272236824, -0.019695620983839035, 0.0630813017487526, 0.030205700546503067, 0.0949888676404953, -0.07014880329370499, -0.07961652427911758, 0.03484412655234337, 0.020992498844861984, -0.027731966227293015, -0.019095279276371002, 0.043340738862752914, -0.020360279828310013, 0.08132302761077881, -0.013041882775723934, -0.012314415536820889, 0.017581121996045113, 0.04958954453468323, 0.045701734721660614, 0.06403730064630508, 0.06019226834177971, -0.005981591064482927, 0.08188038319349289, -0.05203111842274666, -0.06127786263823509, -0.07514941692352295, 0.03504161164164543, 0.0450931042432785, 0.04089582338929176, -0.03564000874757767, 0.018433164805173874, -0.020154742524027824, 0.04322504997253418, -0.02449404075741768, -0.054620660841464996, -0.021558932960033417, -0.049132224172353745, 0.03456014394760132, -0.029226524755358696, -0.06471721082925797, -0.01948244869709015, 6.450519372665549e-33, -0.03143230080604553, 0.051968980580568314, -0.0001580068637849763, -0.05668339878320694, -0.0031294289510697126, 0.05992213636636734, 0.020298052579164505, -0.05579341948032379, -0.05425084009766579, -0.03581743687391281, -0.08031071722507477, -0.10388005524873734, -0.10943623632192612, -0.005191785749047995, 0.021858422085642815, -0.06652919203042984, 0.02805340476334095, 0.023991581052541733, -0.025847705081105232, -0.033942967653274536, 0.0730239748954773, -0.02377953752875328, -0.06516213715076447, -0.006324220914393663, -0.04053426906466484, 0.01987536810338497, 0.07564791291952133, 0.009388117119669914, -0.04375222697854042, 0.054999690502882004, -0.014500198885798454, -0.029616156592965126, -0.04587168991565704, -0.014211096800863743, 0.02540087327361107, -0.0579887256026268, 0.047281209379434586, 0.05820901319384575, 0.02741113305091858, -0.07202450186014175, -0.03523394465446472, 0.01205094251781702, -0.06293923407793045, 0.03349867835640907, -0.08198529481887817, -0.1341937929391861, 0.047899965196847916, 0.05644620582461357, 0.014756448566913605, 0.021010085940361023, -0.01928049512207508, -0.015290378592908382, 0.027519842609763145, -0.08603495359420776, -0.026985617354512215, 0.002235835650935769, 0.029003731906414032, 0.11333158612251282, 0.04075351729989052, 0.049967505037784576, 0.04929927736520767, 0.013430509716272354, -0.006139608100056648, 0.020830661058425903, -0.006892696022987366, 0.08694843202829361, -0.09875326603651047, 0.032339803874492645, 0.0021559512242674828, -0.065060555934906, -0.0047880783677101135, 0.006335776764899492, -0.0062990449368953705, -0.03228416666388512, 0.12167609483003616, -0.06593360751867294, 0.016639819368720055, -0.034216053783893585, -0.023761723190546036, -0.05871251970529556, -0.08506249636411667, 0.03500493988394737, -0.02286211960017681, 0.024595268070697784, -0.054686468094587326, 0.02147280052304268, 0.029265189543366432, -0.08335008472204208, -0.01670306921005249, 0.0720573142170906, -0.10141319036483765, -0.009889607317745686, 0.04472830146551132, 0.07954584807157516, -0.021929671987891197, -6.780179416513966e-33, 0.008595721796154976, 0.0835871770977974, 0.015459522604942322, -0.060180652886629105, -0.05912753567099571, -0.00590706430375576, 0.00682841008529067, 0.06428385525941849, -0.0022258837707340717, -0.008299291133880615, 0.003526222426444292, 0.027700690552592278, -0.0992712453007698, 0.011365856975317001, -0.010949001647531986, -0.011275923810899258, -0.0026388894766569138, 0.015148869715631008, -0.045318666845560074, 0.03528977558016777, 0.021060645580291748, 0.013528875075280666, -0.08531086146831512, 0.0618334636092186, -0.07020256668329239, 0.02040884643793106, -0.007653424981981516, 0.1987949162721634, 0.012313107959926128, -0.002371747512370348, -0.020513148978352547, -0.08857201039791107, -0.055039163678884506, 0.134193554520607, -0.01520618051290512, -0.00024007463071029633, 0.015455502085387707, -0.018503613770008087, 0.007150932680815458, 0.050521474331617355, 0.07177939265966415, -0.026633840054273605, -0.043928734958171844, 0.0382281132042408, 0.04182177037000656, -0.0026198087725788355, -0.05622623488306999, 0.01122738141566515, -0.021031467244029045, -0.04794463515281677, 0.04629940167069435, 0.006450362969189882, -0.004406812600791454, 0.045654214918613434, -0.02632506564259529, 0.0900697261095047, 0.06953015923500061, 0.027556326240301132, 0.0035960141103714705, -0.03551442548632622, -0.1279090940952301, -0.025949664413928986, 0.05989592894911766, 0.0025210564490407705, 0.03133058547973633, 0.008874120190739632, 0.01312684640288353, 0.04950261488556862, 0.1535431444644928, -0.043612588196992874, 0.010756966657936573, 0.09817823022603989, -0.062299054116010666, -0.012445565313100815, 0.12410036474466324, -0.03335483744740486, -0.002042278880253434, -0.07830820232629776, 0.055221688002347946, -0.020587895065546036, -0.05304073542356491, 0.021713659167289734, 0.0094761922955513, 0.023268381133675575, 0.05419516935944557, -0.04591473564505577, 0.07301629334688187, 0.07828884571790695, -0.06102767959237099, -0.023874003440141678, 0.06426949054002762, 0.030258793383836746, 0.03400127589702606, 0.033998750150203705, 0.004774510394781828, -4.868180880635009e-08, -0.0366278812289238, -0.034751612693071365, 0.015189397148787975, 0.026772134006023407, 0.098362036049366, -0.039868686348199844, -0.04273867607116699, -0.04079831391572952, 0.06836701929569244, -0.08795905858278275, 0.015599497593939304, -0.017524346709251404, -0.08625312894582748, -0.10062676668167114, -0.0647365152835846, 0.012928829528391361, -0.0765267014503479, -0.001431593089364469, -0.0010824871715158224, 0.018723079934716225, 0.020520299673080444, 0.0014703263295814395, -0.0015690731815993786, 0.015231397934257984, 0.021358534693717957, -0.06450994312763214, -0.08211690932512283, 0.05025507137179375, -0.01870499923825264, 0.01886105164885521, 0.02147553116083145, 0.04003489017486572, 0.034638550132513046, 0.05005347356200218, 0.022208405658602715, 0.0773678570985794, 0.07946189492940903, 0.015279687941074371, -0.025280943140387535, 0.05261433869600296, 0.0036650793626904488, 0.01957004889845848, 0.04785434156656265, -0.011930043809115887, 0.005193325690925121, 0.012289468199014664, -0.05692214518785477, 0.001677648862823844, 0.00704506179317832, -0.03163936361670494, 0.017763791605830193, 9.966309335140977e-06, -0.01183952484279871, 0.009683980606496334, 0.027278637513518333, -0.09220462292432785, -0.030210310593247414, -0.06202823296189308, -0.043866999447345734, 0.02596942149102688, -0.0035875807516276836, 0.1314031332731247, -0.027602843940258026, -0.008227089419960976]}}, {'_index': 'temp', '_id': '3', '_score': 6.496142, '_source': {'content': '1 Introduction\\nIn principle, recurrent networks can use their feedback connections to store\\nrepresentations of recent input events in the form of activations (short-termmemory, as opposed to long-term memory embodied by slowly changingweights). This is potentially signiﬁcant for many applications, includingspeech processing, non-Markovian control, and music composition (Mozer,1992). The most widely used algorithms for learning what to put in short-term memory, however, take too much time or do not work well at all, espe-cially when minimal time lags between inputs and corresponding teachersignals are long. Although theoretically fascinating, existing methods donot provide clear practical advantages over, say, backpropagation in feed-forward nets with limited time windows. This article reviews an analysis ofthe problem and suggests a remedy.\\nNeural Computation 9, 1735–1780 (1997) c⃝1997 Massachusetts Institute of Technology', 'dense_vector': [-0.01277732290327549, -0.09065468609333038, 0.019510280340909958, 0.008706850931048393, -0.03550518676638603, 0.14896747469902039, -0.014574076980352402, -0.0484461709856987, 0.05116504058241844, -0.0662790909409523, -0.04730113968253136, 0.05752575397491455, -0.0053769974038004875, -0.06319764256477356, -0.09004618227481842, 0.016758305951952934, 0.018112169578671455, 0.09003997594118118, -0.0022805940825492144, -0.10317905247211456, -0.05263878032565117, -0.031241772696375847, -0.018229439854621887, 0.027587464079260826, -0.007840902544558048, 0.05193871259689331, -0.04914391413331032, -0.01387485396116972, 0.07534689456224442, -0.02240467630326748, 0.05269324779510498, -0.01563512347638607, -0.0013085472164675593, -0.03175995126366615, -0.107346311211586, 0.08142286539077759, -0.11542318761348724, -0.0019126397091895342, -0.09022402763366699, -0.04144369438290596, 0.04461488872766495, 0.01824779435992241, -0.019490845501422882, 0.028387829661369324, 0.0051154689863324165, -0.022073473781347275, 0.022692661732435226, -0.02482149377465248, -0.07944850623607635, 0.03609514236450195, -0.030105167999863625, 0.030142314732074738, 0.000219600202399306, 0.03594246506690979, 0.004026308190077543, -0.0001367025397485122, 0.042352303862571716, 0.10258743911981583, -0.08445197343826294, -0.020489158108830452, -0.02129133604466915, -0.0946168527007103, -0.02484089508652687, -0.0920000821352005, -0.03646650165319443, 0.003058486618101597, -0.016325214877724648, 0.03487016633152962, 0.0265735425055027, 0.036422744393348694, 0.0304948091506958, 0.05970878154039383, -0.034822870045900345, 0.02096598967909813, -0.0037934882566332817, 0.0018889475613832474, 0.09696375578641891, -0.04108799621462822, 0.00621292507275939, -0.04407612979412079, 0.11704419553279877, 0.0013658986426889896, 0.04001324623823166, -0.08808687329292297, 0.07839298993349075, 0.011659975163638592, -0.06244639679789543, 0.058801259845495224, 0.0018721554661169648, -0.050619084388017654, 0.014116642996668816, -0.0910075381398201, 0.04812314733862877, -0.03336873650550842, 0.06557140499353409, 0.05063748359680176, -0.0004723295569419861, -0.011175479739904404, -0.020925765857100487, 0.08459066599607468, -0.02796171046793461, 0.06339739263057709, -0.009054115042090416, 0.028688762336969376, 0.05352336913347244, 0.02667204663157463, -0.002987818792462349, 0.03402258828282356, 0.01720893755555153, -0.09623701870441437, -0.012057995423674583, 0.009836339391767979, 0.0568750835955143, 0.08137814700603485, 0.05265939608216286, -0.0720265731215477, 0.0656590610742569, -0.05837513506412506, 0.0371318943798542, 0.07756055146455765, -0.027853192761540413, -0.005841163452714682, -0.10318773239850998, -0.013437985442578793, 0.0031863439362496138, -0.04777512699365616, -0.01910921186208725, 5.313063651693886e-33, -0.026514515280723572, 0.010198505595326424, 0.013593818061053753, -0.03850799798965454, 0.041482023894786835, -0.011743979528546333, 0.04808096960186958, 0.04266974329948425, 0.05905451253056526, -0.04469646140933037, 0.007417347747832537, -0.028956694528460503, -0.03741145879030228, 0.04276794567704201, 0.017422430217266083, -0.08713199198246002, -0.03393656015396118, 0.04350430145859718, 0.0499153658747673, -0.13852393627166748, -0.04766690358519554, -0.03826241195201874, 0.009730735793709755, -0.0319545716047287, 0.025129791349172592, 0.00991312600672245, 0.033954162150621414, -0.01004958525300026, 0.011364402249455452, 0.010310995392501354, -0.009574215859174728, 0.012378951534628868, -0.08405525982379913, -0.038454338908195496, 0.024142256006598473, -0.04084949940443039, 0.048224616795778275, -0.023885313421487808, 0.06471730023622513, -0.04106217995285988, 0.002266952069476247, -0.013090004213154316, -0.03276441991329193, 0.03547422215342522, -0.10285090655088425, -0.1226431354880333, 0.019398579373955727, 0.050056442618370056, -0.038516536355018616, -0.013981275260448456, 0.03493744507431984, 0.010439423844218254, -0.09278678148984909, -0.10354256629943848, -0.008742899633944035, -0.016275884583592415, 0.012509642168879509, 0.10924473404884338, 0.015033002011477947, 0.09477558732032776, 0.06500612944364548, 0.06697265803813934, 0.02910517156124115, 0.055426884442567825, 0.009868521243333817, 0.07492858171463013, -0.06764575839042664, 0.03824393451213837, 0.060474421828985214, -0.011111312545835972, 0.03909232094883919, 0.036452766507864, 0.005241216626018286, -0.04944724962115288, 0.049313899129629135, -0.02960505150258541, 0.029520438984036446, -0.10909591615200043, -0.05439414083957672, 0.05707909166812897, 0.01837567798793316, -0.031423166394233704, -0.03455504775047302, 0.04765598848462105, -0.02614830620586872, -0.023762790486216545, 0.11277759820222855, -0.05915951728820801, 0.021310627460479736, 0.03654053062200546, -0.11862143129110336, -0.0157019030302763, 0.034736037254333496, 0.0318097323179245, -0.04682276025414467, -5.1507760036453146e-33, -0.010003557428717613, 0.026235831901431084, -0.0396895706653595, 0.042273975908756256, 0.024610422551631927, -0.030355550348758698, -0.0866575837135315, 0.0669313594698906, -0.008667497895658016, -0.002062959363684058, -0.002778430702164769, -0.03666181489825249, -0.01121035497635603, -0.035458117723464966, 0.0038081160746514797, -0.030002104118466377, 0.016757719218730927, -0.026725243777036667, 0.028346607461571693, -0.07231168448925018, 0.029401583597064018, 0.04709909111261368, -0.06819074600934982, -0.006949601229280233, -0.036638762801885605, -0.04440751671791077, 0.008295946754515171, 0.10590074211359024, -0.05421406030654907, 0.0027081044390797615, -0.09584101289510727, -0.039347536861896515, -0.010662723332643509, -0.033086467534303665, -0.11035680770874023, 0.05720356106758118, 0.007680365350097418, -0.006455907132476568, -0.024692585691809654, 0.014071711339056492, 0.11390003561973572, -0.01835424266755581, 0.0033308437559753656, -0.02167678065598011, 0.00016295065870508552, -0.03187289834022522, -0.15069608390331268, 0.08762074261903763, 0.04179387912154198, 0.06583455950021744, 0.03667738661170006, -0.04968321695923805, -0.04047389701008797, -0.0602375790476799, -0.0774320736527443, 0.040368109941482544, 0.027550755068659782, -0.006759308744221926, 0.0018721746746450663, 0.051226161420345306, -0.03176038712263107, -0.04612254723906517, -0.037959951907396317, -0.055504173040390015, 0.06617307662963867, 0.03852534294128418, 0.0272185280919075, -0.018288996070623398, 0.12112020701169968, 0.0004568918375298381, 0.10874670743942261, 0.08544332534074783, -0.011418226175010204, 0.036682143807411194, -0.066282257437706, 0.01748020574450493, -0.05073373764753342, -0.024242686107754707, -0.05200197547674179, -0.07563859224319458, -0.027422122657299042, 0.010789540596306324, 0.038792021572589874, 0.016999902203679085, 0.06253214180469513, 0.12969906628131866, 0.014532944187521935, 0.021695535629987717, 0.05175541713833809, -0.03895774111151695, 0.01622527651488781, 0.04545159265398979, 0.04026644676923752, 0.03153297305107117, -0.006848456338047981, -5.113563261716081e-08, -0.02475082129240036, 0.04322627931833267, 0.04113483428955078, 0.07947924733161926, 0.07721905410289764, -0.03347582370042801, 0.08009554445743561, -0.05997199937701225, 0.02851412445306778, -0.04407210275530815, 0.08843454718589783, -0.07199519872665405, 0.02540113590657711, -0.05261354520916939, 0.013553404249250889, 0.07001141458749771, 0.028747139498591423, -0.04114197567105293, -0.014042518101632595, -0.04213960841298103, 0.0980461835861206, 0.05142507702112198, 0.017278769984841347, 0.06785392761230469, 0.02297649346292019, 0.0017788695404306054, 0.04672301933169365, 0.05526489391922951, 0.010314424522221088, 0.00774728786200285, 0.019684089347720146, 0.04814908280968666, -0.0012048057978972793, 0.015187051147222519, 0.014706723392009735, 0.008107678033411503, 0.05278313159942627, -0.01954839564859867, -0.09830169379711151, 0.011289441026747227, -0.026797499507665634, 0.02793513610959053, -0.03928373381495476, -0.0006853258237242699, -0.05161644518375397, -0.001963601680472493, -0.0032808638643473387, -0.1032225713133812, 0.04234492406249046, 0.00882539339363575, 0.07255770266056061, -0.036078110337257385, 0.049058083444833755, -0.0021137024741619825, 0.07008911669254303, 0.035515107214450836, 0.026300769299268723, -0.028111707419157028, -0.01638149656355381, 0.03645133972167969, -0.04515571519732475, 0.08407182991504669, -0.05041073262691498, 0.004384373314678669]}}, {'_index': 'temp', '_id': '40', '_score': 6.2874904, '_source': {'content': '4.3 Network Topology. We use networks with one input layer, one hid-\\nden layer, and one output layer. The (fully) self-connected hidden layercontains memory cells and corresponding gate units (for convenience, werefer to both memory cells and gate units as being located in the hiddenlayer). The hidden layer may also contain conventional hidden units pro-viding inputs to gate units and memory cells. All units (except for gate units)in all layers have directed connections (serve as inputs) to all units in thelayer above (or to all higher layers; see experiments 2a and 2b).\\n4.4 Memory Cell Blocks. Smemory cells sharing the same input gate\\nand the same output gate form a structure called a memory cell block of sizeS. Memory cell blocks facilitate information storage. As with conventional', 'dense_vector': [-0.008177921175956726, -0.023994075134396553, -0.08549487590789795, 0.09318530559539795, -0.017197879031300545, 0.0374918207526207, -0.010428915731608868, 0.002904297551140189, 0.09674201160669327, -0.032401617616415024, 0.03315538167953491, 0.027765657752752304, 0.025368524715304375, -0.0484619177877903, -0.007246014662086964, 0.044370222836732864, 0.0025280804838985205, 0.031141411513090134, -0.03984522446990013, -0.04399958252906799, 0.05274692550301552, -0.0904201939702034, -0.004490992054343224, -0.017893871292471886, -0.026157917454838753, -0.024497540667653084, -0.06458604335784912, -0.0756053477525711, 0.038766875863075256, -0.0792774111032486, 0.014361384324729443, -0.009895145893096924, -0.012823610566556454, 0.029216742143034935, -0.05667226016521454, -0.03965456411242485, -0.02623756229877472, -0.005340159870684147, -0.07381348311901093, -0.11001011729240417, 0.027833126485347748, 0.06106502190232277, -0.012018591165542603, 0.03633315488696098, -0.03435027226805687, 0.05183594301342964, -0.005875364411622286, -0.0355367548763752, -0.04851662367582321, -0.06678196042776108, -0.01651712879538536, 0.030207503587007523, -0.012569666840136051, 0.10764407366514206, 0.032617855817079544, 0.05507523566484451, -0.03135526552796364, 0.06730237603187561, -0.06342659145593643, 0.09379010647535324, 0.009557160548865795, -0.02236349694430828, 0.06282445788383484, -0.011076833121478558, 0.05009138584136963, -0.0037295075599104166, 0.08487531542778015, 0.047412533313035965, 0.01707891747355461, -0.003490501781925559, 0.06411208212375641, 0.008348671719431877, -0.013321597129106522, 0.002604288049042225, 0.0911896824836731, 0.04878858104348183, -0.028007209300994873, 0.027409350499510765, 0.06573133915662766, -0.05344216525554657, 0.03453050181269646, 0.07778767496347427, 0.006826397497206926, -0.009315812028944492, 0.026089681312441826, -0.03314958140254021, -0.030015673488378525, -0.012872068211436272, -0.07845538854598999, -0.06096532940864563, -0.047706179320812225, -0.03447403386235237, -0.07234970480203629, 0.018302209675312042, 0.016217311844229698, -0.09032423049211502, -0.012099559418857098, -0.0014028502628207207, -0.043240174651145935, -0.01869775354862213, 0.08490996062755585, 0.07087254524230957, 0.02768990583717823, 0.029112672433257103, 0.010715769603848457, 0.005951354745775461, 0.056228622794151306, 0.05161841958761215, -0.043364591896533966, -0.017132624983787537, -0.005330652464181185, 0.001807115855626762, -0.04781990870833397, 0.03571135550737381, -0.014324896037578583, -0.098566435277462, 0.014365151524543762, -0.033965274691581726, 0.08329547196626663, 0.02767310105264187, -0.10388229787349701, 0.015664704144001007, -0.03692511096596718, -0.062355998903512955, -0.03356124088168144, -0.029141854494810104, -0.0820363312959671, 3.9864991750077334e-33, -0.01921902596950531, -0.011858847923576832, -0.06773968786001205, 0.004509531427174807, 0.0077851261012256145, -0.022182786837220192, 0.0226064994931221, -0.015738392248749733, 0.002457723254337907, 0.008486265316605568, -0.06914500147104263, -0.07285532355308533, 0.006060190498828888, 0.08797016739845276, 0.07566006481647491, -0.07363710552453995, -0.06704270839691162, 0.041573282331228256, 0.07428933680057526, -0.0855453759431839, -0.012941298075020313, 0.052154913544654846, -0.015111944638192654, -0.03864332661032677, 0.10269980877637863, -0.030976075679063797, 0.008509143255650997, -0.058125048875808716, -0.013106712140142918, 0.028116531670093536, -0.051778364926576614, 0.07745946943759918, 0.035266220569610596, -0.06812117248773575, 0.05277856066823006, 0.05223602056503296, -0.015341502614319324, -0.049400486052036285, 0.06420432031154633, -0.06567462533712387, -0.01321820542216301, -0.010784815065562725, 0.02017245441675186, -0.0653429925441742, -0.09956711530685425, -0.04346731677651405, -0.0644422397017479, 0.030581312254071236, -0.06126935034990311, -0.0594712570309639, 0.004405910614877939, 0.003174396464601159, -0.011705832555890083, -0.05321311578154564, 0.05769310146570206, -0.0348711796104908, 0.020340781658887863, 0.05208835005760193, 0.09701860696077347, 0.1390170305967331, -0.009699770249426365, 0.03795092552900314, -0.06741801649332047, 0.08881436288356781, -0.010564868338406086, 0.06346094608306885, -0.02182789519429207, 0.0058503826148808, 0.0008627382339909673, -0.0777931660413742, 0.01006827037781477, 0.04609069228172302, -0.049487993121147156, -0.015591911971569061, -0.025039399042725563, -0.01643502153456211, -0.05432360991835594, -0.08474015444517136, -0.0796724185347557, 0.04252881184220314, 0.05223894119262695, -0.042186424136161804, -0.11743622273206711, 0.09404977411031723, -0.10015077143907547, -0.01277556549757719, 0.07015464454889297, 0.0028088970575481653, -0.0437270849943161, 0.03845391795039177, 0.07732731103897095, -0.06697191298007965, 0.11809481680393219, 0.0188286229968071, -0.08878818154335022, -4.731688390348168e-33, -0.02911965921521187, 0.07868442684412003, -0.046597931534051895, -0.005966567434370518, -0.004953385330736637, 0.0451851487159729, 0.060414869338274, -0.04230653867125511, -0.050469476729631424, 0.06715919077396393, 0.025747159495949745, 0.04028298333287239, 0.030201824381947517, -0.016860054805874825, -0.023187316954135895, 0.00882874708622694, -0.00902080163359642, -0.07337598502635956, 0.04754600673913956, -0.050476621836423874, -0.01264704018831253, 0.0882745236158371, -0.027132932096719742, 0.08505829423666, -0.014206651598215103, -0.007797034457325935, -0.01620076596736908, 0.10117995738983154, 0.021523065865039825, 0.05739721655845642, -0.004445329308509827, -0.023450633510947227, 0.021805621683597565, 0.021435201168060303, -0.004164527636021376, -0.030726730823516846, 0.06030621752142906, -0.06890997290611267, -0.013302162289619446, -0.07512999325990677, 0.02297348715364933, 0.05217914655804634, -0.024004796519875526, 0.030311977490782738, 0.04109876975417137, 0.06477238237857819, -0.0691845640540123, 0.047446563839912415, -0.0686379224061966, -0.02657330222427845, -0.06701882183551788, -0.022220781072974205, -0.028752369806170464, -0.08327431231737137, -0.01533470954746008, 0.04658889025449753, -0.020982064306735992, 0.05987133830785751, 0.0666310265660286, -0.008280598558485508, 0.025803176686167717, -0.1494635045528412, 0.02182774804532528, -0.03502877801656723, -0.033345434814691544, 0.008972907438874245, 0.06030403822660446, 0.039759401232004166, -0.027068832889199257, 0.0005517735844478011, 0.05647813156247139, 0.0692756250500679, 0.0120994932949543, -0.05808621644973755, -0.014042689464986324, 0.022862501442432404, -0.006214213091880083, -0.03783610463142395, 0.012231501750648022, 0.020210858434438705, -0.09059783816337585, -0.003131951903924346, 0.009263313375413418, -0.035603735595941544, 0.11297070980072021, 0.02153887040913105, 0.03163355588912964, 0.03602335602045059, -0.002299818443134427, -0.032626524567604065, -0.049266789108514786, -0.010130476206541061, -0.030125491321086884, 0.011812652461230755, -0.02883000485599041, -4.8538943531184486e-08, 0.027343714609742165, -0.02118239738047123, 0.09010512381792068, -0.004328129813075066, -0.03900546208024025, -0.12010030448436737, 0.1155480369925499, 0.06406659632921219, 0.05215395987033844, 0.01307777501642704, 0.01151286717504263, -0.009030580520629883, -0.10034213960170746, -0.050913870334625244, 0.11445947736501694, 0.10520900785923004, -0.04797963798046112, -0.03235067427158356, 0.029118221253156662, -0.09733863919973373, 0.0049626147374510765, -0.031567059457302094, -0.034779179841279984, 0.08855907618999481, 0.07590692490339279, -0.012880487367510796, 0.03530767560005188, 0.060012854635715485, 0.011793682351708412, 0.016745565459132195, 0.03013300523161888, 0.01565091870725155, 0.04209483414888382, 0.029953792691230774, -0.014449967071413994, 0.08675029128789902, 0.04969306290149689, 0.07568222284317017, 0.0019581480883061886, 0.0435141883790493, -0.012371638789772987, -0.12778834998607635, 0.006308150012046099, 0.025554727762937546, -0.010521708987653255, 0.0609639510512352, -0.027397237718105316, 0.03710656613111496, -0.07780738174915314, 0.0004583351546898484, -0.04349299520254135, 0.030283909291028976, -0.017287855967879295, 0.07086941599845886, 0.06678985059261322, -0.03529142215847969, -0.0251734871417284, -0.038320012390613556, 0.03804035857319832, 0.0402495451271534, 0.013537229038774967, 0.04332935810089111, -0.024272115901112556, 0.028703046962618828]}}, {'_index': 'temp', '_id': '33', '_score': 6.04898, '_source': {'content': 'cells, or even conventional hidden units if there are any (see section 4.3). Allthese different types of units may convey useful information about the cur-rent state of the net. For instance, an input gate (output gate) may use inputsfrom other memory cells to decide whether to store (access) certain infor-mation in its memory cell. There even may be recurrent self-connectionslikew\\ncjcj. It is up to the user to deﬁne the network topology. See Figure 2 for\\nan example.\\nAt time t,cj’s output ycj(t)is computed as\\nycj(t)=youtj(t)h(scj(t)),\\nwhere the internal state scj(t)is\\nscj(0)=0,scj(t)=scj(t−1)+yinj(t)g(\\nnetcj(t))\\nfort>0.\\nThe differentiable function gsquashes netcj; the differentiable function h\\nscales memory cell outputs computed from the internal state scj.\\n4.2 Why Gate Units? To avoid input weight conﬂicts, injcontrols the\\nerror ﬂow to memory cell cj’s input connections wcji. To circumvent cj’s\\noutput weight conﬂicts, outjcontrols the error ﬂow from unit j’s output', 'dense_vector': [-0.09888125211000443, -0.05292243883013725, -0.10601839423179626, 0.06752384454011917, -0.052399467676877975, 0.057058185338974, -0.014245404861867428, 0.02681618742644787, 0.1400570571422577, -0.0022876220755279064, 0.02078537829220295, 0.008019380271434784, 0.02541062794625759, -0.045121680945158005, -0.03685100004076958, -0.034149378538131714, 0.028998294845223427, 0.05513371154665947, -0.08711247891187668, -0.06498770415782928, 0.04462524875998497, -0.02179587446153164, -0.0233007799834013, -0.02794368751347065, -0.033940475434064865, -0.006312248762696981, -0.037124671041965485, 0.02553333155810833, 0.002684341510757804, -0.055663276463747025, -0.02090032398700714, 0.04266446828842163, -0.009700734168291092, 0.01888306625187397, -0.06118719279766083, -0.008349833078682423, -0.015558416955173016, 0.007503809407353401, 0.006341612432152033, -0.04097290709614754, 0.0020847287960350513, 0.029258105903863907, 0.018879864364862442, 0.06012822687625885, 0.020454933866858482, 0.024467429146170616, -0.004047033376991749, -0.08114549517631531, -0.08244281262159348, -0.02992456778883934, 0.041763391345739365, 0.08437105268239975, -0.01802494376897812, 0.06577602028846741, 0.09365255385637283, 0.01963570900261402, 0.034047458320856094, 0.05906226858496666, -0.058749325573444366, 0.06602267920970917, -0.041678089648485184, -0.016690025106072426, 0.0008012382895685732, 0.0017046048305928707, 0.03204287961125374, -0.048013851046562195, 0.09147752076387405, 0.03210069611668587, 0.05075473338365555, -0.02643483132123947, 0.019877968356013298, -0.0021056830883026123, -0.00732157239690423, -0.02038411796092987, 0.024746818467974663, -0.008604723960161209, 0.044015176594257355, 0.03581435978412628, 0.06182226166129112, -0.02608572505414486, 0.05065751075744629, 0.10747909545898438, 0.05229327455163002, 0.007729654666036367, -0.007172871381044388, 0.0194648876786232, -0.018068119883537292, 0.05852533504366875, 0.08486674726009369, -0.008725377731025219, 0.016203468665480614, -0.003416437190026045, -0.12494901567697525, -0.046413831412792206, 0.03309408202767372, -0.06380826234817505, 0.025002921000123024, 0.0066702100448310375, -0.008532523177564144, -0.014343108050525188, 0.06066112965345383, 0.015282402746379375, 0.11594066768884659, 0.05269321799278259, 0.056242283433675766, 0.05889103561639786, 0.07273485511541367, 0.08916477113962173, 0.08361146599054337, -0.005069816019386053, -0.028761673718690872, -0.013891609385609627, -0.06200667843222618, 0.03875206410884857, 0.029530437663197517, -0.14543627202510834, 0.07185091078281403, -0.019048044458031654, 0.08971758931875229, 0.020549166947603226, -0.07191232591867447, -0.007354184985160828, -0.05365823581814766, -0.031670667231082916, -0.0195171982049942, 0.026844147592782974, -0.04059498757123947, 6.0752505153472106e-33, -0.10057131201028824, -0.024905528873205185, -0.019193576648831367, -0.062048569321632385, -0.002716868184506893, -0.006820372771471739, -0.005675687920302153, -0.007481153588742018, 0.06635742634534836, 0.01004373375326395, -0.13445648550987244, 0.04954042658209801, -0.021837564185261726, 0.05268934741616249, 0.0569952167570591, -0.0030588603112846613, -0.0737077072262764, -0.024062233045697212, 0.03168606013059616, -0.08467411994934082, 0.02754942700266838, 0.05101916939020157, 0.013894865289330482, -0.0354422926902771, 0.027831869199872017, -0.08819364011287689, 0.021615123376250267, 0.01625118963420391, -0.05434393510222435, 0.005780404899269342, 0.013116655871272087, 0.01911655068397522, -0.02479446865618229, -0.06794749945402145, 0.05960647761821747, 0.00615547364577651, 0.01523828785866499, -0.04581836238503456, 0.05905349180102348, -0.1281270682811737, 0.008402583189308643, 0.01027996651828289, 0.007647903170436621, 0.011318361386656761, -0.10219624638557434, -0.05542219430208206, -0.0009215649915859103, 0.0032919838558882475, -0.11629325151443481, -0.04251585155725479, -0.0031259963288903236, 0.008748293854296207, -0.045281294733285904, -0.10595207661390305, 0.011118391528725624, 0.006401801016181707, 0.05847629904747009, 0.05152776464819908, 0.052628565579652786, 0.16708986461162567, -0.08272256702184677, 0.017414458096027374, -0.015233194455504417, 0.0673312246799469, 0.0029979459941387177, 0.0632907897233963, -0.10641803592443466, -0.010216818191111088, -0.0009885742329061031, -0.05702659487724304, -0.005144232418388128, 0.03517846390604973, -0.01740303263068199, -0.07546219229698181, 0.03081583045423031, -0.03223700821399689, 0.03831053152680397, -0.08218488097190857, -0.054288845509290695, 0.014975020661950111, -0.01891225576400757, -0.03932946175336838, -0.05751056969165802, 0.032511964440345764, -0.0548328272998333, -0.040229905396699905, 0.04673311114311218, -0.03907833620905876, -0.041453391313552856, 0.06077854707837105, 0.08581516891717911, -0.019755661487579346, 0.023539302870631218, 0.03433912619948387, -0.07259158790111542, -7.375036863379647e-33, -0.04922938346862793, 0.07124631106853485, -0.08206700533628464, -0.03684588521718979, -0.011025752872228622, 0.030341025441884995, -0.006562006194144487, -0.07366295903921127, 0.00697693508118391, -0.045909419655799866, 0.021330811083316803, 0.05876275151968002, 0.0102617796510458, 0.01015586219727993, -0.0048388466238975525, -0.07066910713911057, -0.02421335130929947, -0.04344454035162926, -0.08215212821960449, -0.0036442347336560488, 0.03992261737585068, 0.06544510275125504, -0.052481699734926224, 0.009223135188221931, -0.019173063337802887, 0.000278180610621348, -0.0844271332025528, 0.09691458940505981, -0.031227149069309235, 0.060100916773080826, 0.003990885801613331, -0.08442352712154388, -0.02721385285258293, 0.050063684582710266, 0.02068421058356762, -0.034843213856220245, 0.06120316684246063, -0.06052178889513016, 0.0032218804117292166, 0.028933752328157425, 0.07606256008148193, 0.04635525494813919, -0.029077066108584404, 0.013894747942686081, 0.0003816909156739712, 0.10004302859306335, -0.04279402270913124, 0.003354959422722459, -0.042247872799634933, -0.009667530655860901, -0.011448276229202747, 0.02309533767402172, -0.06225443258881569, 0.011999087408185005, -0.00395947927609086, 0.057918764650821686, 0.017259763553738594, -0.02475072257220745, 0.05791529268026352, -0.005805802997201681, -0.0524742417037487, -0.11181098222732544, -0.028242817148566246, 0.030411306768655777, -0.02390287257730961, 0.007584253326058388, 0.03760939836502075, 0.025123214349150658, 0.07535688579082489, -0.05040097236633301, -0.007238827645778656, 0.033198852092027664, 0.01985241286456585, -0.04777539521455765, -0.034153278917074203, -0.0526357926428318, 0.01936596818268299, -0.09335460513830185, -0.06526605784893036, -0.00579436868429184, -0.02108827792108059, 0.04273423179984093, 0.00840881746262312, -0.04822542890906334, 0.05739307031035423, -0.010035849176347256, 0.02489427663385868, 0.010906774550676346, 0.027334030717611313, -0.014951779507100582, 0.04274492338299751, -0.006984683219343424, -0.09480690956115723, 0.06109515205025673, -0.03665151447057724, -5.6617903254618795e-08, -0.030611228197813034, -0.02658567950129509, 0.07516905665397644, -0.003042202442884445, 0.05208893120288849, -0.08858945220708847, 0.0392642542719841, -0.018201714381575584, 0.0537988655269146, 0.09880592674016953, 0.07752680033445358, 0.014658136293292046, -0.10585759580135345, -0.07394909858703613, 0.0355197936296463, 0.08964955061674118, -0.03378092125058174, -0.05927085503935814, 0.04141068086028099, -0.04200097173452377, -0.017643487080931664, -0.04689984768629074, -0.043188948184251785, 0.11279904097318649, 0.02166079171001911, -0.051754120737314224, -0.05939863249659538, 0.03580630198121071, 0.009753577411174774, 0.03611938655376434, 0.015010330826044083, 0.08269751816987991, 0.05790780484676361, 0.10926780849695206, 0.024400660768151283, 0.05116043612360954, 0.041455112397670746, 0.09051550924777985, -0.005255675874650478, 0.09172023087739944, -0.026433920487761497, -0.05357922986149788, -0.03395157307386398, 0.01776091754436493, 0.03415274620056152, 0.06289628893136978, -0.06115114688873291, 0.036182329058647156, -0.03398735076189041, -0.0039473422802984715, -0.010318013839423656, 0.037170782685279846, -0.04978971928358078, 0.054852355271577835, 0.044173464179039, -0.03900948166847229, -0.06073860451579094, -0.06111270189285278, -0.022852454334497452, 0.05589212849736214, -0.043288346379995346, 0.0691835954785347, -0.008633862249553204, -0.05454825609922409]}}, {'_index': 'temp', '_id': '37', '_score': 6.024309, '_source': {'content': 'connections. In other words, the net can use injto decide when to keep or\\noverride information in memory cell cjand outjto decide when to access\\nmemory cell cjand when to prevent other units from being perturbed by cj\\n(see Figure 1).\\nError signals trapped within a memory cell’s CEC cannot change, but\\ndifferent error signals ﬂowing into the cell (at different times) via its out-put gate may get superimposed. The output gate will have to learn which', 'dense_vector': [-0.043934810906648636, -0.027974756434559822, -0.08032035827636719, 0.05481237918138504, -0.048891376703977585, 0.012704847380518913, 0.01109585352241993, 0.05803989619016647, 0.09977472573518753, -0.011481757275760174, -0.006690321024507284, 0.03179746866226196, 0.015486768446862698, -0.07472431659698486, -0.06668029725551605, 0.004823951981961727, 0.05506828799843788, 0.0849691703915596, -0.08663079887628555, -0.09221633523702621, 0.04973139241337776, -0.0392964631319046, -0.05407487228512764, -0.009139928966760635, -0.09420014917850494, -0.020444829016923904, -0.0004111443122383207, 0.03279390186071396, 0.02501007728278637, -0.09228194504976273, -0.029716212302446365, 0.05670075863599777, -0.10542944818735123, 0.07305406779050827, -0.07162926346063614, 0.024259287863969803, -0.05244050920009613, -0.02354925312101841, 0.031423330307006836, -0.10402075201272964, 0.011689052917063236, 0.024891352280974388, -0.013691172003746033, 0.02604551985859871, -0.02850344032049179, -0.00681267911568284, -0.05047659948468208, -0.05805443599820137, -0.0347200445830822, -0.06144007295370102, -0.011331516318023205, 0.04718668758869171, 0.013321487233042717, 0.03173910081386566, 0.02891150675714016, 0.07901304960250854, -0.04925975948572159, 0.06445875763893127, -0.04023350402712822, 0.09162646532058716, 0.012625625357031822, -0.033042263239622116, -0.028103435412049294, 0.042864732444286346, 0.04562640190124512, -0.0664513036608696, 0.06826487928628922, 0.026647500693798065, 0.016721095889806747, -0.0036958723794668913, 0.01643378473818302, -0.008050326257944107, -0.03942788764834404, 0.007153911516070366, 0.06513819843530655, 0.01786130480468273, -0.03323797509074211, -0.04605131596326828, 0.0442369319498539, -0.08513226360082626, 0.040320415049791336, 0.09388021379709244, 0.04051040858030319, -0.019705893471837044, 0.07285386323928833, 0.028431950137019157, 0.04097922891378403, 0.06754117459058762, 0.033342547714710236, 0.025322139263153076, 0.023724215105175972, -0.025513675063848495, -0.028604550287127495, 0.02346767671406269, -0.06283224374055862, -0.029398394748568535, 0.10082697123289108, 0.002867265371605754, -0.0011416943743824959, -0.013967528007924557, 0.07587134093046188, 0.04504866153001785, 0.013776319101452827, 0.041051238775253296, 0.051111672073602676, -0.09479576349258423, 0.07341547310352325, 0.013213323429226875, 0.03459332510828972, -0.04806335270404816, -0.010334155522286892, -0.013493577018380165, 0.010707247070968151, 0.009389746002852917, -0.005585170816630125, -0.03428863734006882, 0.05127685144543648, 0.07674825191497803, 0.04623306542634964, 0.03263537585735321, -0.11408596485853195, -0.022616738453507423, -0.004189808387309313, -0.021049248054623604, -0.0828433483839035, -0.007795568555593491, -0.08006354421377182, 5.5095221858368914e-33, -0.0558442659676075, -0.09673584252595901, -0.014758080244064331, -0.025864530354738235, -0.004988962784409523, 0.03901815786957741, -0.04958843067288399, -0.06084755063056946, 0.012191110290586948, -0.044052135199308395, -0.07758819311857224, -0.06525791436433792, 0.013060791417956352, 0.008524037897586823, 0.07040265947580338, -0.010481903329491615, -0.07139094918966293, -0.03491462394595146, 0.014746624045073986, -0.003837586147710681, -0.00048315877211280167, -0.058403316885232925, -0.015951860696077347, -0.019955620169639587, 0.0675165131688118, 0.022754808887839317, 0.01682327874004841, 0.05344897881150246, -0.012399137951433659, 0.03950149565935135, -0.002202862873673439, 0.011566530913114548, -0.043943244963884354, -0.013651029206812382, 0.06375397741794586, 0.008312329649925232, 0.05479717254638672, -0.041587527841329575, 0.025437023490667343, -0.09436844289302826, -0.06258201599121094, -0.011599894613027573, -0.05429927259683609, 0.022095216438174248, -0.07993114739656448, -0.0821363553404808, 0.000995551235973835, 0.028092900291085243, -0.0319279208779335, 0.01829761452972889, -0.006682209204882383, -0.020353656262159348, 0.01538221538066864, -0.04473486915230751, 0.00608851108700037, 0.04077322408556938, 0.023081697523593903, 0.014165698550641537, 0.020610468462109566, 0.12773023545742035, -0.013857480138540268, -0.026440734043717384, -0.11602744460105896, 0.056912217289209366, 0.06652437895536423, 0.11921083182096481, -0.07559269666671753, -0.04872376099228859, -0.058914829045534134, -0.0435018353164196, -0.09166178852319717, -0.007987068966031075, -0.02696128934621811, -0.0006197644397616386, -0.07176913321018219, -0.0123616186901927, -0.047360677272081375, -0.022898895666003227, -0.026567069813609123, -0.02401386946439743, 0.016729649156332016, -0.06972019374370575, -0.05446985736489296, 0.02502257004380226, -0.030856618657708168, 0.020866110920906067, 0.0009591703419573605, -0.02816924639046192, -0.051796481013298035, 0.05539768189191818, 0.0915704071521759, -0.0012350344331935048, 0.09358850121498108, 0.03568493574857712, -0.02518443576991558, -7.249301579511879e-33, -0.0555826835334301, 0.050570353865623474, -0.05090881884098053, 0.002430492779240012, -0.0005656775319948792, 0.021766433492302895, -0.004109604749828577, -0.08685807138681412, -0.015692519024014473, -0.05145011097192764, -2.5359135179314762e-05, 0.013965493999421597, -0.025481313467025757, 0.05580969899892807, -0.043079447001218796, -0.07348653674125671, -0.0538146086037159, -0.01014387421309948, -0.03939130902290344, 0.03177884966135025, 0.0054186079651117325, 0.029982732608914375, 0.010875049978494644, -0.00814373791217804, -0.0698327124118805, 0.04365065321326256, -0.11030809581279755, 0.06704258173704147, -0.0580926388502121, 0.03829735144972801, 0.0349150225520134, -0.007844679057598114, -0.0286529753357172, 0.0768372118473053, 0.030900606885552406, -0.04181169345974922, 0.06656987965106964, -0.04722364991903305, -0.011886571533977985, 0.028987852856516838, 0.060081396251916885, 0.11526331305503845, -0.06695973128080368, 0.05347917973995209, 0.04865202680230141, 0.0611138790845871, -0.058026958256959915, 0.01657060533761978, -0.01609227992594242, 0.007407015655189753, -0.01639173924922943, 0.027141107246279716, -0.001379699562676251, 0.042703431099653244, -0.01662525162100792, 0.06889696419239044, 0.04109985753893852, 0.002391310641542077, 0.0658825933933258, 0.0006404328742064536, -0.05513891577720642, -0.11387427151203156, 0.02668069675564766, -0.01183425821363926, 0.037099726498126984, 0.0783049538731575, 0.015148794278502464, 0.0584527812898159, 0.0899311974644661, -0.04579645022749901, 0.07461841404438019, -0.004855641629546881, -0.03943416103720665, -0.052768826484680176, -0.03555365279316902, -0.014770345762372017, -0.07842018455266953, -0.08232555538415909, -0.04974650591611862, 0.008805195800960064, -0.04234639182686806, 0.05819717049598694, -0.004920572973787785, 0.015423943288624287, 0.06718382984399796, -0.0008209609659388661, 0.020132051780819893, 0.015282959677278996, 0.020405136048793793, -0.059724580496549606, 0.0007443828508257866, 0.0646783858537674, -0.042059216648340225, 0.06662646681070328, -0.04063993692398071, -4.403356612669995e-08, -0.06459375470876694, -0.06593286246061325, 0.046938665211200714, -0.021200409159064293, 0.09850762039422989, -0.0841306671500206, -0.010216044262051582, -0.014773305505514145, 0.08629810065031052, -0.0017847627168521285, 0.011945309117436409, 0.0649988204240799, -0.074769988656044, 0.004352254793047905, 0.10272087156772614, 0.06826313585042953, 0.01922491192817688, -0.034171465784311295, 0.026933956891298294, -0.04732290655374527, -0.005527136847376823, -0.10647784918546677, -0.006179572083055973, 0.16787786781787872, 0.028619280084967613, -0.06412669271230698, -0.00013038853649049997, 0.13756005465984344, -0.033389702439308167, 0.02327549457550049, -0.018191561102867126, 0.016650812700390816, 0.08489546924829483, 0.09849805384874344, -0.0018548443913459778, 0.13209295272827148, 0.04217786341905594, 0.0016774531686678529, -0.0074016135185956955, 0.053993113338947296, -0.03816177695989609, -0.011614499613642693, -0.06030579283833504, 0.0660925954580307, 0.01268893200904131, 0.04480327293276787, -0.012827998958528042, 3.845461833407171e-05, -0.0588175393640995, -0.05457475781440735, -0.0513099804520607, 0.0405343733727932, -0.03554517775774002, 0.019465118646621704, -0.0067785014398396015, -0.045257434248924255, -0.04355150833725929, -0.0042717172764241695, -0.05478828400373459, 0.07589951902627945, -0.04345431178808212, 0.10246627032756805, 0.039568137377500534, -0.04234321787953377]}}, {'_index': 'temp', '_id': '95', '_score': 5.7736235, '_source': {'content': 'gets marked, we set X\\n1to zero.) An error signal is generated only at the\\nsequence end: the target is 0 .5+(X1+X2)/4.0 (the sum X1+X2scaled to\\nthe interval [0 ,1]). A sequence is processed correctly if the absolute error at\\nthe sequence end is below 0.04.\\n5.4.2 Architecture. We use a three-layer net with two input units, one\\noutput unit, and two cell blocks of size 2. The output layer receives connec-tions only from memory cells. Memory cells and gate units receive inputsfrom memory cells and gate units (the hidden layer is fully connected; lessconnectivity may work as well). The input layer has forward connectionsto all units in the hidden layer. All noninput units have bias weights. Thesearchitecture parameters make it easy to store at least two input signals (acell block size of 1 works well, too). All activation functions are logistic withoutput range [0 ,1], except for h, whose range is [−1,1], and g, whose range\\nis [−2,2].', 'dense_vector': [-0.06510955840349197, -0.03751687705516815, -0.06223076209425926, 0.0312948040664196, -0.0012292602332308888, -0.030911963433027267, 0.009787819348275661, -0.024934863671660423, 0.02017534337937832, -0.08388321846723557, -0.012028606608510017, -0.07205278426408768, 0.052059564739465714, -0.0724114254117012, -0.1093086525797844, 0.03996094688773155, 0.02835036814212799, 0.017974963411688805, -0.05733411759138107, -0.07810433954000473, 0.0600816048681736, 0.033614981919527054, -0.022951163351535797, 0.01382910180836916, -0.0337151363492012, -0.10146886110305786, -0.034147992730140686, -0.00952497124671936, 0.05162285268306732, -0.05668287351727486, 0.06481567770242691, -0.018496407195925713, -0.01692189835011959, 0.038511618971824646, -0.03948548436164856, 0.014877846464514732, -0.028560655191540718, -0.07626651227474213, 0.004431974142789841, 0.00327287963591516, 0.04422037675976753, 0.044237300753593445, -0.03219594806432724, 0.020131461322307587, -0.003479338251054287, -0.0038842582143843174, 0.006375954486429691, -0.04422176629304886, -0.022314494475722313, -0.05963746830821037, 0.0028274343349039555, 0.1400272399187088, 0.038588862866163254, 0.05704757198691368, -0.022221269086003304, -0.00938359834253788, -0.03390321508049965, 0.04512541741132736, -0.056095242500305176, 0.06053383648395538, -0.0028473609127104282, -0.02708086185157299, -0.01069661509245634, -0.08302821218967438, 0.019246799871325493, -0.011337729170918465, 0.022773874923586845, -0.010644294321537018, 0.024551426991820335, 0.06816130876541138, -0.00765471626073122, -0.0019239778630435467, -0.0914408415555954, 0.03941604867577553, 0.07879210263490677, 0.03208928927779198, 0.07866988331079483, 0.05214506760239601, 0.07517889887094498, -0.09290295839309692, 0.003290410153567791, -0.008013051003217697, 0.02276397868990898, 0.007925862446427345, 0.018773330375552177, 0.0019221841357648373, -0.05019694194197655, 0.08191944658756256, 0.0393911637365818, -0.030841153115034103, -0.02522880770266056, 0.05498936027288437, -0.06597597151994705, 0.020511459559202194, 0.07143126428127289, -0.04331706464290619, 0.07030648738145828, 0.009160920977592468, -0.03427079692482948, 0.05480039492249489, 0.001523568294942379, -0.0045939418487250805, 0.01794484630227089, 0.0010800579329952598, 0.04061676561832428, -0.007216031663119793, 0.06668386608362198, 0.02542218007147312, -0.0467861108481884, -0.093314990401268, -0.007224940229207277, 0.000281203247141093, 0.020366419106721878, 0.05458288639783859, 0.051543232053518295, -0.04390164464712143, 0.04887973144650459, 0.0077189295552670956, 0.011166130192577839, 0.01648068241775036, -0.08485051989555359, -0.00652949558570981, -0.008958098478615284, 0.07311305403709412, -0.02352409064769745, -0.06919461488723755, -0.027319196611642838, 7.215815418876798e-33, -0.04225454106926918, 0.03209085017442703, -0.020034166052937508, -0.05796799436211586, 0.00042474226211197674, 0.07500939071178436, 0.02151092141866684, -0.04300130903720856, -0.03507672995328903, 0.04853947088122368, -0.1402321755886078, -0.07005788385868073, -0.047448087483644485, 0.07172496616840363, 0.09620019048452377, -0.042002394795417786, 0.020037023350596428, -0.0008014741470105946, 0.04633224382996559, -0.03624691441655159, -0.006225165445357561, -0.037081655114889145, -0.021415770053863525, -0.07245749980211258, 0.03791402652859688, 0.0015586321242153645, -0.006850071717053652, -0.06084265187382698, -0.0429961159825325, 0.016950411722064018, -0.04800950735807419, 0.049212224781513214, 0.061698123812675476, -0.08208309859037399, 0.08209219574928284, 0.007426179014146328, 0.02493303455412388, 0.02833058312535286, 0.06711685657501221, -0.05503756180405617, -0.07020984590053558, 0.034185685217380524, 0.04401590675115585, -0.00466482387855649, -0.043084368109703064, -0.11716815084218979, 0.05677138268947601, 0.05928697809576988, -0.05454471334815025, 0.010066792368888855, 0.022997017949819565, -0.01749533787369728, -0.052066877484321594, -0.06307732313871384, -0.00022882834309712052, -0.05965966731309891, 0.03461992368102074, 0.0895608440041542, 0.05203045532107353, 0.14371049404144287, 0.000821631692815572, -0.004759461618959904, -0.061655767261981964, 0.07735734432935715, 0.02410387434065342, 0.0711611956357956, -0.08939722925424576, -0.04550271853804588, 0.004672558046877384, -0.02774866297841072, 0.006432889029383659, 0.03244417533278465, 0.03785198554396629, -0.05672265589237213, 0.02556271106004715, -0.013615007512271404, 0.010001699440181255, -0.07626506686210632, -0.08652666956186295, 0.017603524029254913, 0.02652021497488022, 0.0073744021356105804, -0.03969942405819893, 0.005475620273500681, -0.06301913410425186, 0.021310463547706604, 0.04175674542784691, 0.03853633999824524, -0.09516084939241409, 0.012121321633458138, -0.011269040405750275, -0.044341713190078735, 0.04034707322716713, 0.018508750945329666, -0.06001853570342064, -5.64641033970007e-33, -0.03523910418152809, 0.10873285681009293, 0.014409816823899746, 0.016335980966687202, -0.02434436045587063, 0.029478417709469795, 0.048120055347681046, -0.04962022602558136, 0.009637226350605488, 0.039214182645082474, 0.026822030544281006, 0.030719218775629997, 0.0032619426492601633, 0.06223886087536812, -0.02206958271563053, -0.040082816034555435, -0.07169276475906372, 0.0066750915721058846, 0.041923120617866516, 0.0035629631020128727, 0.09935006499290466, 0.06794034689664841, -0.1032913327217102, 0.03867730498313904, -0.053490832448005676, 0.03128080815076828, -0.025762125849723816, 0.11891783773899078, -0.004484014119952917, -0.05660610646009445, -0.04541090130805969, -0.03197436407208443, -0.004415738396346569, -0.013489977456629276, 0.09845057874917984, -0.0322093665599823, 0.08013541996479034, 0.014098945073783398, -0.031743403524160385, 0.01736539602279663, 0.029850972816348076, 0.07585327327251434, -0.03237397223711014, 0.02298072725534439, 0.04839014261960983, 0.026871120557188988, -0.03223569691181183, 0.017993485555052757, -0.09608594328165054, 0.013454336673021317, -0.031074630096554756, 0.0025727010797709227, -0.040399204939603806, 0.0632903128862381, -0.004044021014124155, 0.09278000146150589, -0.04898720607161522, 0.031921833753585815, 0.0867156982421875, -0.02461920864880085, -0.08004763722419739, -0.07065406441688538, 0.02000141330063343, -0.03505340963602066, 0.042369939386844635, 0.01891075074672699, -0.008006526157259941, 0.04155280068516731, 0.049315642565488815, 0.025099297985434532, 0.04884792119264603, 0.05551506206393242, 0.0802500769495964, -0.05311717465519905, -0.04786870256066322, -0.04502061381936073, -0.07235920429229736, -0.06909618526697159, 0.02785833366215229, 0.054295457899570465, -0.08717978745698929, 0.06315814703702927, 0.03828023001551628, 0.0421224981546402, 0.11358869820833206, 0.047843337059020996, 0.06344476342201233, 0.07448968291282654, -0.004281158093363047, -0.06517148017883301, 0.012402272783219814, 0.07042084634304047, -0.012050568126142025, -0.014596604742109776, -0.007298868615180254, -6.315843847914948e-08, -0.012135051190853119, -0.026620669290423393, -0.003758193226531148, -0.01006676722317934, 0.06635493785142899, -0.072708860039711, 0.045192018151283264, -0.05901666730642319, 0.013930021785199642, -0.05615610256791115, -0.003158953506499529, -0.00023654849792364985, -0.09902799874544144, -0.06738048046827316, 0.04695464298129082, 0.07268986105918884, 0.04130852594971657, -0.06227594614028931, 0.03442806005477905, -0.06962545216083527, 0.0056616137735545635, -0.018974073231220245, -0.06332680583000183, 0.04797743633389473, -0.01314882654696703, -0.10545036941766739, -0.03327252343297005, 0.08070120215415955, 0.016402579843997955, 0.013305610045790672, 0.06312909722328186, 0.00875012669712305, 0.12253589928150177, 0.02288982644677162, 0.031528182327747345, 0.11378522217273712, 0.03129349648952484, 0.08648530393838882, -0.029742026701569557, -0.02522936463356018, -0.07195935398340225, 0.047242674976587296, -0.02270229533314705, -0.040627192705869675, 0.011625978164374828, -0.028577931225299835, -0.04946529492735863, -0.05924944952130318, 0.016391964629292488, -0.005425482057034969, 0.03127547726035118, 0.04900914803147316, -0.02587961032986641, 0.060855187475681305, 0.04484381899237633, -0.11198332905769348, -0.06554905325174332, -0.12197894603013992, 0.0007393715204671025, 0.0987977683544159, -0.016004404053092003, 0.06921260803937912, -0.055658970028162, -0.03193936496973038]}}, {'_index': 'temp', '_id': '32', '_score': 5.766012, '_source': {'content': '1744 Sepp Hochreiter and J ¨ urgen Schmidhuber\\n \\ngh1.0\\nnet\\nwyinyoutnetc\\ngyin=g+ scsc yin\\nhyout\\nnetwc\\nin outwicc\\nj\\nj\\njj\\noutwj\\ninjj\\njj jjy\\njj\\njj\\ni\\ni i\\nFigure 1: Architecture of memory cell cj(the box) and its gate units inj,outj. The\\nself-recurrent connection (with weight 1.0) indicates feedback with a delay ofone time step. It builds the basis of the CEC. The gate units open and close accessto CEC. See text and appendix A.1 for details.\\nWe also have\\nnetcj(t)=∑\\nuwcjuyu(t−1).\\nThe summation indices umay stand for input units, gate units, memory\\ncells, or even conventional hidden units if there are any (see section 4.3). Allthese different types of units may convey useful information about the cur-rent state of the net. For instance, an input gate (output gate) may use inputsfrom other memory cells to decide whether to store (access) certain infor-mation in its memory cell. There even may be recurrent self-connectionslikew', 'dense_vector': [-0.08044341206550598, -0.058171145617961884, -0.051157381385564804, 0.07040637731552124, -0.013505629263818264, 0.0712684914469719, 0.054538920521736145, 0.033039480447769165, 0.09385871887207031, -0.021952981129288673, 0.00431123748421669, -0.07568683475255966, -0.009518341161310673, -0.04992593824863434, -0.09468679130077362, -0.059857163578271866, 0.020253315567970276, 0.011872688308358192, -0.07274434715509415, -0.08196662366390228, 0.05750511959195137, -0.009141907095909119, -0.031102119013667107, -0.03276558965444565, 0.010536608286201954, -0.021242722868919373, -0.04764872044324875, 0.047197531908750534, 0.07260845601558685, -0.023723570629954338, 0.008553694933652878, 0.029364902526140213, 0.023944057524204254, -0.0020814621821045876, -0.01737799122929573, 0.020640674978494644, -0.03441048786044121, -0.03376992046833038, -0.008985164575278759, -0.04648265242576599, -0.00946077425032854, 0.05067560821771622, 0.043996285647153854, 0.08365865051746368, 0.03117518685758114, 0.018557947129011154, -0.04532666131854057, -0.05127925053238869, -0.1255626380443573, -0.021170739084482193, 0.01639462448656559, 0.09093482792377472, -0.05416001006960869, 0.06852103769779205, 0.06445463001728058, 0.02093185856938362, -0.0346122607588768, 0.022749224677681923, -0.0502631813287735, 0.01301759947091341, -0.02277609333395958, -0.007900314405560493, -0.032818760722875595, -0.021702751517295837, 0.0967918410897255, -0.003566746599972248, 0.06889993697404861, 0.009298075921833515, 0.05056779086589813, -0.012295735999941826, -0.011876975186169147, -0.03672824427485466, -0.012057402171194553, 0.03489132970571518, -0.014496131800115108, 0.009587976150214672, 0.03389443829655647, 0.012284990400075912, 0.06917817890644073, -0.055034101009368896, 0.02766490913927555, 0.06811250001192093, 0.045674312859773636, -0.047883257269859314, 0.003572335233911872, 0.014405054040253162, -0.009805027395486832, 0.06170837953686714, 0.05529746040701866, -0.02489447593688965, 0.09632524847984314, -0.01009448803961277, -0.11319877952337265, -0.013455909676849842, 0.0648503378033638, -0.015737688168883324, 0.01141891535371542, 0.0639781728386879, -0.015670061111450195, 0.03908360004425049, 0.04179050400853157, 0.03908856213092804, 0.02775927633047104, 0.028476150706410408, -0.003215889912098646, 0.03498189523816109, 0.08100379258394241, 0.05380803346633911, -0.00014182085578795522, -0.0036406058352440596, -0.05189653858542442, 0.023793181404471397, -0.03581852465867996, 0.024576695635914803, -0.008453101851046085, -0.030691999942064285, 0.05779265984892845, -0.005510271061211824, 0.10622449964284897, 0.009860374964773655, -0.025163559243083, -0.03796549513936043, -0.06721654534339905, 0.008856936357915401, -0.052228767424821854, -0.042297907173633575, -0.06453965604305267, 9.566213434610085e-33, -0.09652869403362274, -0.028541529551148415, -0.021993162110447884, -0.04004089906811714, -0.020198890939354897, -0.027024298906326294, 0.00153007498010993, -0.04529828205704689, -0.024917881935834885, 0.05758913233876228, -0.10671700537204742, 0.029827067628502846, -0.030012818053364754, 0.006656349170953035, 0.04078482836484909, -0.08028682321310043, -0.02869340032339096, -0.0015998737653717399, 0.04437991976737976, -0.06472525745630264, 0.07644343376159668, 0.05186339095234871, 0.03513864055275917, -0.01881367526948452, 0.03412188962101936, -0.07422091066837311, -0.008506991900503635, 0.02132580243051052, -0.04816357418894768, 0.05970973148941994, 0.03297336399555206, -0.023469675332307816, -0.05623126029968262, -0.11254426836967468, 0.03200295567512512, -0.03931300342082977, 0.03571698069572449, -0.04399506375193596, 0.09895545989274979, -0.07264681160449982, -0.027278956025838852, 0.008367706090211868, -0.022054148837924004, -0.004808347672224045, -0.10433954745531082, -0.07398174703121185, -0.018856672570109367, 0.026452666148543358, -0.04210686311125755, 0.029451807960867882, -0.013347065076231956, -0.07251156121492386, -0.048264987766742706, -0.07502585649490356, -0.0014253028202801943, 0.04168206825852394, 0.0587908998131752, 0.07433022558689117, 0.034911755472421646, 0.19975920021533966, -0.09774146974086761, 0.016390612348914146, -0.07209401577711105, 0.013213038444519043, 0.021068118512630463, 0.03805498778820038, -0.13421586155891418, -0.028801556676626205, 0.02501636929810047, -0.020391998812556267, -0.050160348415374756, 0.033625997602939606, 0.013784734532237053, 0.0055550942197442055, 0.024740388616919518, -0.01899903081357479, 0.016555320471525192, -0.03627968579530716, -0.09945125132799149, -0.021140532568097115, -0.04255126789212227, 0.04094835743308067, -0.02164040319621563, -0.024469219148159027, -0.018549876287579536, -0.0473531149327755, 0.015249840915203094, -0.04166710004210472, -0.07804636657238007, 0.008142760954797268, 0.11574350297451019, 0.03570237755775452, 0.065019391477108, 0.04006194323301315, -0.07197332382202148, -9.311128221745772e-33, -0.04238811880350113, 0.03438534587621689, -0.05251547321677208, -0.04298553243279457, -0.01207824144512415, 0.015497558750212193, -0.023314515128731728, -0.09766124188899994, 0.035437196493148804, -0.032245516777038574, 0.025934629142284393, 0.0335458479821682, -0.008554169908165932, 0.026264173910021782, 0.03169940784573555, -0.06299366801977158, -0.01262092124670744, -0.0047293961979448795, -0.0020135592203587294, 0.03009686805307865, 0.02125929668545723, 0.06381027400493622, -0.0648181289434433, -0.05374618619680405, -0.05014557018876076, 0.031227054074406624, -0.034026850014925, 0.12395370751619339, -0.004553160164505243, 0.05960339680314064, -0.02066953293979168, -0.11026529222726822, -0.018537791445851326, 0.1319374293088913, 0.009270704351365566, -0.05389896035194397, 0.07894165813922882, -0.05987611040472984, 0.0010225159348919988, 0.04067632555961609, 0.08445829153060913, 0.06846168637275696, -0.028858480975031853, -0.005118550267070532, 0.03885212540626526, 0.06809616088867188, -0.05697588622570038, 0.027479443699121475, -0.054400041699409485, -0.04988967627286911, 0.052214328199625015, -0.0027787042781710625, -0.023340225219726562, 0.03180984780192375, -0.03624802082777023, 0.10714490711688995, 0.04088902473449707, -0.04293219372630119, 0.050157513469457626, -0.058468498289585114, -0.02152148261666298, -0.11081546545028687, -0.011384201236069202, 0.027527468279004097, 0.05883961170911789, -0.02407962642610073, 0.015255870297551155, -0.01915285550057888, 0.0713648870587349, -0.058409567922353745, 0.011045007035136223, 0.016132645308971405, -0.011717377230525017, -0.07297040522098541, -0.04512811079621315, -0.056394487619400024, -0.005882484372705221, -0.036201078444719315, -0.016812097281217575, -0.02539949305355549, -0.06509128212928772, 0.08789398521184921, 0.019689807668328285, -0.06637953966856003, 0.06171410158276558, 0.009232447482645512, 0.04780008643865585, 0.0686829686164856, 0.01874256506562233, -0.041248779743909836, -0.005451281554996967, 0.04644567519426346, -0.015965061262249947, 0.07758630067110062, 0.020061789080500603, -6.219040216137728e-08, 0.01607264205813408, -0.06075618043541908, -0.037527572363615036, 0.020974857732653618, 0.08453978598117828, -0.10564081370830536, 0.013192681595683098, -0.04625052958726883, 0.028974777087569237, 0.0385257788002491, 0.07245306670665741, 0.02606740966439247, -0.07294877618551254, -0.05537714436650276, 0.03307671844959259, 0.06298568844795227, -0.07844249159097672, -0.026290306821465492, 0.015396971255540848, -0.04739779233932495, 0.05439441651105881, -0.019261334091424942, 0.0024609367828816175, 0.10078936070203781, -0.0469699390232563, -0.010415598750114441, -0.015710188075900078, 0.07336132973432541, -0.00912508461624384, 0.05003056675195694, 0.016369018703699112, 0.052326422184705734, 0.03863879293203354, 0.06680826097726822, 0.00626958254724741, 0.0815156102180481, 0.012471442110836506, 0.03624975308775902, 0.011962509714066982, 0.13312697410583496, -0.022159874439239502, -0.03673841059207916, -0.02342083491384983, 0.03778360038995743, 0.1009448915719986, 0.061110969632864, -0.07583674043416977, 0.02945661172270775, -0.0189359188079834, -0.028779450803995132, 0.004343284294009209, 0.01571340672671795, -0.10485360026359558, 0.04245069622993469, 0.0021450927015393972, -0.04236937314271927, -0.07125941663980484, -0.035179879516363144, 0.00251927156932652, 0.05276889726519585, -0.016815008595585823, 0.0807233452796936, -0.030803291127085686, -0.09036754071712494]}}, {'_index': 'temp', '_id': '107', '_score': 5.6315675, '_source': {'content': 'There are as many output units as there are classes. Each class is locally\\nrepresented by a binary target vector with one nonzero component. Withboth tasks, error signals occur only at the end of a sequence. The sequenceis classiﬁed correctly if the ﬁnal absolute error of all output units is below0.3.\\nArchitecture. We use a three-layer net with eight input units, two (three)\\ncell blocks of size 2, and four (eight) output units for task 6a (6b). Again allnoninput units have bias weights, and the output layer receives connectionsfrom memory cells, only. Memory cells and gate units receive inputs frominput units, memory cells, and gate units (the hidden layer is fully con-nected; less connectivity may work as well). The architecture parametersfor task 6a (6b) make it easy to store at least two (three) input signals. Allactivation functions are logistic with output range [0 ,1], except for h, whose\\nrange is [−1,1], and g, whose range is [−2,2].', 'dense_vector': [-0.047196291387081146, -0.05763041600584984, -0.11474001407623291, -0.010191272012889385, -0.04025701433420181, 0.03093576990067959, -0.002427405212074518, -0.04250238090753555, -0.009685034863650799, -0.05040443316102028, -0.059218309819698334, -0.05280131474137306, 0.04544413462281227, -0.08775675296783447, -0.07571033388376236, 0.03170909732580185, 0.008660952560603619, 0.03203272446990013, -0.0678635835647583, -0.04617508128285408, 0.08872447162866592, 0.023694604635238647, -0.006382140796631575, 0.03395993635058403, -0.06619791686534882, -0.025894958525896072, 0.00380324968136847, -0.04108051583170891, 0.02023342065513134, -0.09050213545560837, 0.04533212631940842, -0.008688705042004585, 0.026058003306388855, 0.07344867289066315, -0.0002869520103558898, 0.05157702788710594, -0.053351983428001404, -0.06708314269781113, -0.019359629601240158, -0.027127116918563843, 0.012191991321742535, 0.02002785913646221, -0.028343183919787407, 0.006824879441410303, -0.0033739872742444277, 0.0026513512711972, -0.040301449596881866, -0.06161368638277054, -0.00842781737446785, -0.04279126226902008, 0.002550295554101467, 0.07231704890727997, 0.0378388836979866, 0.10569743067026138, -0.0005732544232159853, 0.0012664443347603083, -0.021740878000855446, 0.03579907864332199, -0.06926931440830231, 0.020410120487213135, -0.045973822474479675, -0.034755680710077286, 0.010908055119216442, -0.059954725205898285, 0.057980917394161224, 0.03839236870408058, 0.007558891549706459, -0.028474923223257065, 0.0037013760302215815, 0.016038503497838974, -0.06983347982168198, -0.029825935140252113, -0.08787839859724045, 0.08393648266792297, 0.036739248782396317, 0.060461416840553284, 0.04121575877070427, 0.010347504168748856, 0.024374278262257576, -0.10216835886240005, 0.016412314027547836, 0.0067013283260166645, -0.010948002338409424, 0.01950351893901825, 0.07442451268434525, 0.00977714266628027, -0.07842020690441132, 0.054548900574445724, 0.03527785837650299, -0.036569491028785706, -0.03886037319898605, 0.005476732738316059, 0.04300427436828613, 0.03171195462346077, 0.08297136425971985, -0.03231246396899223, 0.02937743440270424, -0.036414649337530136, -0.006743655540049076, 0.04703240841627121, 0.009066535159945488, -0.008905907161533833, 0.02810245379805565, 0.030593762174248695, 0.032579272985458374, -0.004261159338057041, 0.027408866211771965, 0.03189852088689804, -0.04445783793926239, -0.13079796731472015, -0.031225867569446564, 0.006883610971271992, -0.019179070368409157, 0.05917393043637276, 0.10579556226730347, -0.04438437148928642, 0.05283040180802345, -0.001792280119843781, 0.021449610590934753, 0.06535414606332779, -0.07671543210744858, -0.00661839684471488, -0.005330355372279882, 0.055656466633081436, 0.0030348473228514194, -0.069664366543293, -0.020010316744446754, 7.393160047533449e-33, -0.05885531008243561, 0.0140695096924901, -0.03997745364904404, -0.04987184703350067, 0.04641232267022133, 0.05104367807507515, 0.03869577497243881, -0.014350907877087593, -0.017918987199664116, 0.02200212888419628, -0.1355021446943283, -0.06686999648809433, 0.009135960601270199, 0.07617449760437012, 0.12366767227649689, -0.03978388383984566, 0.027452174574136734, 0.07851265370845795, 0.04232002794742584, -0.04918508604168892, -0.006524626165628433, -0.03507193177938461, -0.0012175699230283499, -0.07914068549871445, 0.04978204146027565, 0.0014115357771515846, 0.025266846641898155, -0.030743790790438652, -0.05419072508811951, 0.004855588544160128, -0.025638148188591003, 0.0343480221927166, 0.016296209767460823, -0.04165045917034149, 0.030315015465021133, -0.016454486176371574, 0.02530105784535408, -0.004157538991421461, 0.049014072865247726, -0.04591730982065201, -0.04494274780154228, 0.03003856912255287, 0.06283524632453918, -0.01204860769212246, -0.033265043050050735, -0.10119068622589111, 0.03620265796780586, 0.022155188024044037, -0.04263957962393761, -0.009814893826842308, 0.055930931121110916, -0.0008537325775250793, 0.018886685371398926, -0.07762651890516281, 0.03104255720973015, -0.05531778931617737, -0.004323759116232395, 0.12566089630126953, 0.09440196305513382, 0.1749301701784134, -0.011537912301719189, 0.0009534714627079666, -0.07260330021381378, 0.09667970985174179, 0.04503466933965683, 0.058752428740262985, -0.049635425209999084, -0.035740870982408524, -0.0035984858404845, -0.02133454568684101, -0.0009865977335721254, -0.015584836713969707, 0.008810367435216904, -0.001022153184749186, 0.005334008950740099, 0.016373571008443832, 0.008935736492276192, -0.11280890554189682, -0.12426050007343292, 0.036065250635147095, 0.02397303842008114, -0.017421605065464973, -0.07074827700853348, -0.0012566440273076296, -0.044767074286937714, 0.01200333796441555, 0.021019309759140015, -0.035338182002305984, -0.10309652984142303, 0.04911727085709572, 0.011195305734872818, -0.021883103996515274, 0.034164343029260635, 0.04184708744287491, -0.06229259446263313, -6.126216644321003e-33, 0.005950975697487593, 0.09935126453638077, -0.02061864174902439, -0.0008701350307092071, 0.015050563029944897, 0.02118000015616417, 0.05109315738081932, -0.07013063877820969, -0.08108895272016525, 0.02868770994246006, -0.03572990745306015, 0.01357936579734087, 0.028951171785593033, 0.0424695760011673, -0.04185014218091965, -0.01466209813952446, -0.04670311138033867, -0.005734958220273256, 0.07019190490245819, -0.012700730003416538, 0.07101810723543167, 0.0861453041434288, -0.09203770756721497, 0.013589106500148773, -0.02550719678401947, 0.04182742163538933, -0.045491259545087814, 0.09833406656980515, 0.01175103709101677, -0.03565690666437149, -0.05000052973628044, -0.02867436781525612, -0.012600184418261051, 0.010256499983370304, 0.11149601638317108, -0.03883635625243187, 0.060303281992673874, 0.01857476308941841, -0.004153149668127298, 0.04294170066714287, 0.058566346764564514, 0.03344295546412468, -0.01013608742505312, 0.01302915345877409, 0.07329516112804413, 0.0014382831286638975, -0.040527526289224625, 0.03869745880365372, -0.12857232987880707, 0.029438044875860214, -0.04605141654610634, 0.0022267228923738003, -0.03190765529870987, 0.038578882813453674, 0.037054989486932755, 0.05864616855978966, -0.004428220447152853, -0.015740592032670975, 0.07137306779623032, -0.006239903625100851, -0.016104944050312042, -0.06359367072582245, 0.04274534806609154, -0.02298242226243019, 0.05445185676217079, 0.057319510728120804, -0.0006824611336924136, 0.047105368226766586, 0.03132452070713043, 0.049813758581876755, 0.06172724440693855, 0.05250990018248558, 0.04459685459733009, -0.016593320295214653, -0.05489259958267212, -0.053590621799230576, -0.06393968313932419, -0.062046803534030914, 0.030062133446335793, 0.07314608991146088, -0.13863298296928406, 0.03998565301299095, 0.008324765600264072, 0.06992783397436142, 0.05252227187156677, 0.03716185688972473, 0.12724551558494568, 0.03451620042324066, -0.00723730493336916, -0.05138888582587242, -0.014595306478440762, 0.0568227656185627, -0.001305162557400763, 0.00837000273168087, -0.026536786928772926, -6.602121516152692e-08, 0.012965190224349499, -0.06667608767747879, 0.013555771671235561, -0.015725040808320045, 0.06267578899860382, -0.08999801427125931, 0.006240979302674532, -0.054352518171072006, 0.024297233670949936, -0.053966835141181946, 0.040132250636816025, -0.03145240247249603, -0.054105211049318314, -0.014817283488810062, 0.07363138347864151, 0.03537098318338394, 0.016840817406773567, -0.025851856917142868, 0.00099981389939785, -0.07488241791725159, 0.09060821682214737, -0.0018160135950893164, -0.032986097037792206, 0.08218908309936523, 0.04438508301973343, -0.10411635041236877, 0.002789527177810669, 0.0943957045674324, 0.028373360633850098, 0.03855276107788086, 0.022083725780248642, 0.04244310408830643, 0.0717468336224556, 0.004771553911268711, 0.03937254846096039, 0.08549439162015915, 0.02965654246509075, 0.02197328582406044, -0.028294507414102554, 0.002770205494016409, -0.03663555160164833, -0.013837252743542194, -0.05704870447516441, -0.013881172984838486, 0.022056786343455315, -0.016468990594148636, -0.09924938529729843, -0.06993050128221512, 0.003727793460711837, -0.039446596056222916, -0.0023427417036145926, 0.051860511302948, -0.03826706111431122, 0.09235868602991104, 0.030054571107029915, -0.07092060893774033, -0.05418683588504791, -0.18487191200256348, -0.002038713311776519, 0.07582787424325943, 0.017401326447725296, 0.08416364341974258, -0.03719089925289154, -0.05806693062186241]}}]}})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_vector = dense_embedding_model.encode(user_query).tolist()\n",
        "\n",
        "dense_query = {\n",
        "    \"query\": {\n",
        "        \"script_score\": {\n",
        "            \"query\": {\n",
        "                \"match_all\": {}\n",
        "            },\n",
        "            \"script\": {\n",
        "                \"source\": \"cosineSimilarity(params.query_vector, 'dense_vector') + 1.0\",\n",
        "                \"params\": {\n",
        "                    \"query_vector\": query_vector\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    },\n",
        "    \"size\": 10\n",
        "}\n",
        "\n",
        "dense_results = es_client.search(index=index_name, body=dense_query)\n"
      ],
      "metadata": {
        "id": "qrfEmEAA98r-"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dense_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJicS-ZW_oA1",
        "outputId": "ff68f35e-30ff-4fc0-a0d4-e79029d86200"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectApiResponse({'took': 63, 'timed_out': False, '_shards': {'total': 1, 'successful': 1, 'skipped': 0, 'failed': 0}, 'hits': {'total': {'value': 161, 'relation': 'eq'}, 'max_score': 1.560598, 'hits': [{'_index': 'temp', '_id': '127', '_score': 1.560598, '_source': {'content': 'To ﬁnd out about LSTM’s practical limitations we intend to apply it to\\nreal-world data. Application areas will include time-series prediction, musiccomposition, and speech processing. It will also be interesting to augmentsequence chunkers (Schmidhuber, 1992b, 1993) by LSTM to combine theadvantages of both.\\nAppendix\\nA.1 Algorithm Details. In what follows, the index kranges over output\\nunits, iranges over hidden units, cjstands for the jth memory cell block, cv\\nj\\ndenotes the vth unit of memory cell block cj,u,l,mstand for arbitrary units,\\nand tranges over all time steps of a given input sequence.\\nThe gate unit logistic sigmoid (with range [0 ,1]) used in the experiments\\nis\\nf(x)=1\\n1+exp(−x). (A.1)', 'dense_vector': [-0.04604389891028404, -0.0751531645655632, 0.0009022757294587791, -0.03636178746819496, 0.031175322830677032, 0.09012547135353088, -0.03815111145377159, -0.009829736314713955, -0.02542305365204811, -0.07234065234661102, 3.2243283385469113e-06, -0.00909651443362236, 0.012789322063326836, -0.06569314748048782, -0.04890067130327225, -0.028000032529234886, -0.025114785879850388, 0.07368838042020798, -0.008465485647320747, -0.07226812839508057, 0.07027315348386765, 0.0340452715754509, -0.0219801627099514, 0.011837481521070004, 0.02536717988550663, -0.023424241691827774, -0.031092794612050056, 0.02616969309747219, 0.07720243185758591, -0.014303852804005146, 0.053434669971466064, 0.043102651834487915, 0.07725522667169571, 0.03197205439209938, -0.12772074341773987, -0.015666404739022255, -0.09195958077907562, -0.07288950681686401, 0.0034932687412947416, -5.8516121498541906e-05, 0.02687794342637062, 0.043864767998456955, 0.022713568061590195, -0.03678939491510391, 0.018379410728812218, 0.026621105149388313, -0.014791102148592472, -0.03162059560418129, -0.1238722950220108, 0.07429763674736023, -0.060517244040966034, 0.11912595480680466, -0.0014821812510490417, 0.1388794183731079, -0.11577996611595154, -0.030913151800632477, 0.0010131598683074117, 0.08431356400251389, -0.00013871915871277452, 0.004997854586690664, -0.04998726770281792, -0.03802517056465149, -0.014470531605184078, -0.07106614857912064, 0.009361224249005318, -0.09215381741523743, 0.052019018679857254, 0.01697136089205742, 0.041950639337301254, 0.06449021399021149, 0.004800825379788876, 0.016892466694116592, -0.03503463789820671, 0.07244788855314255, -0.018879558891057968, 0.04973572492599487, 0.08381657302379608, -0.0011034529889002442, 0.018842199817299843, -0.01007172279059887, 0.002748709637671709, -0.004473679233342409, 0.04071839898824692, -0.10213109850883484, -0.0004819985188078135, -0.047888629138469696, -0.028706833720207214, 0.09712330251932144, 0.005810289178043604, 0.013160460628569126, -0.0016097916522994637, -0.08957801759243011, -0.03370995447039604, -0.03482590243220329, 0.08401418477296829, 0.01811332255601883, 0.015380503609776497, 0.03364808112382889, 0.029619142413139343, -0.01904284581542015, 0.008021916262805462, 0.08777323365211487, 0.0070177530869841576, 0.026729192584753036, 0.04745647683739662, -0.04477386921644211, 0.05350160226225853, 0.0056901415809988976, 0.013511186465620995, -0.08248786628246307, 0.06613655388355255, 0.02663842774927616, 0.03989775851368904, -0.012157764285802841, 0.132695272564888, -0.08884989470243454, 0.038368966430425644, -0.08798393607139587, 0.029817719012498856, 0.07990870624780655, -0.026878755539655685, -0.028630109503865242, -0.03754078596830368, 0.07199417799711227, -0.035677965730428696, -0.025541456416249275, -0.022334659472107887, 5.633838060276041e-33, -0.0036530743818730116, 0.014883887022733688, -0.014105036854743958, -0.07552825659513474, 0.02753148041665554, -0.018889784812927246, -0.0495416559278965, 0.0326865017414093, -0.006592400372028351, 0.05309944227337837, -0.08154737949371338, 0.012059304863214493, 0.02604186348617077, 0.06745154410600662, -0.007016101386398077, -0.044349342584609985, -0.005874415393918753, 0.009542464278638363, 0.005851767957210541, -0.03324607387185097, -0.01708095893263817, -0.05389995127916336, 0.05435950309038162, -0.03561733663082123, 0.010151948779821396, 0.005571546033024788, 0.022196142002940178, -0.028750935569405556, -0.024011768400669098, 0.009736144915223122, -0.072147898375988, 0.04158865287899971, -0.06667657196521759, -0.04622203856706619, 0.11093669384717941, 0.029337769374251366, -0.037312787026166916, -0.05552374944090843, 0.05957212671637535, -0.12415606528520584, -0.07109905034303665, 0.01525688823312521, 0.08359065651893616, -0.05456395447254181, -0.13925054669380188, -0.06942599266767502, -0.018066124990582466, 0.0015978423180058599, 0.047131214290857315, -0.0352356918156147, 0.0315367691218853, -0.005983181297779083, -0.10072818398475647, -0.07063428312540054, 0.06598017364740372, 0.03065449744462967, 0.07596267014741898, 0.0695548728108406, 0.019858429208397865, 0.14172354340553284, -0.018868418410420418, 0.06746567785739899, 0.06430242210626602, 0.08780154585838318, 0.018799249082803726, 0.05519790202379227, -0.07278671115636826, 0.006936386227607727, 0.02903163991868496, -0.047585099935531616, 0.010236398316919804, -0.08612112700939178, 0.06980094313621521, -0.00019899135804735124, 0.030706239864230156, -0.002940927166491747, 0.11992685496807098, -0.0948321744799614, -0.08341790735721588, 0.0034540602937340736, -0.01677824929356575, 0.023771893233060837, -0.02608269825577736, -0.07996439188718796, -0.016504254192113876, -0.054716020822525024, 0.09680619090795517, -0.06494123488664627, -0.05680150166153908, -0.05182921886444092, -0.06945376843214035, 0.001654843334108591, 0.0643121674656868, 0.03775282949209213, -0.11076592653989792, -7.262348097438068e-33, -0.050206899642944336, 0.04033457115292549, 0.01558875385671854, -0.004415918607264757, -0.01126317959278822, -0.031568627804517746, -0.036823779344558716, 0.029593899846076965, -0.017528817057609558, 0.05816638842225075, 0.007557498291134834, 0.02496788464486599, 0.03642145171761513, -0.009565661661326885, -0.020156502723693848, -0.027186689898371696, 0.06647364795207977, -0.029763922095298767, -0.0036390970926731825, -0.027364613488316536, 0.05111991986632347, 0.023341268301010132, -0.03524381294846535, 0.022940609604120255, -0.11329632252454758, -0.0010695314267650247, -0.04282551258802414, 0.06503386795520782, -0.037608593702316284, -0.07268994301557541, -0.05524427816271782, -0.06452139467000961, -0.0566922128200531, -0.05669955164194107, -0.0016810415545478463, 0.014991885051131248, 0.008489510975778103, -0.0018754838965833187, 0.02931971848011017, 6.756108632544056e-05, 0.06912534683942795, 0.06790981441736221, 0.03265107423067093, -0.02237408421933651, 0.028922419995069504, -0.022925380617380142, -0.08228202909231186, 0.057274363934993744, -0.0028096160385757685, -0.01393471471965313, 0.012711352668702602, 0.05782323703169823, 0.010388900525867939, 0.010049708187580109, -0.008840561844408512, 0.023069312795996666, -0.07378922402858734, 0.00906812772154808, 0.000622795254457742, -0.03135313466191292, -0.02909708395600319, -0.011738507077097893, -0.019641615450382233, -0.08916256576776505, 0.033225782215595245, 0.011364865116775036, 0.030902916565537453, -0.0018968547228723764, -0.007038897834718227, 0.007462737150490284, 0.10321737825870514, 0.04432147741317749, 0.08002791553735733, 0.030115077272057533, -0.11048457026481628, -0.06426748633384705, -0.0773855522274971, -0.11902082711458206, -0.06902280449867249, -0.04637030512094498, 0.0173967108130455, 0.023707056418061256, 0.01954795978963375, 0.03660575672984123, 0.037114374339580536, 0.060002222657203674, 0.012127546593546867, 0.0533837266266346, 0.03839249536395073, -0.07436477392911911, -0.011586946435272694, 0.03188396617770195, -0.025469614192843437, -0.030412347987294197, 0.004674522206187248, -5.996594865109728e-08, -0.004298028070479631, -0.02033805660903454, -0.015253483317792416, 0.009547533467411995, 0.05962206423282623, -0.10979434847831726, -0.01509543601423502, -0.005675760563462973, 0.06336317211389542, -0.04406984895467758, 0.14760372042655945, -0.015395654365420341, -0.11574480682611465, 0.008834580890834332, 0.02867339737713337, -0.00315800285898149, -0.013647186569869518, 0.03330158442258835, 6.172766734380275e-05, 0.018349289894104004, 0.00271604978479445, 0.06681706011295319, 0.014880722388625145, 0.0240681990981102, 0.04292601719498634, -0.01311170682311058, 0.004727643448859453, 0.0420481376349926, 0.017716247588396072, 0.04011280834674835, 0.03371196985244751, 0.06354951113462448, -0.0015092859975993633, 0.030450094491243362, -0.010777190327644348, 0.009557053446769714, 0.009854814037680626, 0.05425242707133293, -0.05501282215118408, 0.005227051209658384, 0.011923989281058311, 0.014293709769845009, -0.02900533936917782, 0.05878652259707451, -0.07267390191555023, -0.015066780149936676, -0.04057685285806656, -0.09472806751728058, 0.005380674265325069, -0.03649260476231575, 0.009323595091700554, 0.060191333293914795, 0.01602490246295929, 0.039012931287288666, 0.11613185703754425, -0.028917301446199417, -0.06959456205368042, -0.021802052855491638, 0.04169691354036331, 0.009230528958141804, -0.029084790498018265, 0.0270271934568882, -0.024522125720977783, 0.01042398065328598]}}, {'_index': 'temp', '_id': '33', '_score': 1.5435283, '_source': {'content': 'cells, or even conventional hidden units if there are any (see section 4.3). Allthese different types of units may convey useful information about the cur-rent state of the net. For instance, an input gate (output gate) may use inputsfrom other memory cells to decide whether to store (access) certain infor-mation in its memory cell. There even may be recurrent self-connectionslikew\\ncjcj. It is up to the user to deﬁne the network topology. See Figure 2 for\\nan example.\\nAt time t,cj’s output ycj(t)is computed as\\nycj(t)=youtj(t)h(scj(t)),\\nwhere the internal state scj(t)is\\nscj(0)=0,scj(t)=scj(t−1)+yinj(t)g(\\nnetcj(t))\\nfort>0.\\nThe differentiable function gsquashes netcj; the differentiable function h\\nscales memory cell outputs computed from the internal state scj.\\n4.2 Why Gate Units? To avoid input weight conﬂicts, injcontrols the\\nerror ﬂow to memory cell cj’s input connections wcji. To circumvent cj’s\\noutput weight conﬂicts, outjcontrols the error ﬂow from unit j’s output', 'dense_vector': [-0.09888125211000443, -0.05292243883013725, -0.10601839423179626, 0.06752384454011917, -0.052399467676877975, 0.057058185338974, -0.014245404861867428, 0.02681618742644787, 0.1400570571422577, -0.0022876220755279064, 0.02078537829220295, 0.008019380271434784, 0.02541062794625759, -0.045121680945158005, -0.03685100004076958, -0.034149378538131714, 0.028998294845223427, 0.05513371154665947, -0.08711247891187668, -0.06498770415782928, 0.04462524875998497, -0.02179587446153164, -0.0233007799834013, -0.02794368751347065, -0.033940475434064865, -0.006312248762696981, -0.037124671041965485, 0.02553333155810833, 0.002684341510757804, -0.055663276463747025, -0.02090032398700714, 0.04266446828842163, -0.009700734168291092, 0.01888306625187397, -0.06118719279766083, -0.008349833078682423, -0.015558416955173016, 0.007503809407353401, 0.006341612432152033, -0.04097290709614754, 0.0020847287960350513, 0.029258105903863907, 0.018879864364862442, 0.06012822687625885, 0.020454933866858482, 0.024467429146170616, -0.004047033376991749, -0.08114549517631531, -0.08244281262159348, -0.02992456778883934, 0.041763391345739365, 0.08437105268239975, -0.01802494376897812, 0.06577602028846741, 0.09365255385637283, 0.01963570900261402, 0.034047458320856094, 0.05906226858496666, -0.058749325573444366, 0.06602267920970917, -0.041678089648485184, -0.016690025106072426, 0.0008012382895685732, 0.0017046048305928707, 0.03204287961125374, -0.048013851046562195, 0.09147752076387405, 0.03210069611668587, 0.05075473338365555, -0.02643483132123947, 0.019877968356013298, -0.0021056830883026123, -0.00732157239690423, -0.02038411796092987, 0.024746818467974663, -0.008604723960161209, 0.044015176594257355, 0.03581435978412628, 0.06182226166129112, -0.02608572505414486, 0.05065751075744629, 0.10747909545898438, 0.05229327455163002, 0.007729654666036367, -0.007172871381044388, 0.0194648876786232, -0.018068119883537292, 0.05852533504366875, 0.08486674726009369, -0.008725377731025219, 0.016203468665480614, -0.003416437190026045, -0.12494901567697525, -0.046413831412792206, 0.03309408202767372, -0.06380826234817505, 0.025002921000123024, 0.0066702100448310375, -0.008532523177564144, -0.014343108050525188, 0.06066112965345383, 0.015282402746379375, 0.11594066768884659, 0.05269321799278259, 0.056242283433675766, 0.05889103561639786, 0.07273485511541367, 0.08916477113962173, 0.08361146599054337, -0.005069816019386053, -0.028761673718690872, -0.013891609385609627, -0.06200667843222618, 0.03875206410884857, 0.029530437663197517, -0.14543627202510834, 0.07185091078281403, -0.019048044458031654, 0.08971758931875229, 0.020549166947603226, -0.07191232591867447, -0.007354184985160828, -0.05365823581814766, -0.031670667231082916, -0.0195171982049942, 0.026844147592782974, -0.04059498757123947, 6.0752505153472106e-33, -0.10057131201028824, -0.024905528873205185, -0.019193576648831367, -0.062048569321632385, -0.002716868184506893, -0.006820372771471739, -0.005675687920302153, -0.007481153588742018, 0.06635742634534836, 0.01004373375326395, -0.13445648550987244, 0.04954042658209801, -0.021837564185261726, 0.05268934741616249, 0.0569952167570591, -0.0030588603112846613, -0.0737077072262764, -0.024062233045697212, 0.03168606013059616, -0.08467411994934082, 0.02754942700266838, 0.05101916939020157, 0.013894865289330482, -0.0354422926902771, 0.027831869199872017, -0.08819364011287689, 0.021615123376250267, 0.01625118963420391, -0.05434393510222435, 0.005780404899269342, 0.013116655871272087, 0.01911655068397522, -0.02479446865618229, -0.06794749945402145, 0.05960647761821747, 0.00615547364577651, 0.01523828785866499, -0.04581836238503456, 0.05905349180102348, -0.1281270682811737, 0.008402583189308643, 0.01027996651828289, 0.007647903170436621, 0.011318361386656761, -0.10219624638557434, -0.05542219430208206, -0.0009215649915859103, 0.0032919838558882475, -0.11629325151443481, -0.04251585155725479, -0.0031259963288903236, 0.008748293854296207, -0.045281294733285904, -0.10595207661390305, 0.011118391528725624, 0.006401801016181707, 0.05847629904747009, 0.05152776464819908, 0.052628565579652786, 0.16708986461162567, -0.08272256702184677, 0.017414458096027374, -0.015233194455504417, 0.0673312246799469, 0.0029979459941387177, 0.0632907897233963, -0.10641803592443466, -0.010216818191111088, -0.0009885742329061031, -0.05702659487724304, -0.005144232418388128, 0.03517846390604973, -0.01740303263068199, -0.07546219229698181, 0.03081583045423031, -0.03223700821399689, 0.03831053152680397, -0.08218488097190857, -0.054288845509290695, 0.014975020661950111, -0.01891225576400757, -0.03932946175336838, -0.05751056969165802, 0.032511964440345764, -0.0548328272998333, -0.040229905396699905, 0.04673311114311218, -0.03907833620905876, -0.041453391313552856, 0.06077854707837105, 0.08581516891717911, -0.019755661487579346, 0.023539302870631218, 0.03433912619948387, -0.07259158790111542, -7.375036863379647e-33, -0.04922938346862793, 0.07124631106853485, -0.08206700533628464, -0.03684588521718979, -0.011025752872228622, 0.030341025441884995, -0.006562006194144487, -0.07366295903921127, 0.00697693508118391, -0.045909419655799866, 0.021330811083316803, 0.05876275151968002, 0.0102617796510458, 0.01015586219727993, -0.0048388466238975525, -0.07066910713911057, -0.02421335130929947, -0.04344454035162926, -0.08215212821960449, -0.0036442347336560488, 0.03992261737585068, 0.06544510275125504, -0.052481699734926224, 0.009223135188221931, -0.019173063337802887, 0.000278180610621348, -0.0844271332025528, 0.09691458940505981, -0.031227149069309235, 0.060100916773080826, 0.003990885801613331, -0.08442352712154388, -0.02721385285258293, 0.050063684582710266, 0.02068421058356762, -0.034843213856220245, 0.06120316684246063, -0.06052178889513016, 0.0032218804117292166, 0.028933752328157425, 0.07606256008148193, 0.04635525494813919, -0.029077066108584404, 0.013894747942686081, 0.0003816909156739712, 0.10004302859306335, -0.04279402270913124, 0.003354959422722459, -0.042247872799634933, -0.009667530655860901, -0.011448276229202747, 0.02309533767402172, -0.06225443258881569, 0.011999087408185005, -0.00395947927609086, 0.057918764650821686, 0.017259763553738594, -0.02475072257220745, 0.05791529268026352, -0.005805802997201681, -0.0524742417037487, -0.11181098222732544, -0.028242817148566246, 0.030411306768655777, -0.02390287257730961, 0.007584253326058388, 0.03760939836502075, 0.025123214349150658, 0.07535688579082489, -0.05040097236633301, -0.007238827645778656, 0.033198852092027664, 0.01985241286456585, -0.04777539521455765, -0.034153278917074203, -0.0526357926428318, 0.01936596818268299, -0.09335460513830185, -0.06526605784893036, -0.00579436868429184, -0.02108827792108059, 0.04273423179984093, 0.00840881746262312, -0.04822542890906334, 0.05739307031035423, -0.010035849176347256, 0.02489427663385868, 0.010906774550676346, 0.027334030717611313, -0.014951779507100582, 0.04274492338299751, -0.006984683219343424, -0.09480690956115723, 0.06109515205025673, -0.03665151447057724, -5.6617903254618795e-08, -0.030611228197813034, -0.02658567950129509, 0.07516905665397644, -0.003042202442884445, 0.05208893120288849, -0.08858945220708847, 0.0392642542719841, -0.018201714381575584, 0.0537988655269146, 0.09880592674016953, 0.07752680033445358, 0.014658136293292046, -0.10585759580135345, -0.07394909858703613, 0.0355197936296463, 0.08964955061674118, -0.03378092125058174, -0.05927085503935814, 0.04141068086028099, -0.04200097173452377, -0.017643487080931664, -0.04689984768629074, -0.043188948184251785, 0.11279904097318649, 0.02166079171001911, -0.051754120737314224, -0.05939863249659538, 0.03580630198121071, 0.009753577411174774, 0.03611938655376434, 0.015010330826044083, 0.08269751816987991, 0.05790780484676361, 0.10926780849695206, 0.024400660768151283, 0.05116043612360954, 0.041455112397670746, 0.09051550924777985, -0.005255675874650478, 0.09172023087739944, -0.026433920487761497, -0.05357922986149788, -0.03395157307386398, 0.01776091754436493, 0.03415274620056152, 0.06289628893136978, -0.06115114688873291, 0.036182329058647156, -0.03398735076189041, -0.0039473422802984715, -0.010318013839423656, 0.037170782685279846, -0.04978971928358078, 0.054852355271577835, 0.044173464179039, -0.03900948166847229, -0.06073860451579094, -0.06111270189285278, -0.022852454334497452, 0.05589212849736214, -0.043288346379995346, 0.0691835954785347, -0.008633862249553204, -0.05454825609922409]}}, {'_index': 'temp', '_id': '40', '_score': 1.541861, '_source': {'content': '4.3 Network Topology. We use networks with one input layer, one hid-\\nden layer, and one output layer. The (fully) self-connected hidden layercontains memory cells and corresponding gate units (for convenience, werefer to both memory cells and gate units as being located in the hiddenlayer). The hidden layer may also contain conventional hidden units pro-viding inputs to gate units and memory cells. All units (except for gate units)in all layers have directed connections (serve as inputs) to all units in thelayer above (or to all higher layers; see experiments 2a and 2b).\\n4.4 Memory Cell Blocks. Smemory cells sharing the same input gate\\nand the same output gate form a structure called a memory cell block of sizeS. Memory cell blocks facilitate information storage. As with conventional', 'dense_vector': [-0.008177921175956726, -0.023994075134396553, -0.08549487590789795, 0.09318530559539795, -0.017197879031300545, 0.0374918207526207, -0.010428915731608868, 0.002904297551140189, 0.09674201160669327, -0.032401617616415024, 0.03315538167953491, 0.027765657752752304, 0.025368524715304375, -0.0484619177877903, -0.007246014662086964, 0.044370222836732864, 0.0025280804838985205, 0.031141411513090134, -0.03984522446990013, -0.04399958252906799, 0.05274692550301552, -0.0904201939702034, -0.004490992054343224, -0.017893871292471886, -0.026157917454838753, -0.024497540667653084, -0.06458604335784912, -0.0756053477525711, 0.038766875863075256, -0.0792774111032486, 0.014361384324729443, -0.009895145893096924, -0.012823610566556454, 0.029216742143034935, -0.05667226016521454, -0.03965456411242485, -0.02623756229877472, -0.005340159870684147, -0.07381348311901093, -0.11001011729240417, 0.027833126485347748, 0.06106502190232277, -0.012018591165542603, 0.03633315488696098, -0.03435027226805687, 0.05183594301342964, -0.005875364411622286, -0.0355367548763752, -0.04851662367582321, -0.06678196042776108, -0.01651712879538536, 0.030207503587007523, -0.012569666840136051, 0.10764407366514206, 0.032617855817079544, 0.05507523566484451, -0.03135526552796364, 0.06730237603187561, -0.06342659145593643, 0.09379010647535324, 0.009557160548865795, -0.02236349694430828, 0.06282445788383484, -0.011076833121478558, 0.05009138584136963, -0.0037295075599104166, 0.08487531542778015, 0.047412533313035965, 0.01707891747355461, -0.003490501781925559, 0.06411208212375641, 0.008348671719431877, -0.013321597129106522, 0.002604288049042225, 0.0911896824836731, 0.04878858104348183, -0.028007209300994873, 0.027409350499510765, 0.06573133915662766, -0.05344216525554657, 0.03453050181269646, 0.07778767496347427, 0.006826397497206926, -0.009315812028944492, 0.026089681312441826, -0.03314958140254021, -0.030015673488378525, -0.012872068211436272, -0.07845538854598999, -0.06096532940864563, -0.047706179320812225, -0.03447403386235237, -0.07234970480203629, 0.018302209675312042, 0.016217311844229698, -0.09032423049211502, -0.012099559418857098, -0.0014028502628207207, -0.043240174651145935, -0.01869775354862213, 0.08490996062755585, 0.07087254524230957, 0.02768990583717823, 0.029112672433257103, 0.010715769603848457, 0.005951354745775461, 0.056228622794151306, 0.05161841958761215, -0.043364591896533966, -0.017132624983787537, -0.005330652464181185, 0.001807115855626762, -0.04781990870833397, 0.03571135550737381, -0.014324896037578583, -0.098566435277462, 0.014365151524543762, -0.033965274691581726, 0.08329547196626663, 0.02767310105264187, -0.10388229787349701, 0.015664704144001007, -0.03692511096596718, -0.062355998903512955, -0.03356124088168144, -0.029141854494810104, -0.0820363312959671, 3.9864991750077334e-33, -0.01921902596950531, -0.011858847923576832, -0.06773968786001205, 0.004509531427174807, 0.0077851261012256145, -0.022182786837220192, 0.0226064994931221, -0.015738392248749733, 0.002457723254337907, 0.008486265316605568, -0.06914500147104263, -0.07285532355308533, 0.006060190498828888, 0.08797016739845276, 0.07566006481647491, -0.07363710552453995, -0.06704270839691162, 0.041573282331228256, 0.07428933680057526, -0.0855453759431839, -0.012941298075020313, 0.052154913544654846, -0.015111944638192654, -0.03864332661032677, 0.10269980877637863, -0.030976075679063797, 0.008509143255650997, -0.058125048875808716, -0.013106712140142918, 0.028116531670093536, -0.051778364926576614, 0.07745946943759918, 0.035266220569610596, -0.06812117248773575, 0.05277856066823006, 0.05223602056503296, -0.015341502614319324, -0.049400486052036285, 0.06420432031154633, -0.06567462533712387, -0.01321820542216301, -0.010784815065562725, 0.02017245441675186, -0.0653429925441742, -0.09956711530685425, -0.04346731677651405, -0.0644422397017479, 0.030581312254071236, -0.06126935034990311, -0.0594712570309639, 0.004405910614877939, 0.003174396464601159, -0.011705832555890083, -0.05321311578154564, 0.05769310146570206, -0.0348711796104908, 0.020340781658887863, 0.05208835005760193, 0.09701860696077347, 0.1390170305967331, -0.009699770249426365, 0.03795092552900314, -0.06741801649332047, 0.08881436288356781, -0.010564868338406086, 0.06346094608306885, -0.02182789519429207, 0.0058503826148808, 0.0008627382339909673, -0.0777931660413742, 0.01006827037781477, 0.04609069228172302, -0.049487993121147156, -0.015591911971569061, -0.025039399042725563, -0.01643502153456211, -0.05432360991835594, -0.08474015444517136, -0.0796724185347557, 0.04252881184220314, 0.05223894119262695, -0.042186424136161804, -0.11743622273206711, 0.09404977411031723, -0.10015077143907547, -0.01277556549757719, 0.07015464454889297, 0.0028088970575481653, -0.0437270849943161, 0.03845391795039177, 0.07732731103897095, -0.06697191298007965, 0.11809481680393219, 0.0188286229968071, -0.08878818154335022, -4.731688390348168e-33, -0.02911965921521187, 0.07868442684412003, -0.046597931534051895, -0.005966567434370518, -0.004953385330736637, 0.0451851487159729, 0.060414869338274, -0.04230653867125511, -0.050469476729631424, 0.06715919077396393, 0.025747159495949745, 0.04028298333287239, 0.030201824381947517, -0.016860054805874825, -0.023187316954135895, 0.00882874708622694, -0.00902080163359642, -0.07337598502635956, 0.04754600673913956, -0.050476621836423874, -0.01264704018831253, 0.0882745236158371, -0.027132932096719742, 0.08505829423666, -0.014206651598215103, -0.007797034457325935, -0.01620076596736908, 0.10117995738983154, 0.021523065865039825, 0.05739721655845642, -0.004445329308509827, -0.023450633510947227, 0.021805621683597565, 0.021435201168060303, -0.004164527636021376, -0.030726730823516846, 0.06030621752142906, -0.06890997290611267, -0.013302162289619446, -0.07512999325990677, 0.02297348715364933, 0.05217914655804634, -0.024004796519875526, 0.030311977490782738, 0.04109876975417137, 0.06477238237857819, -0.0691845640540123, 0.047446563839912415, -0.0686379224061966, -0.02657330222427845, -0.06701882183551788, -0.022220781072974205, -0.028752369806170464, -0.08327431231737137, -0.01533470954746008, 0.04658889025449753, -0.020982064306735992, 0.05987133830785751, 0.0666310265660286, -0.008280598558485508, 0.025803176686167717, -0.1494635045528412, 0.02182774804532528, -0.03502877801656723, -0.033345434814691544, 0.008972907438874245, 0.06030403822660446, 0.039759401232004166, -0.027068832889199257, 0.0005517735844478011, 0.05647813156247139, 0.0692756250500679, 0.0120994932949543, -0.05808621644973755, -0.014042689464986324, 0.022862501442432404, -0.006214213091880083, -0.03783610463142395, 0.012231501750648022, 0.020210858434438705, -0.09059783816337585, -0.003131951903924346, 0.009263313375413418, -0.035603735595941544, 0.11297070980072021, 0.02153887040913105, 0.03163355588912964, 0.03602335602045059, -0.002299818443134427, -0.032626524567604065, -0.049266789108514786, -0.010130476206541061, -0.030125491321086884, 0.011812652461230755, -0.02883000485599041, -4.8538943531184486e-08, 0.027343714609742165, -0.02118239738047123, 0.09010512381792068, -0.004328129813075066, -0.03900546208024025, -0.12010030448436737, 0.1155480369925499, 0.06406659632921219, 0.05215395987033844, 0.01307777501642704, 0.01151286717504263, -0.009030580520629883, -0.10034213960170746, -0.050913870334625244, 0.11445947736501694, 0.10520900785923004, -0.04797963798046112, -0.03235067427158356, 0.029118221253156662, -0.09733863919973373, 0.0049626147374510765, -0.031567059457302094, -0.034779179841279984, 0.08855907618999481, 0.07590692490339279, -0.012880487367510796, 0.03530767560005188, 0.060012854635715485, 0.011793682351708412, 0.016745565459132195, 0.03013300523161888, 0.01565091870725155, 0.04209483414888382, 0.029953792691230774, -0.014449967071413994, 0.08675029128789902, 0.04969306290149689, 0.07568222284317017, 0.0019581480883061886, 0.0435141883790493, -0.012371638789772987, -0.12778834998607635, 0.006308150012046099, 0.025554727762937546, -0.010521708987653255, 0.0609639510512352, -0.027397237718105316, 0.03710656613111496, -0.07780738174915314, 0.0004583351546898484, -0.04349299520254135, 0.030283909291028976, -0.017287855967879295, 0.07086941599845886, 0.06678985059261322, -0.03529142215847969, -0.0251734871417284, -0.038320012390613556, 0.03804035857319832, 0.0402495451271534, 0.013537229038774967, 0.04332935810089111, -0.024272115901112556, 0.028703046962618828]}}, {'_index': 'temp', '_id': '35', '_score': 1.5356822, '_source': {'content': 'by rotating Figure 1 by 90 degrees anticlockwise, it will match with the corre-sponding parts of Figure 2). The example assumes dense connectivity: each gateunit and each memory cell sees all non-output units. For simplicity, however,outgoing weights of only one type of unit are shown for each layer. With theefﬁcient, truncated update rule, error ﬂows only through connections to outputunit, and through ﬁxed self-connections within cell blocks (not shown here; seeFigure 1). Error ﬂow is truncated once it “wants” to leave memory cells or gateunits. Therefore, no connection shown above serves to propagate error back tothe unit from which the connection originates (except for connections to outputunits), although the connections themselves are modiﬁable. That is why the trun-cated LSTM algorithm is so efﬁcient, despite its ability to bridge very long timelags. See the text and the appendix for details. Figure 2 shows the architectureused for experiment 6a; only the bias of the noninput', 'dense_vector': [-0.028971998021006584, -0.08168286830186844, -0.06518225371837616, 0.017958417534828186, -0.023104049265384674, -0.011527527123689651, -0.050066519528627396, 0.005385712254792452, 0.03244149312376976, -0.06564425677061081, 0.06532071530818939, 0.017998913303017616, 0.041463371366262436, -0.12150559574365616, -0.09802062064409256, 0.018663272261619568, -0.02190629579126835, 0.022939719259738922, -0.07646729052066803, -0.07236272841691971, -0.030441749840974808, -0.016977231949567795, -0.03894833102822304, 0.030690332874655724, -0.04806071147322655, -0.01348028238862753, -0.0341050885617733, -0.009306997060775757, 0.09382785111665726, -0.07086345553398132, 0.02660529874265194, 0.06188831478357315, -0.09059612452983856, 0.06489777565002441, -0.07860419154167175, -0.024526536464691162, -0.01450071856379509, -0.06014182046055794, 0.010569464415311813, 0.031141886487603188, 0.07084015011787415, 0.01291700080037117, 0.022615697234869003, 0.011006503365933895, 0.019057149067521095, 0.02719416841864586, -0.018757762387394905, -0.04685669764876366, -0.14907562732696533, 0.008881286717951298, -0.0069344183430075645, 0.12393233925104141, -0.007249222602695227, 0.11269515752792358, -0.06553634256124496, 0.015124581754207611, 0.015609674155712128, 0.09402686357498169, -0.025464167818427086, 0.008891977369785309, 0.017665579915046692, -0.011721012182533741, -0.03473791480064392, -0.03137071803212166, 0.020634138956665993, -0.0966356098651886, 0.08433017879724503, 0.024953221902251244, 0.018058273941278458, 0.05461423844099045, 0.04804567992687225, 0.004500736948102713, -0.04640510305762291, 0.050591401755809784, 0.04202818125486374, 0.0641486868262291, 0.052885569632053375, 0.014565424993634224, 0.043265827000141144, -0.057534877210855484, 0.036801695823669434, 0.025179816409945488, 0.08581940829753876, -0.0005994912353344262, 0.057798538357019424, -0.03564627468585968, -0.08570779860019684, 0.05860181525349617, -0.06145581603050232, -0.016236145049333572, 0.07731858640909195, 0.0320226326584816, -0.052566494792699814, 0.023722797632217407, 0.03287094086408615, 0.04027213156223297, 0.02772599272429943, 0.00969722680747509, -0.0504937581717968, 0.02589401789009571, -0.005437156651169062, 0.041620392352342606, -0.038586657494306564, 0.062206000089645386, 0.024765538051724434, 0.03416513279080391, 0.06729066371917725, 0.06466275453567505, 0.020744312554597855, -0.03738928586244583, 0.04750729352235794, -0.04320310056209564, -0.006537542212754488, 0.013910426758229733, 0.04454483464360237, -0.09023147821426392, 0.03306875750422478, -0.05337323248386383, 0.03047892078757286, 0.030390867963433266, -0.09112323075532913, -0.022861506789922714, -0.013480653055012226, 0.034022632986307144, -0.03544493392109871, -0.01937774382531643, -0.05354557931423187, 7.856809549644315e-33, -0.016945285722613335, -0.009060617536306381, -0.038775734603405, -0.1517145186662674, 0.07287712395191193, 0.04285963997244835, -0.044502269476652145, -0.03385443985462189, -0.0032685089390724897, 0.0445186085999012, -0.13780051469802856, -0.03992069140076637, -0.01422214601188898, -0.02128368429839611, 0.04120530188083649, -0.04149575158953667, 0.005826192442327738, 0.008914206176996231, 0.01870264858007431, -0.05007383972406387, 0.014543996192514896, 0.006292879581451416, 0.023571521043777466, -0.06496116518974304, 0.006009251810610294, -0.013425404205918312, 0.040750764310359955, 0.03194572404026985, -0.0330105721950531, 0.05241455137729645, -0.05631217733025551, 0.0512680858373642, 0.026645701378583908, -0.05648104101419449, 0.04460331052541733, -0.14450229704380035, 0.010325826704502106, -0.05188889056444168, -0.016199098899960518, -0.07270337641239166, -0.008657955564558506, 0.022121690213680267, 0.0529170036315918, -0.014461014419794083, -0.0647302120923996, -0.0897955372929573, 0.03326791152358055, 0.04484641179442406, 0.012424083426594734, -0.0028525907546281815, 0.07999936491250992, 0.006581731606274843, -0.11386065930128098, -0.10425267368555069, 0.03682330250740051, 0.008296444080770016, -0.001955391140654683, 0.07814096659421921, 0.037523962557315826, 0.1533745676279068, -0.004142157733440399, 0.04691818356513977, -0.048440709710121155, 0.07186632603406906, 0.011345809325575829, 0.09745560586452484, -0.058625224977731705, -0.0584464892745018, -0.005121682304888964, -0.039517808705568314, -0.0361519381403923, 0.007026433479040861, -0.004739406052976847, 0.041567329317331314, 0.034404072910547256, -0.08465666323900223, 0.10731219500303268, 0.0004819865571334958, -0.06806298345327377, 0.008091392926871777, 0.07530855387449265, -0.023832742124795914, -0.050710152834653854, 0.02647130936384201, -0.08418959379196167, -0.008552098646759987, 0.0764753594994545, -0.04186677560210228, -0.06718854606151581, 0.0485212542116642, 0.058279599994421005, -0.0213569775223732, 0.052385058254003525, 0.0287310853600502, -0.04458692669868469, -7.013888264608577e-33, -0.024444682523608208, 0.04741324856877327, -0.032954949885606766, 0.01426606997847557, -0.033918533474206924, 0.01526758074760437, 0.006780602969229221, -0.035540319979190826, 0.04689720645546913, -0.0010956268524751067, 0.10596136748790741, 0.03659569472074509, -0.08948348462581635, 0.05176059529185295, -8.718737808521837e-05, -0.09131214022636414, 0.03696886822581291, -0.08577857911586761, -0.006716497708112001, -0.017518799751996994, 0.11514989286661148, 0.039805471897125244, -0.043729253113269806, 0.040531009435653687, -0.035631801933050156, 0.02430858463048935, -0.04260144382715225, 0.08078037202358246, -0.005463551264256239, 0.013572798110544682, 0.030984926968812943, -0.0802401602268219, -0.03050990030169487, 0.023896852508187294, -0.025895243510603905, 0.042859889566898346, 0.026874136179685593, -0.008509327657520771, 0.010344631969928741, -0.04563342034816742, 0.07470641285181046, 0.020172571763396263, -0.044890210032463074, -0.03765911981463432, 0.03941948339343071, 0.02079656906425953, -0.06713841110467911, -0.02794983610510826, -0.06864383816719055, 0.008296681568026543, 0.012633911333978176, 0.022218653932213783, -0.0022830988746136427, 0.07431771606206894, 0.04519147053360939, 0.05170168727636337, -0.030385905876755714, 0.0210674237459898, 0.03614314645528793, -0.04051562026143074, -0.0263864453881979, -0.05731377750635147, -0.00656832056120038, -0.0966583862900734, 0.06491697579622269, 0.02730238251388073, 0.05104929581284523, -0.0033877415116876364, 0.055812541395425797, 0.0462828204035759, 0.058786436915397644, 0.09159496426582336, -0.022039374336600304, -0.017320821061730385, -0.015193364582955837, -0.01316200289875269, -0.052335962653160095, -0.052644986659288406, 0.028043966740369797, -0.03325318545103073, -0.08433911204338074, 0.04813598096370697, 0.04669708386063576, 0.024027757346630096, 0.1178416907787323, 0.005150167271494865, 0.006689462810754776, 0.07556798309087753, 0.07012312859296799, -0.007590490859001875, 0.09512554854154587, -0.00760261807590723, 0.01689840294420719, -0.011773837730288506, 0.014187677763402462, -6.37026502658955e-08, -0.017626741901040077, -0.07174583524465561, -0.06855280697345734, 0.002155054360628128, 0.062329959124326706, -0.14548933506011963, 0.030884582549333572, -0.05236762762069702, 0.02977919392287731, -0.01568998023867607, 0.024363147094845772, 0.022640865296125412, -0.13802222907543182, -0.03789053112268448, 0.041139453649520874, 0.057829149067401886, -0.029128726571798325, -0.06253805011510849, 0.004733028821647167, -0.007301779929548502, -0.059210244566202164, -0.0027574298437684774, -0.05013253167271614, 0.04317907616496086, 0.023057028651237488, -0.015131603926420212, -0.010164428502321243, 0.05903315544128418, -0.02919197455048561, 0.05423611029982567, 0.02961362898349762, -0.03847214952111244, 0.011234734207391739, 0.06248946115374565, 0.004199546296149492, 0.06364176422357559, 0.07363106310367584, 0.04026943817734718, -0.05528169870376587, 0.013653380796313286, 0.008343343622982502, 0.018051985651254654, 0.013174512423574924, 0.03523068130016327, 0.015760181471705437, 0.05862211808562279, -0.03323609381914139, -0.049748726189136505, -0.022783586755394936, 0.003888685256242752, 0.043840065598487854, 0.02383626438677311, 0.004629733040928841, 0.056192271411418915, 0.003259331453591585, -0.09401347488164902, -0.01752226985991001, -0.105959951877594, -0.011155852116644382, 0.03942576423287392, 0.011497222818434238, 0.03366098180413246, -0.002140972064808011, -0.04703407734632492]}}, {'_index': 'temp', '_id': '126', '_score': 1.5275309, '_source': {'content': '•The LSTM algorithm’s update complexity per weight and time step is\\nessentially that of BPTT, namely, O(1). This is excellent in comparison\\nto other approaches such as RTRL. Unlike full BPTT, however, LSTMis local in both space and time.\\n7 Conclusion\\nEach memory cell’s internal architecture guarantees constant error ﬂow\\nwithin its CEC, provided that truncated backpropagation cuts off error ﬂowtrying to leak out of memory cells. This represents the basis for bridgingvery long time lags. Two gate units learn to open and close access to errorﬂow within each memory cell’s CEC. The multiplicative input gate affordsprotection of the CEC from perturbation by irrelevant inputs. Similarly,the multiplicative output gate protects other units from perturbation bycurrently irrelevant memory contents.\\nTo ﬁnd out about LSTM’s practical limitations we intend to apply it to', 'dense_vector': [-0.04668501019477844, -0.03837600722908974, -0.0426851324737072, 0.0476570762693882, 0.03451021760702133, -0.016045350581407547, -0.07365716248750687, 0.047713059931993484, 0.031477827578783035, -0.0394953191280365, -0.002286435104906559, 0.00208097230643034, -0.015438348986208439, -0.1251172125339508, -0.03443130478262901, 0.00417881365865469, -0.019005274400115013, 0.034581493586301804, -0.029130548238754272, -0.056392595171928406, 0.0017266664654016495, -0.04058720916509628, -0.04811513051390648, 0.048864804208278656, -0.06759440898895264, 0.0015007720794528723, -0.06927525997161865, -0.01766711100935936, 0.07849521189928055, -0.09564344584941864, 0.015608308836817741, 0.01467757299542427, -0.004743353929370642, 0.038183677941560745, -0.08188124746084213, 0.050732504576444626, 0.017934974282979965, -0.09629223495721817, 0.010643445886671543, -0.006296084728091955, 0.025940915569663048, 0.009225159883499146, -0.0019856730941683054, -0.013999228365719318, 0.0696161761879921, 0.02141938917338848, 0.046387191861867905, -0.04772019758820534, -0.14901649951934814, -0.014929662458598614, 0.02019200474023819, 0.06522117555141449, -0.03287744149565697, 0.10866783559322357, -0.07288943976163864, 0.013445088639855385, -0.002085307613015175, 0.11115208268165588, -0.04121016710996628, 0.06369586288928986, -0.06103348731994629, -0.026623478159308434, -0.007783546112477779, -0.022945532575249672, 0.038228604942560196, -0.037963345646858215, 0.04083112254738808, 0.0015514541883021593, 0.04388955235481262, 0.04137486219406128, -0.0021256108302623034, -0.0015868870541453362, -0.06227373704314232, 0.0512290894985199, 0.05626227706670761, -0.011884781531989574, 0.01701265573501587, -0.036198053508996964, 0.0035743352491408587, -0.0340348444879055, 0.055188365280628204, 0.03796521946787834, -0.02905745804309845, -0.059919342398643494, 0.07373546808958054, -0.050393953919410706, -0.02284306101500988, 0.046697355806827545, -0.013704956509172916, -0.017531489953398705, 0.0976218581199646, -0.0698937326669693, -0.041381873190402985, 0.026657070964574814, 0.07795483618974686, 0.015679968520998955, 0.02573930285871029, -9.096568101085722e-05, -0.03716002777218819, -0.02611580118536949, 0.02523762919008732, 0.0972885861992836, -0.0484953410923481, 0.060651104897260666, 0.03790263086557388, -0.019399752840399742, 0.09530684351921082, 0.026845227926969528, 0.050640303641557693, -0.06444775313138962, 0.042021267116069794, 0.02741517499089241, 0.004885513801127672, 0.02370782010257244, 0.009623341262340546, -0.07644223421812057, -0.05402649939060211, -0.08532103896141052, 0.017528677359223366, 0.04165864363312721, -0.09880765527486801, -0.037896301597356796, -0.04345627501606941, -0.00651936698704958, -0.04063853994011879, -0.06508224457502365, -0.04987261816859245, 7.595901966323585e-33, -0.010286347940564156, 0.01567072607576847, -0.10527315735816956, -0.045511893928050995, -0.005763762164860964, 0.057826846837997437, 0.0008720745099708438, -0.005589603912085295, -0.008786262944340706, -0.007353547494858503, 0.027609016746282578, -0.07850224524736404, 0.01482430100440979, 0.004809739533811808, 0.04686286300420761, -0.03690154850482941, -0.0607854388654232, 0.04095153883099556, 0.02568606473505497, -0.052082546055316925, 0.053794216364622116, -0.04912565276026726, -0.016893181949853897, -0.08668098598718643, 0.05083901435136795, 0.012117448262870312, 0.04294775426387787, -0.005767333786934614, -0.01686888374388218, 0.0476599782705307, -0.08560401201248169, 0.07356753945350647, -0.035347167402505875, 0.02398252673447132, 0.09193318337202072, -0.01407378725707531, 0.010413859970867634, -0.11596769094467163, 0.0636189803481102, -0.09593603760004044, -0.06863719969987869, 0.02910435013473034, -0.0019312171498313546, -0.04995924234390259, -0.07176213711500168, -0.10674615204334259, -0.002905511064454913, 0.01995074190199375, -0.048248808830976486, -0.032257430255413055, 0.03964945673942566, 0.0153493732213974, -0.09541252255439758, -0.027524402365088463, 0.07666092365980148, -0.02901678904891014, 0.023029163479804993, 0.03467172756791115, 0.0340028740465641, 0.16830089688301086, 0.01681484468281269, 0.007437962107360363, -0.014982870779931545, 0.09337247163057327, 0.04313620924949646, 0.09975197911262512, -0.020646175369620323, 0.01966056600213051, 0.011955403722822666, -0.051180530339479446, -0.001897423411719501, -0.09365221858024597, 0.04707055911421776, 0.022440074011683464, -0.011098322458565235, -0.013984269462525845, 0.11175739765167236, -0.025929177179932594, -0.018174711614847183, 0.03086523711681366, 0.03654735907912254, -0.02623063512146473, -0.027685966342687607, 0.013539913110435009, -0.010606580413877964, -0.0736500695347786, 0.049416594207286835, -0.07184416800737381, -0.06397601962089539, 0.003890085266903043, -0.026839379221200943, -0.030209166929125786, 0.11072039604187012, -0.0037120990455150604, -0.1323007345199585, -7.670515000874091e-33, -0.020132455974817276, -0.025526465848088264, -0.017237985506653786, 0.052929751574993134, -0.030167272314429283, -0.07028542459011078, -0.0987142026424408, -0.034140702337026596, 0.01967773400247097, 0.03086954914033413, 0.014692272990942001, 0.004951799288392067, -0.003583696437999606, -0.03286487236618996, 0.0739511102437973, -0.021174367517232895, 0.033277131617069244, -0.02243812382221222, -0.012787522748112679, -0.01147312019020319, 0.0957578718662262, -0.011976264417171478, -0.008929814212024212, 0.07185226678848267, -0.0497603714466095, 0.03501452878117561, -0.1205894947052002, 0.08519314974546432, 0.007032280787825584, -0.040009286254644394, -0.05055958032608032, -0.05165380612015724, -0.025940021499991417, 0.0250321663916111, 0.022400354966521263, 0.02169726975262165, 0.05215395987033844, 0.025141926482319832, 0.054408203810453415, 0.0015392822679132223, 0.1491088569164276, 0.0480310320854187, -0.04798433184623718, -0.12243475764989853, 0.043066203594207764, 0.05109867453575134, -0.08443111926317215, 0.021973995491862297, 0.0005296539166010916, 0.017389046028256416, 0.05990835651755333, 0.043331075459718704, 0.007910667918622494, 0.00806026067584753, -0.010629482567310333, -0.010009131394326687, -0.06211670860648155, 0.08924947679042816, 0.05467193201184273, -0.06866364181041718, -0.0198252871632576, -0.042690761387348175, 0.01276939082890749, -0.019969645887613297, -2.059013604593929e-05, 0.03118154965341091, 0.04817090183496475, -0.02878786250948906, 0.027253756299614906, 0.029666952788829803, 0.0984797477722168, 0.03196059912443161, -0.04034576565027237, -0.0005191526142880321, -0.07294293493032455, 0.015751812607049942, -0.02663313038647175, -0.1307225227355957, -0.024984901770949364, -0.028439819812774658, -0.016071058809757233, 0.027261484414339066, 0.039219677448272705, 0.0024970744270831347, 0.031670331954956055, 0.056310977786779404, -0.006046800874173641, -0.0025281691923737526, 0.018695952370762825, -0.08515306562185287, -0.004390875808894634, 0.014152259565889835, -0.010404065251350403, -0.006108905654400587, -0.0026107188314199448, -5.284945814310049e-08, -0.0036128293722867966, -0.046078480780124664, 0.0046652331948280334, 0.06874041259288788, 0.04706495255231857, -0.14070546627044678, 0.027977554127573967, -0.013180024921894073, 0.053831543773412704, 0.013776700012385845, 0.11785363405942917, -0.02953454479575157, -0.0894961953163147, -0.08574315905570984, 0.05368647351861, 0.05704593285918236, -0.0275751780718565, -0.04341931268572807, 0.003934228327125311, -0.00152956775855273, 0.05313979461789131, -0.01457188744097948, 0.052406832575798035, 0.05484052747488022, 0.032950256019830704, -0.04885488748550415, 0.03944912925362587, 0.07798593491315842, 0.018648920580744743, -0.01726040057837963, 0.051449961960315704, 0.005224503111094236, 0.035305481404066086, 0.08561670035123825, 0.040798697620630264, 0.028200291097164154, 0.05146065354347229, 0.020432330667972565, -0.03604753687977791, 0.11988093703985214, 0.01766551285982132, 0.024491772055625916, -0.026946023106575012, 0.08986736088991165, -0.04862183704972267, 0.0034872754476964474, -0.04093810170888901, -0.07786440849304199, -0.07515936344861984, -0.04114056006073952, 0.04811128228902817, 0.04928230494260788, 0.002800961257889867, 0.03734764829277992, 0.01624828390777111, 0.03259846195578575, -0.007285861764103174, -0.030904823914170265, 0.016043350100517273, 0.05266284942626953, -0.04615657404065132, 0.04034416377544403, 0.025736352428793907, 0.005620047450065613]}}, {'_index': 'temp', '_id': '43', '_score': 1.5121506, '_source': {'content': 'Long Short-Term Memory 1747\\nthis, once an error signal arrives at a memory cell output, it gets scaled by\\noutput gate activation and h′. Then it is within the memory cell’s CEC, where\\nit can ﬂow back indeﬁnitely without ever being scaled. When it leaves thememory cell through the input gate and g, it is scaled once more by input\\ngate activation and g\\n′. It then serves to change the incoming weights before\\nit is truncated (see the appendix for formulas).\\n4.6 Computational Complexity. As with Mozer’s focused recurrent back-\\npropagation algorithm (Mozer, 1989), only the derivatives ∂scj/∂wilneed to\\nbe stored and updated. Hence the LSTM algorithm is very efﬁcient, withan excellent update complexity of O(W), where Wthe number of weights\\n(see details in the appendix). Hence, LSTM and BPTT for fully recurrentnets have the same update complexity per time step (while RTRL’s is muchworse). Unlike full BPTT, however, LSTM is local in space and time:\\n3there', 'dense_vector': [-0.03883599489927292, -0.06342572718858719, -0.02181524597108364, 0.0033373022451996803, -0.011126983910799026, -0.001624758937396109, -0.08623480796813965, 0.04121098294854164, 0.061940934509038925, -0.06584596633911133, -0.013496995903551579, 0.05428210645914078, 0.01886880397796631, -0.11443395912647247, -0.0529954731464386, 0.01724051497876644, 0.03655419871211052, 0.05646391957998276, -0.02982303686439991, -0.0471046045422554, 0.02126781828701496, -0.015951093286275864, -0.030914930626749992, 0.0019282938446849585, -0.03656034916639328, 0.029215548187494278, -0.061771128326654434, -0.00492288451641798, 0.10571083426475525, -0.06691443920135498, 0.05173492804169655, 0.08879198879003525, 0.012141856364905834, 0.06625733524560928, -0.11034636944532394, 0.1034388318657875, -0.05016819015145302, -0.04123812913894653, -0.013071402907371521, 0.02678215503692627, 0.023026473820209503, 0.04252329841256142, -0.02085007354617119, 0.01992831751704216, 0.08881083130836487, -0.02673770859837532, 0.03445403277873993, -0.012589216232299805, -0.1307053416967392, -0.022501595318317413, 0.023832784965634346, 0.05189499258995056, -0.04627983644604683, 0.07394612580537796, -0.04863440990447998, 0.03323204815387726, 0.016948096454143524, 0.13773570954799652, -0.027355311438441277, 0.06625886261463165, -0.08551164716482162, -0.021388662979006767, -0.07412949949502945, -0.061210256069898605, 0.05587496981024742, -0.05266999453306198, 0.046689458191394806, -0.05638926848769188, 0.056553542613983154, 0.020384926348924637, 0.04072355851531029, -0.02782144956290722, -0.026799174025654793, 0.006193268578499556, 0.0013759578578174114, -0.015325428918004036, 0.0798836275935173, 0.024211103096604347, 0.001480212202295661, 0.0036965166218578815, 0.09683805704116821, 0.03430855646729469, 0.01819039322435856, -0.0742390900850296, 0.0785917341709137, -0.042898934334516525, -0.06551368534564972, 0.09045806527137756, 0.020485838875174522, -0.019942577928304672, 0.07665941119194031, -0.07234510034322739, -0.09570091962814331, 0.009913880378007889, 0.014713464304804802, -0.006750577129423618, 0.0150001784786582, 0.01729189045727253, 0.003607773454859853, 0.000708858366124332, 0.03693185746669769, 0.04228689894080162, -0.01036884542554617, 0.0581209622323513, 0.035094406455755234, -0.01528438925743103, 0.08127939701080322, 0.09585878252983093, 0.020800888538360596, -0.09840013831853867, 0.0304688960313797, 0.046394698321819305, 0.05819619819521904, 0.058377381414175034, 0.05975988134741783, -0.10393519699573517, -0.012835648842155933, -0.08471915125846863, 0.03229396045207977, 0.08782437443733215, -0.03949530050158501, -0.03213293477892876, -0.030982768163084984, -0.021012183278799057, -0.08615781366825104, 0.002183638047426939, -0.03480822220444679, 6.281819770275227e-33, -0.03416164964437485, 0.03857170417904854, -0.04074307158589363, -0.058644432574510574, 0.013161390088498592, 0.09873149544000626, 0.030553288757801056, -0.026774674654006958, 0.00814912561327219, -0.034784454852342606, -0.02312587760388851, -0.01904889941215515, -0.05081476643681526, 0.03662506863474846, 0.018137317150831223, -0.06708125025033951, -0.05346845090389252, 0.05134662240743637, -0.04319634288549423, -0.031327296048402786, 0.03721829131245613, 0.014862236566841602, 0.0026316381990909576, -0.04884033277630806, 0.019533749669790268, 0.0011904718121513724, 0.07905814051628113, -0.021418781951069832, -0.007048915605992079, 0.023319968953728676, -0.005988267250359058, 0.021185988560318947, -0.02167578600347042, 0.009539381600916386, 0.03361870348453522, -0.053265687078237534, -0.013032091781497002, -0.08733969926834106, 0.07570894807577133, -0.1431700587272644, -0.09358682483434677, 0.047069672495126724, -0.013758211396634579, 0.00041194327059201896, -0.07371661812067032, -0.13771560788154602, -0.004650545306503773, 0.008268766105175018, -0.01206244993954897, -0.03944888710975647, 0.03436920791864395, -0.014129452407360077, -0.07683731615543365, -0.05559596046805382, 0.004458583891391754, -0.004571639467030764, 0.025242522358894348, 0.05842503532767296, 0.051573216915130615, 0.15186144411563873, 0.032864660024642944, 0.046026431024074554, -0.0011058771051466465, 0.0809096023440361, 0.07090815901756287, 0.09183735400438309, -0.07578621059656143, 0.009284481406211853, 0.0011735939187929034, -0.0066270711831748486, 0.07155663520097733, 0.006738079246133566, 0.03868965059518814, 0.01567590795457363, 0.05126376450061798, -0.05894683673977852, 0.09405852854251862, -0.03736712411046028, -0.07247274369001389, 0.013652176596224308, 0.0011067955056205392, 0.03475583344697952, 0.01440668385475874, 0.02254500985145569, -0.032545823603868484, -0.056346677243709564, 0.04626306891441345, -0.0893763080239296, -0.07077190279960632, 0.04239587485790253, -0.034966956824064255, -0.053031209856271744, 0.05484352260828018, -0.008059087209403515, -0.09244568645954132, -5.479913687191586e-33, 0.005917288362979889, -0.008221284486353397, 0.006125295534729958, 0.04228725656867027, -0.020242057740688324, -0.06579717248678207, -0.11351549625396729, 0.06343907862901688, 9.137490997090936e-05, -0.023461926728487015, 0.015963129699230194, 0.009965809993445873, -0.017924081534147263, 0.02491496130824089, 0.03936197981238365, -0.008024090901017189, 0.013747509568929672, -0.06133299693465233, -0.0737522765994072, -0.028096286579966545, 0.06283953040838242, 0.005748073570430279, -0.055062707513570786, 0.06043827906250954, -0.07833945751190186, -0.009298136457800865, -0.05788761377334595, 0.09591656178236008, 0.03232720494270325, -0.057465605437755585, -0.04472120851278305, -0.03277396038174629, -0.04792063310742378, 0.05605458468198776, 0.02081233821809292, 0.05421127751469612, 0.04303295537829399, 0.02846410498023033, 0.023508312180638313, 0.032506342977285385, 0.10638342052698135, 0.050202030688524246, 0.012849841266870499, -0.08654608577489853, 0.053551703691482544, 0.015360216610133648, -0.12343583256006241, -0.011634198017418385, 0.04491579905152321, -0.002161566400900483, 0.009751350618898869, 0.013397435657680035, -0.008780715055763721, 0.015104017220437527, -0.024443423375487328, -0.012617245316505432, -0.08667363971471786, 0.031385816633701324, -0.007875382900238037, -0.07354018837213516, -0.026091713458299637, -0.06376022845506668, -0.0003345496952533722, -0.07554788887500763, 0.007517680991441011, 0.06526512652635574, 0.057399459183216095, -0.041783519089221954, 0.0455605573952198, 0.029210379347205162, 0.02711462788283825, 0.04033006727695465, -0.03779849782586098, 0.027372164651751518, -0.070706807076931, -0.035350117832422256, -0.04635677859187126, -0.12016653269529343, -0.03111952915787697, -0.057541050016880035, 0.028132878243923187, 0.0471833162009716, 0.03482258319854736, 0.0021210110280662775, 0.04204103723168373, 0.04528762400150299, 0.04459767043590546, 0.08952707797288895, 0.02362692914903164, -0.011166702955961227, 0.039127472788095474, -0.01936906948685646, 0.025724370032548904, -0.045775413513183594, -0.0050529735162854195, -5.701346594833012e-08, -0.008939986117184162, -0.05127117037773132, -0.004728872328996658, 0.0340847373008728, 0.013181114569306374, -0.0923510491847992, 0.06876829266548157, -0.024196816608309746, 0.04624602198600769, -0.015685297548770905, 0.06273477524518967, -0.015588883310556412, -0.07878687232732773, -0.06095267832279205, 0.006849169265478849, 0.07500553876161575, -0.028635336086153984, -0.08851642161607742, 0.019290396943688393, -0.008648872375488281, 0.0024068248458206654, 0.0019929136615246534, 0.03780544176697731, 0.10809052735567093, 0.025612978264689445, -0.035517897456884384, -0.027710147202014923, 0.06909684836864471, 0.011561742052435875, -0.00014453921176027507, 0.046692922711372375, 0.042635105550289154, 0.042740195989608765, 0.100084587931633, -0.025677500292658806, -0.009005854837596416, 0.07134942710399628, 0.024397438392043114, -0.03230569511651993, 0.05949939042329788, 0.04532686248421669, 0.016434183344244957, 0.01754668727517128, 0.06257744878530502, -0.01111362874507904, -0.03697381541132927, -0.05464276671409607, -0.07018785178661346, -0.012901770882308483, -0.02884766459465027, 0.08253882825374603, 0.043491464108228683, -0.025465035811066628, 0.00469095166772604, 0.01677992008626461, -0.01718023233115673, -0.05601915717124939, -0.05677832290530205, 0.053154606372117996, 0.009557769633829594, -0.04215102642774582, 0.06141446530818939, 0.029076633974909782, -0.01473564188927412]}}, {'_index': 'temp', '_id': '64', '_score': 1.5101968, '_source': {'content': '5.1.4 Architectures. Architectures for RTRL, ELM, and RCC are reported\\nin the references listed above. For LSTM, we use three (four) memory cellblocks. Each block has two (one) memory cells. The output layer’s onlyincoming connections originate at memory cells. Each memory cell andeach gate unit receives incoming connections from all memory cells andgate units (the hidden layer is fully connected; less connectivity may workas well). The input layer has forward connections to all units in the hiddenlayer. The gate units are biased. These architecture parameters make it easyto store at least three input signals (architectures 3-2 and 4-1 are employedto obtain comparable numbers of weights for both architectures: 264 for 4-1and 276 for 3-2). Other parameters may be appropriate as well, however. Allsigmoid functions are logistic with output range [0 ,1], except for h, whose\\nrange is [−1,1], and g, whose range is [−2,2]. All weights are initialized in', 'dense_vector': [-0.020116258412599564, -0.09346945583820343, -0.11867210268974304, -0.007581530138850212, -0.049994729459285736, 0.05147835984826088, -0.04852999001741409, 0.04170169681310654, -0.04818246141076088, -0.00923402514308691, 0.02402406744658947, -0.03463716059923172, -0.0017100418917834759, -0.06807646155357361, -0.024300148710608482, 0.07282082736492157, 0.021185800433158875, 0.015618239529430866, -0.03900066763162613, -0.022474564611911774, 0.032929543405771255, -0.010389761999249458, -0.00462673045694828, -0.01921391487121582, 0.003549351589754224, -0.04631945863366127, -0.009001858532428741, -0.012924741953611374, 0.06161482259631157, -0.07292168587446213, 0.03905949741601944, -0.03795203939080238, 0.035961639136075974, 0.09243538975715637, -0.09492086619138718, -0.004314106423407793, -0.10497776418924332, -0.13101497292518616, -0.028982939198613167, 0.03216662257909775, -0.022632623091340065, 0.02987455204129219, -0.011183934286236763, -0.0245478805154562, 0.012827814556658268, 0.02701813355088234, -0.025041967630386353, -0.0021955573465675116, -0.06256108731031418, 0.008859689347445965, 0.0037460874300450087, 0.08507426083087921, -0.007042378652840853, 0.1024637520313263, -0.09235668182373047, 0.011903668753802776, -0.04634664207696915, 0.0005992149817757308, -0.05480291321873665, 0.017007045447826385, -0.0629328116774559, 0.008474617265164852, -0.03179870918393135, -0.017410757020115852, -0.029054824262857437, 0.034418586641550064, -0.008442072197794914, 0.017916131764650345, 0.12160121649503708, 0.013319783844053745, -0.013335537165403366, -0.04547389969229698, -0.021264055743813515, 0.03492334112524986, 0.04389944672584534, 0.004236441105604172, 0.09395312517881393, 0.0059195626527071, 0.013503404334187508, -0.0887201726436615, 0.020869892090559006, 0.029962778091430664, -0.02044171467423439, -0.050143204629421234, 0.10218777507543564, 0.005636006593704224, -0.03801463544368744, 0.07249186933040619, 0.04575493931770325, 0.001402544672600925, 0.037570126354694366, 0.0368928536772728, -0.0504511259496212, -0.0181361623108387, 0.03558671101927757, -0.0258281622081995, 0.0285422895103693, -0.019177354872226715, -0.054684218019247055, -0.012268275953829288, 0.003625661600381136, 0.030978811904788017, 0.04352806508541107, 0.016583431512117386, 0.03456913307309151, 0.011526268906891346, 0.05822908133268356, 0.006906136404722929, -0.03808857128024101, -0.10408423840999603, 0.011436750181019306, 0.02910073846578598, 0.008177576586604118, 0.046195317059755325, 0.06677096337080002, -0.05147207900881767, 0.06259307265281677, -0.09548744559288025, 0.029506299644708633, 0.10302480310201645, -0.11713968217372894, -0.06331468373537064, -0.03712112084031105, 0.058431338518857956, -0.02445928193628788, -0.014405625872313976, -0.10107982158660889, 7.913455887726472e-33, -0.027975553646683693, 0.04106718301773071, -0.08867525309324265, -0.035415321588516235, 0.0026482283137738705, 0.09192489087581635, 0.03446594625711441, -0.04703103378415108, -0.011476792395114899, 0.07677361369132996, -0.10219459235668182, -0.0036582581233233213, -0.01899157650768757, 0.07005110383033752, 0.020750220865011215, -0.06500735878944397, -0.0017165063181892037, 0.06159088388085365, 0.030823910608887672, -0.018837852403521538, -0.04997648298740387, 0.009916837327182293, -0.010497026145458221, -0.07026780396699905, 0.07898496836423874, 0.03367684409022331, 0.07236082851886749, -0.036353401839733124, -0.020686747506260872, 0.041237398982048035, -0.035586148500442505, 0.011518693529069424, -0.07264344394207001, -0.04872991889715195, 0.07648587971925735, 0.00549877667799592, -0.0759543925523758, -0.04083942249417305, 0.018978118896484375, -0.08477992564439774, -0.06212562322616577, 0.013666356913745403, 0.006451342720538378, -0.08652849495410919, -0.05920381844043732, -0.08980866521596909, 0.017494546249508858, 0.08433719724416733, -0.021906083449721336, -0.06751860678195953, 0.04131835326552391, -0.01376561913639307, -0.07902087271213531, -0.0661906823515892, 0.05387401580810547, -0.04099500551819801, -0.004073700867593288, 0.12103220820426941, 0.0856545940041542, 0.19674229621887207, -0.052690111100673676, 0.032090723514556885, -0.01906285248696804, 0.05193207412958145, 0.06702311336994171, 0.041855957359075546, -0.04798901081085205, -0.08801448345184326, -0.025407707318663597, -0.014187692664563656, 0.040401700884103775, -0.08580365031957626, 0.0605383925139904, 0.037587039172649384, -0.0077687595039606094, 0.039106596261262894, 0.04397057741880417, -0.04416043683886528, -0.05861975625157356, 0.03658894822001457, 0.02825881727039814, 0.07399625331163406, -0.04266984015703201, 0.03161030635237694, 0.005453865509480238, -0.03832826763391495, 0.059661995619535446, -0.09656490385532379, -0.0838465467095375, 0.04319155961275101, -0.027392972260713577, -0.00953533872961998, 0.04958508536219597, 0.007739179767668247, -0.1181764006614685, -7.119626920200918e-33, -0.040337011218070984, 0.039689235389232635, -0.031380508095026016, 0.03474801406264305, -0.05931330844759941, -0.01822676695883274, -0.032657865434885025, -0.020984968170523643, -0.0065413471311330795, 0.07517239451408386, 0.13030533492565155, -0.010052147321403027, 0.010167649947106838, -0.006122217047959566, 0.08151759952306747, -0.022784624248743057, 0.015763839706778526, -0.005675828084349632, 0.01660621352493763, -0.01307632215321064, 0.056697044521570206, 0.07745074480772018, -0.030471935868263245, 0.04740171879529953, -0.041349709033966064, 0.005172321572899818, -0.06180018559098244, 0.10170131921768188, -0.013619688339531422, -0.04470209404826164, -0.0675177201628685, -0.015519795008003712, -0.0028994621243327856, 0.0198178980499506, 0.040690843015909195, 0.0029115115758031607, 0.06657100468873978, 0.05164149031043053, 0.05906888097524643, 0.004184741992503405, 0.09241170436143875, 0.03318265825510025, 0.010382353328168392, -0.03369663655757904, 0.013537806458771229, -0.00947441253811121, -0.09508385509252548, -0.012054894119501114, -0.06114574149250984, -0.001485553104430437, -0.02389133907854557, 0.0011022549588233232, 0.019889840856194496, 0.009488471783697605, -0.038905832916498184, 0.026126980781555176, -0.04388763755559921, -0.01070354413241148, 0.04959770292043686, -0.0757279023528099, 0.00441863713786006, -0.08824263513088226, 0.03878043591976166, -0.07439988106489182, 0.011202066205441952, -0.024319887161254883, 0.044776856899261475, 0.001813647453673184, -0.02700384519994259, 0.045216698199510574, 0.07692176103591919, 0.026743484660983086, 0.06721019744873047, 0.014240331947803497, -0.08521862328052521, -0.020495904609560966, -0.06283533573150635, -0.11029285192489624, 0.042512860149145126, -0.002905050292611122, -0.07373346388339996, 0.04718505218625069, 0.0596805065870285, 0.07879694551229477, 0.07243390381336212, 0.0629071518778801, 0.04657679796218872, 0.04207591712474823, 0.0568520687520504, -0.059623297303915024, 0.028469298034906387, 0.04911066219210625, 0.02598794363439083, 0.0570785291492939, 0.034151602536439896, -6.316255252158953e-08, 0.005729469936341047, -0.0204072967171669, 0.010541146621108055, 0.016188347712159157, 0.00351186515763402, -0.09384530782699585, -0.005318078212440014, -0.034684374928474426, 0.014011189341545105, -0.023405905812978745, 0.0963289737701416, 0.02291812002658844, -0.10720529407262802, -0.010576911270618439, 0.06272140890359879, 0.035062339156866074, -0.02454959787428379, -0.030068952590227127, 0.024604806676506996, -0.023639386519789696, 0.06492264568805695, 0.0016485961386933923, 0.014873092994093895, 0.07105940580368042, 0.07682584226131439, -0.05674228072166443, 0.028275519609451294, 0.03876982256770134, 0.017697149887681007, 0.03271638602018356, 0.05500534549355507, 0.030476437881588936, 0.04158032312989235, 0.026061123237013817, 0.015389014966785908, 0.0433066263794899, 0.028713274747133255, 0.04317743703722954, -0.05624393746256828, 0.05080908164381981, 0.012458354234695435, -0.024401551112532616, -0.055435910820961, 0.021907266229391098, -0.023864973336458206, 0.020385175943374634, -0.000541072164196521, -0.10840868949890137, -0.039603106677532196, 0.0009133202256634831, -0.004879008047282696, 0.032247502356767654, 0.0028792847879230976, 0.03183708339929581, 0.04381965473294258, -0.08255907148122787, -0.020404420793056488, -0.07359102368354797, 0.02874203771352768, 0.014772429130971432, -0.020707188174128532, 0.062123674899339676, -0.06926275044679642, -0.02109387330710888]}}, {'_index': 'temp', '_id': '116', '_score': 1.5019792, '_source': {'content': 'b1stands for “all hidden units”, gafor “only gate units,” and allfor “all noninput units.”\\nCol. 13: the function h, where idis identity function, h1is logistic sigmoid in [ −2,2]. Col. 14:\\nthe logistic function g, where g1 is sigmoid in [0 ,1],g2i n[−1,1]. Col. 15: learning rate α.\\npositive (negative) internal state. Once the input gate opens for the second\\ntime, so does the output gate, and the memory cell output is fed back toits own input. This causes (X,Y)to be represented by a positive internal\\nstate, because Xcontributes to the new internal state twice (via current\\ninternal state and cell output feedback). Similarly, (Y,X)gets represented\\nby a negative internal state.\\n5.7 Summary of Experimental Conditions. Tables 10 and 11 provide an\\noverview of the most important LSTM parameters and architectural detailsfor experiments 1 through 6. The conditions of the simple experiments 2a', 'dense_vector': [-0.009654754772782326, -0.11630833148956299, -0.07835137844085693, 0.00893881544470787, 0.02432117983698845, -0.06565745919942856, 0.01181350089609623, -0.017398659139871597, -0.0023150919005274773, 0.020613892003893852, 0.06788207590579987, -0.03317376598715782, 0.019605442881584167, -0.027393348515033722, -0.01968265324831009, 0.05518724024295807, 0.007410318125039339, -0.0028934450820088387, -0.0887107327580452, -0.025256242603063583, 0.09307675808668137, -0.025644788518548012, -0.020067725330591202, 0.002990545006468892, -0.06184038519859314, -0.1037781611084938, 0.022158145904541016, -0.007423280738294125, 0.04596181586384773, -0.11484953761100769, 0.020119894295930862, 0.019881976768374443, 0.04062279686331749, 0.047025881707668304, -0.05184630677103996, -0.009231224656105042, -0.032420914620161057, -0.1335642784833908, -0.029834341257810593, -0.003400923451408744, -0.018061675131320953, -0.029306510463356972, 0.008087308146059513, -0.01439851988106966, 0.03151513263583183, 0.06176847964525223, -0.019531408324837685, -0.037348341196775436, -0.06912636756896973, 0.03209790587425232, 0.025783536955714226, 0.0902387723326683, -0.015979409217834473, 0.04965175688266754, -0.05165368691086769, 0.06947045028209686, -0.009348469786345959, 0.05104793980717659, -0.08216647803783417, 0.06158434599637985, -0.052420999854803085, 0.0077704559080302715, -0.00623656390234828, -0.06439995765686035, 0.026628892868757248, -0.02114090882241726, 0.0387609601020813, -0.008504098281264305, 0.021546747535467148, 0.047012802213430405, 0.009933372028172016, -0.07283099740743637, -0.0067756217904388905, -0.0957389622926712, 0.0518801286816597, 0.007939774543046951, 0.05379489064216614, 0.0612608827650547, 0.027468586340546608, -0.03078567050397396, 0.05854029580950737, 0.08697392046451569, 0.0007139910012483597, 0.022053098306059837, 0.03846406191587448, -0.05278116092085838, 0.008900100365281105, 0.07678911834955215, 0.001218942692503333, 0.026042496785521507, 0.011883229948580265, -0.023722747340798378, -0.0781053900718689, -0.043940868228673935, 0.06651443988084793, -0.05181141197681427, 0.042982716113328934, -0.010125637985765934, -0.03680438920855522, 0.01247099507600069, -0.02953370288014412, 0.055887285619974136, 0.08876311033964157, 0.038222670555114746, 0.0034594316966831684, -0.012131272815167904, 0.0706956684589386, 0.05043618753552437, 0.08088855445384979, -0.01618417724967003, 0.03094964474439621, 0.05577656626701355, -0.011799277737736702, 0.05657627806067467, 0.04836525022983551, -0.01987624540925026, 0.06197492778301239, -0.03302861750125885, -0.007085793651640415, 0.010033467784523964, -0.08000779151916504, 0.008348681963980198, -0.07138548046350479, -0.02728791907429695, -0.07918427884578705, 0.005688880570232868, -0.08946148306131363, 5.5628235078069895e-33, -0.028601516038179398, 0.011747494339942932, -0.056922804564237595, -0.12276970595121384, -0.035957977175712585, 0.04701557010412216, -0.0020906892605125904, 0.0032936884090304375, -0.011548326350748539, 0.04044916108250618, -0.07872425764799118, 0.0027817559894174337, -0.012792087160050869, 0.13250738382339478, -0.0054092612117528915, -0.030696135014295578, -0.057822853326797485, 0.012267609126865864, 0.005878271069377661, -0.03640670329332352, 0.019712937995791435, 0.02291039563715458, -0.04414442554116249, -0.09813635796308517, 0.08155091851949692, -0.00392543151974678, 0.033273231238126755, 0.011915900744497776, -0.11670379340648651, 0.00520595908164978, -0.044477514922618866, 0.02522354945540428, -0.06637725979089737, -0.014599229209125042, 0.0969453677535057, 0.014594806358218193, 0.019610904157161713, -0.0053172423504292965, 0.0662044957280159, -0.1466166377067566, -0.11730222404003143, 0.07434637099504471, 0.05862599238753319, -0.04124611243605614, -0.14114902913570404, -0.08591856807470322, 0.07512098550796509, -0.03713543340563774, 0.03077690117061138, -0.1184772476553917, 0.014329462312161922, -0.018321558833122253, -0.07940563559532166, -0.0626741349697113, 0.05384286865592003, 0.05409194156527519, 0.04666656255722046, 0.08079816401004791, -0.009023460559546947, 0.11295081675052643, -0.04511367157101631, 0.03043822944164276, 0.06047134846448898, 0.017293844372034073, -0.027135564014315605, 0.12065935134887695, -0.061951976269483566, -0.009763039648532867, 0.07349023222923279, -0.026924854144454002, 0.00922512449324131, 0.007478238549083471, 0.03394721448421478, -0.059751689434051514, 0.0479276068508625, 0.008363491855561733, 0.10505528002977371, -0.040249571204185486, -0.031813472509384155, 0.0060585858300328255, 0.03927374258637428, -0.004900024738162756, -0.04344159737229347, 0.010115324519574642, -0.07209459692239761, -0.026312561705708504, 0.07643520832061768, -0.04466665908694267, -0.10105527192354202, 0.01587284728884697, -0.006373554468154907, -0.0028036197181791067, 0.07337614893913269, 0.054272573441267014, -0.07637884467840195, -6.351547891114097e-33, -0.01379615068435669, 0.03835124522447586, -0.03632001578807831, -0.03583117574453354, -0.059751782566308975, 0.011585683561861515, -0.0007798282313160598, -0.03917478770017624, -0.0370643325150013, 0.005411517806351185, 0.05695357918739319, 0.0658121258020401, -0.037585146725177765, 0.037802066653966904, 0.004776942543685436, 0.01605372317135334, -0.005288754589855671, 0.023703860118985176, -0.001001268858090043, -0.05415280535817146, 0.04213038086891174, 0.1357848048210144, -0.06591422855854034, -0.05101726949214935, -0.021229859441518784, -0.0023896677885204554, -0.016313154250383377, 0.10768899321556091, -0.02660539373755455, -0.06628317385911942, -0.042740657925605774, -0.012790302745997906, -0.05323823541402817, 0.07415771484375, 0.06026770919561386, -0.05213666334748268, 0.03196629136800766, -0.05051771551370621, 0.007863258011639118, -0.03705868124961853, 0.042439013719558716, 0.10768655687570572, 0.07153412699699402, 0.03873882442712784, 0.013848037458956242, 0.006369851063936949, -0.04018600285053253, 0.006634822580963373, -0.0028578697238117456, 0.0038107566069811583, -0.07750832289457321, -0.007708513643592596, -0.008674600161612034, 0.016482394188642502, -0.06062387675046921, -0.008588381111621857, -0.03963945060968399, 0.013613819144666195, 0.04366811364889145, -0.004964097868651152, -0.0735735222697258, -0.009059657342731953, 0.0073382677510380745, -0.02435312792658806, -0.08181370049715042, -0.04660097137093544, 0.06044600531458855, 0.014261551201343536, 0.07252922654151917, -0.018060266971588135, 0.05077873542904854, 0.08382920920848846, 0.06059813126921654, 0.008248688653111458, -0.05319766327738762, -0.0012635384919121861, -0.07296131551265717, -0.07656016200780869, -0.023478267714381218, -0.0632946640253067, 0.016448592767119408, 0.020411169156432152, -0.06036108359694481, 0.07517591118812561, 0.052479248493909836, -0.01921047270298004, 0.03944960609078407, 0.0838010236620903, 0.04768766462802887, -0.05004136636853218, 0.0009095471468754113, 0.07244876027107239, -0.08290818333625793, 0.02737809345126152, 0.00744682177901268, -6.456311751890098e-08, -0.011578015983104706, -0.03269864246249199, 0.027550986036658287, 0.0455874428153038, 0.04236837103962898, -0.036174844950437546, 0.0138040566816926, -0.038566954433918, 0.004035532008856535, -0.0021675738971680403, 0.05768623948097229, 0.056935932487249374, -0.11812209337949753, -0.1074448749423027, -0.007457486819475889, 0.04926382005214691, 0.06635183840990067, -0.0022458406165242195, 0.03636631742119789, 0.015615671873092651, 0.028453601524233818, -0.036685045808553696, -0.026917023584246635, 0.024836696684360504, -0.0020941447000950575, -0.03760235384106636, 0.0232559721916914, 0.021931666880846024, -0.041928939521312714, 0.06319784373044968, 0.052288275212049484, 0.08592316508293152, 0.03367175906896591, 0.04025174304842949, 0.023647524416446686, 0.07036612182855606, 0.010089057497680187, 0.05302312597632408, 0.017856508493423462, -0.005860531236976385, -0.023219933733344078, -5.502174644789193e-06, -0.057568613439798355, -0.02463020384311676, -0.06286957859992981, 0.0225648432970047, -0.03693220019340515, -0.06005759909749031, 0.0016977885970845819, 0.046171076595783234, 0.03633659705519676, 0.011780014261603355, -0.0011578985722735524, 0.07418622821569443, 0.08941924571990967, -0.021593553945422173, -0.031393785029649734, -0.036564551293849945, 0.0267162062227726, 0.08850201964378357, 0.028921378776431084, 0.07115332037210464, -0.010408684611320496, -0.0011297209421172738]}}, {'_index': 'temp', '_id': '121', '_score': 1.4934826, '_source': {'content': 'Long Short-Term Memory 1767\\nto vanish quickly. For the same reason, full BPTT does not outperform\\ntruncated BPTT.\\n•Each memory cell block needs two additional units (input and output\\ngate). In comparison to standard recurrent nets, however, this doesnot increase the number of weights by more than a factor of 9: eachconventional hidden unit is replaced by at most three units in theLSTM architecture, increasing the number of weights by a factor of 3\\n2\\nin the fully connected case. Note, however, that our experiments usequite comparable weight numbers for the architectures of LSTM andcompeting approaches.\\n•Due to its constant error ﬂow through CECs within memory cells,', 'dense_vector': [-0.043283432722091675, -0.05126650631427765, -0.001315225614234805, 0.024933205917477608, -0.0351870097219944, 0.048873741179704666, -0.06241428107023239, 0.05172412842512131, 0.014250824227929115, -0.08101112395524979, -0.006201077718287706, -0.036523256450891495, 0.010968311689794064, -0.07562640309333801, -0.0057843467220664024, -0.002319508232176304, 0.017935462296009064, 0.02564362995326519, 0.013552150689065456, -0.07146582752466202, -0.010870073921978474, -0.0028333268128335476, -0.05878187343478203, 0.013456805609166622, -0.006512272637337446, -0.039946697652339935, -0.09671302884817123, -0.014881378971040249, 0.13728070259094238, -0.05450383946299553, 0.05325921252369881, 0.04492783173918724, 0.015464338473975658, 0.05518687143921852, -0.06007443368434906, 0.049607280641794205, -0.0856216698884964, -0.10730889439582825, -0.032026708126068115, -0.020748885348439217, 0.04574776813387871, 0.037347737699747086, -0.007022767327725887, 0.03108781948685646, 0.052089985460042953, -0.00294367503374815, 0.022141331806778908, -0.04700648412108421, -0.10551228374242783, -0.01648273505270481, 0.0036372984759509563, 0.07898113131523132, 0.003469100221991539, 0.13429324328899384, -0.05004113167524338, -0.04107702150940895, -0.016473358497023582, 0.1158396452665329, -0.048279665410518646, 0.0406799390912056, -0.05328473448753357, -0.026160789653658867, -0.04143989458680153, -0.056254465132951736, 0.00617029657587409, -0.03829637169837952, -0.015279142186045647, 0.041036635637283325, 0.07955506443977356, 0.03836342692375183, 0.042488452047109604, -0.004193097818642855, -0.06111850589513779, 0.07136396318674088, 0.008185840211808681, 0.04164120927453041, 0.01708178035914898, -0.03208526223897934, -0.024954907596111298, -0.043260883539915085, 0.005286316853016615, 0.0024452805519104004, -0.020444601774215698, -0.03802002593874931, 0.057941801846027374, -0.02706889621913433, -0.04492354393005371, 0.0534215047955513, -0.06364607810974121, -0.007353018969297409, 0.07110339403152466, -0.027817830443382263, -0.02026699297130108, 0.024859372526407242, 0.026850303635001183, 0.04001009464263916, -0.02328425459563732, -0.0245834868401289, -0.061930060386657715, -0.0019147967686876655, 0.03814121335744858, 0.10686187446117401, -0.02676515094935894, 0.044841207563877106, -0.007719252724200487, -0.02965942770242691, 0.05033744499087334, 0.01342233270406723, 0.012578037567436695, -0.08633356541395187, 0.036370329558849335, 0.028380541130900383, 0.05186900123953819, 0.013763808645308018, 0.02245943434536457, -0.11214753240346909, -0.032405707985162735, -0.09617219120264053, 0.021963004022836685, 0.11526690423488617, -0.05345446243882179, -0.02918366715312004, -0.11919748783111572, 0.0062586357817053795, -0.06964997202157974, -0.008620967157185078, -0.03132135048508644, 6.874692835739925e-33, -0.01969532109797001, 0.009406877681612968, -0.05350111052393913, -0.051427312195301056, 0.029074888676404953, 0.04493771493434906, 0.01850893534719944, 0.008277583867311478, -0.05776844546198845, -0.021275311708450317, -0.013945608399808407, -0.04321771860122681, -0.04747019335627556, 0.062217194586992264, 0.025238603353500366, -0.07843267172574997, -0.051211919635534286, 0.06723318248987198, 0.019235001876950264, -0.06500931829214096, -0.009335266426205635, -0.006673854775726795, 0.005198990926146507, -0.09152674674987793, 0.11075913161039352, -0.023724762722849846, 0.03756379336118698, -0.017475297674536705, -0.021304767578840256, 0.04851953312754631, -0.050648342818021774, 0.048943281173706055, -0.006272017955780029, 0.015708275139331818, 0.034836795181035995, -0.07558315992355347, 0.03590022027492523, -0.069612056016922, 0.06102191284298897, -0.10377022624015808, -0.02677798829972744, 0.024149440228939056, 0.0077949888072907925, -0.015711501240730286, -0.07697956264019012, -0.09796033799648285, 0.033666010946035385, 0.03765610232949257, -0.03197436407208443, -0.030161019414663315, 0.04775131866335869, -0.0019714387599378824, -0.1216641440987587, -0.0311066173017025, 0.059611886739730835, -0.022517794743180275, 0.015815120190382004, 0.06184292957186699, 0.024076594039797783, 0.17003753781318665, 0.027295049279928207, 0.09173448383808136, -0.011636392213404179, 0.0683647096157074, 0.06955226510763168, 0.09059995412826538, -0.03519526496529579, 0.015823137015104294, 0.012601853348314762, -0.002517740009352565, 0.03809069097042084, -0.0362272709608078, 0.05594422295689583, 0.009110926650464535, 0.03175125643610954, -0.026215054094791412, 0.07987808436155319, -0.07649820297956467, -0.06216484308242798, 0.03615700080990791, 0.06552975624799728, 0.024133091792464256, 0.0061400095000863075, 0.012860588729381561, -5.8842622820520774e-05, -0.04466693475842476, 0.07644820213317871, -0.1079893559217453, -0.05664866790175438, 0.05260486900806427, 0.03262024745345116, -0.06972392648458481, 0.07978972047567368, 0.04317722097039223, -0.14349617063999176, -6.76136195600921e-33, -0.032553911209106445, 0.02938091568648815, -0.04509131610393524, 0.011200799606740475, -0.03639771416783333, -0.0878525823354721, -0.08718571811914444, -0.01669631339609623, -0.02924681082367897, 0.08824813365936279, 0.06717455387115479, -0.019278360530734062, -0.019995788112282753, 0.018018225207924843, 0.05620047077536583, -0.03773508220911026, 0.0514550507068634, -0.021301589906215668, -0.01220236998051405, -0.05392596870660782, 0.0766245499253273, 0.05149690434336662, -0.06614856421947479, 0.08394224941730499, -0.022356007248163223, -0.0016464674845337868, -0.08193069696426392, 0.05207405611872673, 0.01284792274236679, -0.05937732756137848, -0.05205163732171059, -0.05077347904443741, -0.03279970958828926, 0.0029226155020296574, -0.012547148391604424, 0.002142086625099182, 0.08107621222734451, 0.043748240917921066, 0.05457749217748642, -0.017464397475123405, 0.12102069705724716, 0.017522798851132393, -0.0170658640563488, -0.08968757838010788, 0.010503316298127174, -0.03898832947015762, -0.08590582758188248, -0.01382460631430149, 0.03287003934383392, 0.001147196744568646, -0.009777653031051159, 0.005533980205655098, 0.008751400746405125, 0.029534758999943733, -0.014758637174963951, -0.02374204434454441, -0.08660514652729034, 0.060761891305446625, 0.012470375746488571, -0.10164295881986618, -0.014632699079811573, -0.07950865477323532, -0.0134923430159688, -0.0554177425801754, 0.04851331189274788, 0.02149125374853611, 0.06005524843931198, -0.011299234814941883, -0.007160293869674206, -0.016910629346966743, 0.06711363792419434, 0.055531278252601624, -0.027185626327991486, -0.00551700871437788, -0.08558089286088943, 0.025028644129633904, -0.04815911501646042, -0.09852444380521774, -0.026128292083740234, -0.025733068585395813, -0.05316716805100441, 0.0443950891494751, 0.04389598220586777, -0.008559073321521282, 0.08135847002267838, 0.07874370366334915, -0.017078682780265808, 0.05262749269604683, -0.005899718496948481, -0.0008589119534008205, -0.009099235758185387, -0.018164411187171936, 0.061953477561473846, -0.008799868635833263, -0.013245662674307823, -4.9559034209778474e-08, -0.022644497454166412, -0.006414775270968676, -0.022543583065271378, 0.07750562578439713, 0.04612588509917259, -0.11934757232666016, -0.003949338104575872, 0.027078470215201378, 0.07698175311088562, 0.04486002027988434, 0.03717637062072754, -0.06936687976121902, -0.08451617509126663, -0.036220502108335495, 0.04469216614961624, 0.06153181195259094, -0.007524574175477028, -0.04923390597105026, 0.04315347597002983, -0.030543286353349686, 0.059925228357315063, 0.036623675376176834, 0.023218758404254913, 0.07908749580383301, -0.02051953226327896, -0.02711368538439274, 0.01952667348086834, 0.11418730020523071, 0.04942448437213898, 0.031122030690312386, 0.04807019606232643, 0.022823652252554893, -0.026683013886213303, 0.020339542999863625, 0.03146030008792877, 0.02510274387896061, 0.040955446660518646, 0.0554690808057785, -0.03292065113782883, 0.05258285254240036, 0.06270362436771393, 0.01650639995932579, -0.010907812044024467, 0.07509904354810715, 0.011797676794230938, -0.003624511882662773, -0.05726949870586395, -0.05224914103746414, -0.011702198535203934, -0.0743577629327774, 0.09247299283742905, 0.010025410912930965, 0.009911821223795414, 0.045269083231687546, 0.06386377662420273, -0.012453047558665276, -0.02269597351551056, -0.038231879472732544, -0.01428952720016241, 0.047241758555173874, -0.017864903435111046, 0.0046427370980381966, -0.002483868272975087, 0.031940266489982605]}}, {'_index': 'temp', '_id': '71', '_score': 1.4924581, '_source': {'content': 'Architectures.\\nRTRL: One self-recurrent hidden unit, p+1 nonrecurrent output units.\\nEach layer has connections from all layers below. All units usethe logistic activation function sigmoid in [0 ,1].\\nBPTT: Same architecture as the one trained by RTRL.\\nCH: Both net architectures like RTRL’s, but one has an additional out-\\nput for predicting the hidden unit of the other one (see Schmid-huber, 1992b, for details).\\nLSTM: As with RTRL, but the hidden unit is replaced by a memory cell\\nand an input gate (no output gate required). gis the logistic sig-\\nmoid, and his the identity function h:h(x)=x,∀x. Memory cell\\nand input gate are added once the error has stopped decreasing(see abuse problem: solution 1 in section 4).\\nResults. Using RTRL and a short four-time-step delay ( p=4), 7/9o fa l l\\ntrials were successful. No trial was successful with p=10. With long time', 'dense_vector': [-0.039887767285108566, -0.06168694421648979, -0.03517698124051094, 0.062063731253147125, -0.0034626703709363937, 0.019160034134984016, 0.021520225331187248, 0.03888195380568504, 0.015670401975512505, -0.023260369896888733, 0.07603800296783447, -0.03329682722687721, 0.033414263278245926, -0.034755539149045944, -0.014710132032632828, -0.0007900336640886962, -0.06562743335962296, 0.018404750153422356, -0.052905287593603134, -0.047499630600214005, 0.00829806923866272, -0.0068122623488307, -0.016726303845643997, 0.039424676448106766, -0.024785295128822327, -0.08083630353212357, -0.012558719143271446, -0.008243064396083355, 0.05232357606291771, -0.05940456688404083, 0.02311032824218273, 0.03265533223748207, -0.01104229036718607, 0.03050241619348526, -0.06055840849876404, 0.06494565308094025, -0.05648265779018402, -0.084968701004982, -0.05900246649980545, -0.03801647573709488, 0.042355019599199295, -0.058202363550662994, 0.011109892278909683, 0.015367275103926659, 0.06437203288078308, 0.020609596744179726, -0.025336802005767822, 0.02344798482954502, -0.08820287138223648, 0.03077586553990841, -0.036885667592287064, 0.13084357976913452, 0.03504154086112976, 0.10815954953432083, -0.07365325838327408, -0.007945888675749302, 0.05890732258558273, 0.0665297657251358, -0.09424740076065063, -0.011066325008869171, -0.05291140824556351, -0.00439038360491395, -0.054949983954429626, -0.06463824957609177, -0.03781503438949585, 0.027490315958857536, 0.0006266115233302116, -0.027180835604667664, 0.13838715851306915, 0.07116120308637619, 0.04333234950900078, -0.06366926431655884, -0.045788224786520004, 0.012831090949475765, 0.0848897323012352, -0.005138074047863483, 0.07837589830160141, 0.014604448340833187, 0.0025127343833446503, -0.023508774116635323, 0.03785635903477669, 0.05421292409300804, 0.00034823836176656187, -0.02372206561267376, 0.06317882984876633, 0.004516610410064459, -0.029688751325011253, 0.044067565351724625, -0.002607176546007395, -0.032806310802698135, 0.020846951752901077, -0.008539766073226929, -0.044855620712041855, -0.005740320775657892, 0.0991113930940628, -0.029740968719124794, 0.0033497822005301714, 0.007196631748229265, -0.07301909476518631, -0.006792291533201933, -0.023120874539017677, 0.080825075507164, -0.0419497974216938, 0.008889821358025074, 0.02850603312253952, 0.03959668055176735, 0.0515146367251873, 0.01901334337890148, 0.018172308802604675, -0.06786836683750153, 0.03579412400722504, 0.028948698192834854, 0.031487759202718735, 0.029391545802354813, 0.0998864620923996, -0.034655436873435974, 0.05342387408018112, -0.06421823799610138, -0.0032776931766420603, 0.11518748104572296, -0.09555662423372269, -0.0011930163018405437, -0.08005402982234955, 0.0077002025209367275, 0.011252516880631447, -0.021929271519184113, -0.0756094753742218, 6.954619104756148e-33, -0.011281633749604225, 0.04468800872564316, -0.06317077577114105, -0.1002407893538475, -0.0335741750895977, 0.05769634246826172, -0.009478751569986343, -0.00011306980741210282, -0.052007000893354416, 0.03397654369473457, -0.12389206141233444, -0.1244729533791542, -0.030487587675452232, 0.06318967789411545, 0.02246565744280815, -0.055982254445552826, -0.031337492167949677, 0.06567683815956116, 0.02259979210793972, -0.041076209396123886, -0.04650883004069328, 0.017433062195777893, -0.00025876917061395943, -0.08021362870931625, 0.06444056332111359, 0.06637685745954514, 0.04802248254418373, 0.014385097660124302, -0.016406159847974777, 0.0313149131834507, 0.023498453199863434, 0.050574399530887604, -0.07204854488372803, -0.03257502615451813, 0.024989711120724678, -0.007446514908224344, -0.012395068071782589, -0.034214507788419724, 0.019268931820988655, -0.07647523283958435, -0.08102325350046158, 0.039513181895017624, 0.00943803507834673, -0.001026734011247754, -0.05175121873617172, -0.1436271071434021, 0.011808967217803001, 0.035832732915878296, -0.0179013442248106, -0.06847263872623444, 0.05701963230967522, 0.02560868300497532, -0.1311471164226532, -0.05238157510757446, 0.02693677693605423, -0.029148628935217857, 0.03862766548991203, 0.10794078558683395, 0.028048330917954445, 0.11752650886774063, 0.04090659320354462, 0.033450573682785034, -0.022972526028752327, 0.12878035008907318, 0.02328939363360405, 0.09362862259149551, 0.061430130153894424, -0.03772474452853203, -0.025272609665989876, -0.05136246979236603, -0.005754898302257061, -0.08295539766550064, 0.03115111030638218, -0.031490590423345566, 0.03326500207185745, -0.020368443801999092, 0.09140220284461975, -0.06981257349252701, -0.013772476464509964, 0.07173621654510498, 0.043412577360868454, 0.012936673127114773, -0.021483629941940308, 0.03398226946592331, -0.022827642038464546, -0.076668381690979, 0.10017390549182892, -0.05237289518117905, -0.0708250105381012, 0.0037280188407748938, 0.004259072244167328, 0.02171947993338108, 0.05756065621972084, 0.060411643236875534, -0.09405840188264847, -6.704160931530258e-33, -0.08194732666015625, 0.06590697914361954, 0.025716006755828857, -0.020390167832374573, -0.053139057010412216, -0.039044082164764404, -0.05602223798632622, -0.0603460818529129, -0.02928408980369568, 0.01353214867413044, 0.1259925812482834, -0.026949146762490273, -0.04741859436035156, -0.00014827700215391815, 0.028839759528636932, -0.0011667090002447367, 0.0044686091132462025, -0.040637046098709106, 0.0026102957781404257, -0.004875377286225557, 0.0541803352534771, 0.06994256377220154, -0.06997153908014297, 0.0315796360373497, -0.09703480452299118, 0.006772182881832123, -0.05865175649523735, 0.13909219205379486, 0.01740586943924427, -0.07133803516626358, -0.035496700555086136, 0.0032152479980140924, -0.06521549820899963, 0.03362066298723221, 0.05943523719906807, 0.08082691580057144, 0.008125418797135353, -0.0013913155999034643, 0.02416282892227173, 0.028554093092679977, 0.08581971377134323, 0.0461973212659359, 0.03139004856348038, -0.06627900898456573, -0.011644632555544376, -0.004537876229733229, -0.06818730384111404, -0.03416517749428749, 0.008243590593338013, 0.015012259595096111, -0.00941692665219307, 0.07337940484285355, -0.014325846917927265, -0.005263383500277996, -0.0425485260784626, 0.05896899476647377, -0.07135612517595291, 0.05252029374241829, -0.008576720952987671, -0.03796694055199623, 0.013331756927073002, -0.05416248366236687, 0.028005043044686317, 0.018001195043325424, -0.0032852930016815662, -0.02105823904275894, 0.0704372227191925, 0.04263468459248543, 0.01031744759529829, -0.002394867828115821, 0.09742867201566696, 0.12177988886833191, 0.08824455738067627, 0.021942537277936935, -0.04998138174414635, -0.045924700796604156, -0.14795543253421783, -0.0566643662750721, -0.01630687154829502, -0.03289370238780975, 0.003827342763543129, 0.01904171146452427, -0.00774663919582963, 0.037913959473371506, 0.05566682666540146, 0.0321270227432251, -0.0032974404748529196, 0.07671955972909927, 0.034967850893735886, -0.028570011258125305, 0.007410421967506409, 0.01203319150954485, -0.02685999684035778, 0.011669585481286049, 0.04389362037181854, -6.516096817676953e-08, 0.029251644387841225, -0.03273829072713852, 0.04150219261646271, -0.001774299656972289, 0.010833369567990303, -0.10252354294061661, -0.023252705112099648, -0.017103465273976326, -0.017346007749438286, 0.0009986370569095016, 0.0404975488781929, -0.008775850757956505, -0.10420608520507812, -0.05922200158238411, 0.04484081268310547, -0.009388353675603867, -0.020276077091693878, -0.035452742129564285, 0.02191450633108616, -0.052905648946762085, 0.06599600613117218, 0.017301391810178757, -0.014764038845896721, -0.012513034977018833, -0.005793906282633543, -0.03749142959713936, -0.002128494903445244, 0.054901860654354095, -0.044844646006822586, 0.0034041483886539936, 0.07301201671361923, -0.004930603317916393, 0.005280961282551289, 0.08419287204742432, 0.008154197596013546, 0.07441774010658264, 0.05512678623199463, 0.033521316945552826, -0.05015677586197853, 0.03084011934697628, -0.01078865583986044, -0.01746426522731781, -0.048215724527835846, 0.03815307468175888, -0.060399264097213745, 0.03826112300157547, -0.020721398293972015, -0.11667454987764359, -0.002219732850790024, -0.022186506539583206, 0.012127380818128586, 0.06713137775659561, -0.012142941355705261, 0.008352365344762802, 0.06501227617263794, -0.015817388892173767, -0.019103560596704483, -0.07034336030483246, 0.013307863846421242, 0.058480508625507355, -0.05737268552184105, 0.013701794669032097, -0.02094699628651142, -0.037910740822553635]}}]}})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# response  = es_client.search(\n",
        "#     index=\"temp\",\n",
        "#     body={\n",
        "#         \"query\": {\n",
        "#             \"match\": {\n",
        "#             \"content\": {\n",
        "#                 \"query\": user_query\n",
        "#               }\n",
        "#           }\n",
        "#       }\n",
        "#   },\n",
        "#   knn={\n",
        "#       \"field\": \"dense_vector\",\n",
        "#       \"query_vector\":  query_vector,\n",
        "#       \"k\": 10,\n",
        "#       \"num_candidates\": 100\n",
        "#     },\n",
        "#   rank={\n",
        "#       \"rrf\": {}\n",
        "#     }\n",
        "# )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "id": "wlMMASyeElfh",
        "outputId": "0c613e2d-34fd-457d-b8ad-e31f9ed5ee64"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-bbe697f1bba6>:1: DeprecationWarning: Received 'knn' via a specific parameter in the presence of a 'body' parameter, which is deprecated and will be removed in a future version. Instead, use only 'body' or only specific parameters.\n",
            "  response  = es_client.search(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "BadRequestError(400, 'search_phase_execution_exception', 'failed to create query: to perform knn search on field [dense_vector], its mapping must have [index] set to [true]')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-34-bbe697f1bba6>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m response  = es_client.search(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     body={\n\u001b[1;32m      4\u001b[0m         \"query\": {\n\u001b[1;32m      5\u001b[0m             \"match\": {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, index, aggregations, aggs, allow_no_indices, allow_partial_search_results, analyze_wildcard, analyzer, batched_reduce_size, ccs_minimize_roundtrips, collapse, default_operator, df, docvalue_fields, error_trace, expand_wildcards, explain, ext, fields, filter_path, force_synthetic_source, from_, highlight, human, ignore_throttled, ignore_unavailable, include_named_queries_score, indices_boost, knn, lenient, max_concurrent_shard_requests, min_compatible_shard_node, min_score, pit, post_filter, pre_filter_shard_size, preference, pretty, profile, q, query, rank, request_cache, rescore, rest_total_hits_as_int, retriever, routing, runtime_mappings, script_fields, scroll, search_after, search_type, seq_no_primary_term, size, slice, sort, source, source_excludes, source_includes, stats, stored_fields, suggest, suggest_field, suggest_mode, suggest_size, suggest_text, terminate_after, timeout, track_scores, track_total_hits, typed_keys, version, body)\u001b[0m\n\u001b[1;32m   4147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m__body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4148\u001b[0m             \u001b[0m__headers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content-type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4149\u001b[0;31m         return self.perform_request(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   4150\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4151\u001b[0m             \u001b[0m__path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mpath_parts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_parts\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         ) as otel_span:\n\u001b[0;32m--> 271\u001b[0;31m             response = self._perform_request(\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             raise HTTP_EXCEPTIONS.get(meta.status, ApiError)(\n\u001b[0m\u001b[1;32m    353\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresp_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             )\n",
            "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'search_phase_execution_exception', 'failed to create query: to perform knn search on field [dense_vector], its mapping must have [index] set to [true]')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# AuthorizationException: AuthorizationException(403, 'security_exception', 'current license is non-compliant for [Reciprocal Rank Fusion (RRF)]')\n",
        "# BadRequestError: BadRequestError(400, 'search_phase_execution_exception', 'failed to create query: to perform knn search on field [dense_vector], its mapping must have [index] set to [true]')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "VKuQA7cc_vxY",
        "outputId": "75e07aeb-d390-4487-a061-f0780d2195e7"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-31-8d14351d65ff>:1: DeprecationWarning: Received 'retriever' via a specific parameter in the presence of a 'body' parameter, which is deprecated and will be removed in a future version. Instead, use only 'body' or only specific parameters.\n",
            "  resp = es_client.search(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "BadRequestError(400, 'parsing_exception', 'Unknown key for a START_OBJECT in [retriever].')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-8d14351d65ff>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m resp = es_client.search(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"temp\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mretriever\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretriever\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     body={\n\u001b[1;32m      5\u001b[0m         \"query\": {\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    444\u001b[0m                         \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 446\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mapi\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    447\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[0;31m# type: ignore[return-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/__init__.py\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, index, aggregations, aggs, allow_no_indices, allow_partial_search_results, analyze_wildcard, analyzer, batched_reduce_size, ccs_minimize_roundtrips, collapse, default_operator, df, docvalue_fields, error_trace, expand_wildcards, explain, ext, fields, filter_path, force_synthetic_source, from_, highlight, human, ignore_throttled, ignore_unavailable, include_named_queries_score, indices_boost, knn, lenient, max_concurrent_shard_requests, min_compatible_shard_node, min_score, pit, post_filter, pre_filter_shard_size, preference, pretty, profile, q, query, rank, request_cache, rescore, rest_total_hits_as_int, retriever, routing, runtime_mappings, script_fields, scroll, search_after, search_type, seq_no_primary_term, size, slice, sort, source, source_excludes, source_includes, stats, stored_fields, suggest, suggest_field, suggest_mode, suggest_size, suggest_text, terminate_after, timeout, track_scores, track_total_hits, typed_keys, version, body)\u001b[0m\n\u001b[1;32m   4147\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m__body\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4148\u001b[0m             \u001b[0m__headers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"content-type\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"application/json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4149\u001b[0;31m         return self.perform_request(  # type: ignore[return-value]\n\u001b[0m\u001b[1;32m   4150\u001b[0m             \u001b[0;34m\"POST\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4151\u001b[0m             \u001b[0m__path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36mperform_request\u001b[0;34m(self, method, path, params, headers, body, endpoint_id, path_parts)\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mpath_parts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_parts\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         ) as otel_span:\n\u001b[0;32m--> 271\u001b[0;31m             response = self._perform_request(\n\u001b[0m\u001b[1;32m    272\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/elasticsearch/_sync/client/_base.py\u001b[0m in \u001b[0;36m_perform_request\u001b[0;34m(self, method, path, params, headers, body, otel_span)\u001b[0m\n\u001b[1;32m    350\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             raise HTTP_EXCEPTIONS.get(meta.status, ApiError)(\n\u001b[0m\u001b[1;32m    353\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresp_body\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m             )\n",
            "\u001b[0;31mBadRequestError\u001b[0m: BadRequestError(400, 'parsing_exception', 'Unknown key for a START_OBJECT in [retriever].')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def rrf_rank(bm25_results, dense_results, rank_constant=20):\n",
        "    combined_scores = {}\n",
        "\n",
        "    # Process BM25 results\n",
        "    for rank, hit in enumerate(bm25_results['hits']['hits']):\n",
        "        doc_id = hit['_id']\n",
        "        score = 1 / (rank + 1 + rank_constant)  # 1-based index for rank\n",
        "        combined_scores[doc_id] = combined_scores.get(doc_id, 0) + score\n",
        "\n",
        "    # Process Dense results\n",
        "    for rank, hit in enumerate(dense_results['hits']['hits']):\n",
        "        doc_id = hit['_id']\n",
        "        score = 1 / (rank + 1 + rank_constant)  # 1-based index for rank\n",
        "        combined_scores[doc_id] = combined_scores.get(doc_id, 0) + score\n",
        "\n",
        "    # Sort by combined RRF score\n",
        "    ranked_results = sorted(combined_scores.items(), key=lambda item: item[1], reverse=True)\n",
        "    return ranked_results[:5]  # Return top 5 results\n",
        "\n",
        "# Combine results using RRF\n",
        "top_5_results = rrf_rank(bm25_results, dense_results)"
      ],
      "metadata": {
        "id": "YYeHxpFA_vp3"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "top_5_results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30Jjgd5l_vmU",
        "outputId": "3581b6f3-913b-49a7-95c7-28fd00b6952e"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('127', 0.09523809523809523),\n",
              " ('33', 0.08391608391608392),\n",
              " ('40', 0.08347826086956522),\n",
              " ('41', 0.045454545454545456),\n",
              " ('141', 0.043478260869565216)]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rrf_doc_ids = [doc_id for doc_id, _ in top_5_results]\n",
        "\n",
        "print(\"BM25 Results for Specified Document IDs:\")\n",
        "for hit in bm25_results['hits']['hits']:\n",
        "    if hit['_id'] in rrf_doc_ids:\n",
        "        doc_id = hit['_id']\n",
        "        score = hit['_score']  # The BM25 score\n",
        "        content = hit['_source']['content']  # Adjust based on your actual document structure\n",
        "        print(f\"Document ID: {doc_id}, BM25 Score: {score}, Content: {content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nkZwiczG_viW",
        "outputId": "f14e512e-a54d-49cd-9387-7a486acafd23"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BM25 Results for Specified Document IDs:\n",
            "Document ID: 127, BM25 Score: 8.395834, Content: To ﬁnd out about LSTM’s practical limitations we intend to apply it to\n",
            "real-world data. Application areas will include time-series prediction, musiccomposition, and speech processing. It will also be interesting to augmentsequence chunkers (Schmidhuber, 1992b, 1993) by LSTM to combine theadvantages of both.\n",
            "Appendix\n",
            "A.1 Algorithm Details. In what follows, the index kranges over output\n",
            "units, iranges over hidden units, cjstands for the jth memory cell block, cv\n",
            "j\n",
            "denotes the vth unit of memory cell block cj,u,l,mstand for arbitrary units,\n",
            "and tranges over all time steps of a given input sequence.\n",
            "The gate unit logistic sigmoid (with range [0 ,1]) used in the experiments\n",
            "is\n",
            "f(x)=1\n",
            "1+exp(−x). (A.1)\n",
            "Document ID: 41, BM25 Score: 6.765766, Content: 4.4 Memory Cell Blocks. Smemory cells sharing the same input gate\n",
            "and the same output gate form a structure called a memory cell block of sizeS. Memory cell blocks facilitate information storage. As with conventional\n",
            "neural nets, it is not so easy to code a distributed input within a single cell.Since each memory cell block has as many gate units as a single memory cell(namely, two), the block architecture can be even slightly more efﬁcient. Amemory cell block of size 1 is just a simple memory cell. In the experimentsin section 5, we will use memory cell blocks of various sizes.\n",
            "4.5 Learning. We use a variant of RTRL (e.g., Robinson & Fallside, 1987)\n",
            "that takes into account the altered, multiplicative dynamics caused by inputand output gates. To ensure nondecaying error backpropagation throughinternal states of memory cells, as with truncated BPTT (e.g., Williams &Peng, 1990), errors arriving at memory cell net inputs (for cell c\n",
            "j, this includes\n",
            "Document ID: 141, BM25 Score: 6.5975647, Content: ∂yinj(t−k)\n",
            "∂yu(t−k−1)∀uand∂netcj(t−k)\n",
            "∂yu(t−k−1)∀u.\n",
            "In what follows, an error ϑj(t)starts ﬂowing back at cj’s output. We re-\n",
            "deﬁne\n",
            "ϑj(t):=∑\n",
            "iwicjϑi(t+1). (A.29)\n",
            "Following the deﬁnitions and conventions of section 3.1, we compute\n",
            "error ﬂow for the truncated backpropagation learning rule. The error occur-ring at the output gate is\n",
            "ϑ\n",
            "outj(t)≈tr∂youtj(t)\n",
            "∂netoutj(t)∂ycj(t)\n",
            "∂youtj(t)ϑj(t). (A.30)\n",
            "The error occurring at the internal state is\n",
            "ϑscj(t)=∂scj(t+1)\n",
            "∂scj(t)ϑscj(t+1)+∂ycj(t)\n",
            "∂scj(t)ϑj(t). (A.31)\n",
            "Since we use truncated backpropagation we have\n",
            "ϑj(t)=∑\n",
            "i:ino gate and no memory cellwicjϑi(t+1);\n",
            "Document ID: 40, BM25 Score: 6.2874904, Content: 4.3 Network Topology. We use networks with one input layer, one hid-\n",
            "den layer, and one output layer. The (fully) self-connected hidden layercontains memory cells and corresponding gate units (for convenience, werefer to both memory cells and gate units as being located in the hiddenlayer). The hidden layer may also contain conventional hidden units pro-viding inputs to gate units and memory cells. All units (except for gate units)in all layers have directed connections (serve as inputs) to all units in thelayer above (or to all higher layers; see experiments 2a and 2b).\n",
            "4.4 Memory Cell Blocks. Smemory cells sharing the same input gate\n",
            "and the same output gate form a structure called a memory cell block of sizeS. Memory cell blocks facilitate information storage. As with conventional\n",
            "Document ID: 33, BM25 Score: 6.04898, Content: cells, or even conventional hidden units if there are any (see section 4.3). Allthese different types of units may convey useful information about the cur-rent state of the net. For instance, an input gate (output gate) may use inputsfrom other memory cells to decide whether to store (access) certain infor-mation in its memory cell. There even may be recurrent self-connectionslikew\n",
            "cjcj. It is up to the user to deﬁne the network topology. See Figure 2 for\n",
            "an example.\n",
            "At time t,cj’s output ycj(t)is computed as\n",
            "ycj(t)=youtj(t)h(scj(t)),\n",
            "where the internal state scj(t)is\n",
            "scj(0)=0,scj(t)=scj(t−1)+yinj(t)g(\n",
            "netcj(t))\n",
            "fort>0.\n",
            "The differentiable function gsquashes netcj; the differentiable function h\n",
            "scales memory cell outputs computed from the internal state scj.\n",
            "4.2 Why Gate Units? To avoid input weight conﬂicts, injcontrols the\n",
            "error ﬂow to memory cell cj’s input connections wcji. To circumvent cj’s\n",
            "output weight conﬂicts, outjcontrols the error ﬂow from unit j’s output\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# index = \"temp\"\n",
        "es_client.indices.delete(index=\"temp\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL-tH9Ae2sIM",
        "outputId": "02873d58-5220-432a-c722-51d5994c27d5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ObjectApiResponse({'acknowledged': True})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S6mduyBl7ZfF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}