{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWBPFiJ84G3FzVUdcndKbr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarshSonaiya/DL/blob/main/ANN%20vs%20RNN%20.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ANN vs RNN\n",
        "\n",
        "# Architecture\n",
        "\n",
        "Artificial Neural Networks (ANNs) are composed of input, hidden and output layers with fully connected neurons.\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are also composed of input, hidden and output layers but the neurons may or may not be connected fully and there is a special connection where the output of a layer loops back into the same layer i.e. they have self loops or connections along with connection with other neurons in other subsequent layers.\n",
        "\n",
        "# Inputs\n",
        "\n",
        "In ANNs the entire data is fed into the network in one iteration. For eg, if in a image there are 16 pixels then all the 16 pixels are provided as input in one go to the network.\n",
        "\n",
        "IN RNNs the data is fed sequentially in to the neural network. For eg, there is a sentence \"Hey, How is your day going?\" then each and every one is fed into the network one by one.\n",
        "\n",
        "#Input format\n",
        "\n",
        "There is a dataset with 100 rows and 20 columns then these 20 columns are called the features of the dataset and there are two posibilities for the content of these 20 columns present in the 100 rows : Numeric and Textual data\n",
        "\n",
        "**Numeric Data**\n",
        "\n",
        "In ANNs if the magnitude of these columns is very large then they are usually scaled down and if it is very small they are scaled up and if none of those then they are combined together in a vector or a column matrix where each and every element in the matrix represents a corresponding feature and is fed to the neurons in the input layer where the neurons represnet these features now.\n",
        "\n",
        "In RNNs the upscaling and downscaling also takes place but primarily the input is of the format [Batch Size, Sequence Length, Features Dimension].For eg, there is a weather dataset and it contains hourly data for features temparature, wind and humidity for several days.Now here the hourly data points are sequential i.e. realted to each other.Batch Size signifies number of days for which data is present in the dataset, Sequence Length is the number of hours in the day for which data was recorded every day i.e. 24, Feature Dimension is the number of features in the dataset == 3. Now, feding data sequentially looks like this:\n",
        "            \n",
        "\n",
        "```\n",
        "#  [  [temparature, wind, humidity] ,  [temparature, wind, humidity],...24 times ]  #1 Sequence -> Day 1\n",
        "                Hour 1                            Hour 2 .....            Hour 24\n",
        "#  [  [temparature, wind, humidity] ,  [temparature, wind, humidity],...24 times ]  #2 Sequence -> Day 2\n",
        "                Hour 1                            Hour 2 .....            Hour 24\n",
        "```   \n",
        "so on for the Batch Size = No. of days = Sequences\n",
        "\n",
        "**Textual Data**\n",
        "\n",
        "The input format for the textual data is very similar for both ANNs and RNNs i.e. [batch_size, max_sequence_length, embedding_dim], where suppose a bunch of sentences are to be fed to both the neural networks then batch_size is the number of sentences to be processed, max_sequence_length is the number of tokens in the sentence and embedding_dim is a vector used to represent each token.\n",
        "\n",
        "The difference lies in the implementation manner, let's assume [32,5,5] is the input format:\n",
        "\n",
        "In ANNs all the 32 setences are independent of each other and are to fed to the network at the same time so for every sentence eg \"How are you?\" every word is first broken down into tokens(words) using tokenization techniques and then each token is created into word embeddings which are a way to represent a token (word) into a vector of numbers. Here, sequence_length = 5 that is sentence is to be broken in 5 tokens if more words are present they are truncated and if less are present padding is done then each and every token is converted into a vector of 5 numbers that represent that word only. This is done for all the 32 sentences and in ANN all the tokens of a sentence are fed together so there are 5 (tokens) x 5(embeddings for each token) = 25 neurons in input layer.\n",
        "\n",
        "In RNNs the same procedure for the tokenization and embedding is followed but there are only 5 neurons in the input layer because here the sentence are sequentially important and so each token of each and every sentence is fed one by one using its 5 dimensions to persist information.\n",
        "\n",
        "# Process in hidden layers\n",
        "\n",
        "In a single ANN every hidden layer consist of its own activation function as well as weights and bias. An ANN can consist of multiple activation functions depending upon the use case and they process the information coming from the input neurons.\n",
        "\n",
        "In a Single RNN there can be multiple hidden layers with their own activation functions, weights and biases but here the data comes in time steps so even a layers weights and bais remains same until and unless the entire sequence is not processed, i.e. if there are two hidden layers they can have different activation functions, weights and biases but after initialization the weights and bias of the layers will change only after a entire sequence is processed ( 1 day of climate data of features temprature,wind and humidity is processed)\n",
        "\n",
        "# Output\n",
        "\n",
        "In ANNs the output is of fixed length just like the inputs and it does not have any temporal dependencies\n",
        "\n",
        "In RNNs the output can be of variable length they can give output for each time step as well as (multiple outputs) after t time steps ( a single output).\n",
        "\n",
        "# Use Case\n",
        "\n",
        "ANNs are used in scenarios where well-defined satic data of a fixed length is present and there is not dependecy among data points e.g. Image processing\n",
        "\n",
        "RNNs are used in scenrios where the data  is sequential and is not of fixed length and the data points are related to each other e.g. Time_series Analysis.\n",
        "\n",
        "## Issues with ANN\n",
        "\n",
        "Overfitting\n",
        "\n",
        "Vanishing and exploding gradients\n",
        "\n",
        "Computational complexity\n",
        "\n",
        "Need for large datasets\n",
        "\n",
        "## Issues with RNN\n",
        "\n",
        "Vanishing and exploding gradients\n",
        "\n",
        "Long training times\n",
        "\n",
        "Difficulty handling long-term dependencies\n",
        "\n",
        "Computational complexity\n",
        "\n",
        "Difficulty with variable-length sequences\n",
        "\n",
        "Limited parallelism\n",
        "\n",
        "## Why not to prefer RNN\n",
        "\n",
        "Though the RNNs have the ability to persist information, but they cannot do so for a long period of time.\n",
        "\n",
        "Secondly, the dont have the ability to forget the information at will i.e. when the context changes persisted information should also change.\n",
        "\n",
        "## Long-Short Term Memory (LSTM)\n",
        "\n",
        "This is a special RNN that has the capability to persist information for a longer time along with the ability to forget unnecessary information at will."
      ],
      "metadata": {
        "id": "m5eTCKQujTSG"
      }
    }
  ]
}